[["index.html", "An Introduction to Data Analysis", " An Introduction to Data Analysis Michael Franke last rendered at: 2021-10-11 16:52:38 This book provides basic reading material for an introduction to data analysis. It uses R to handle, plot and analyze data. After covering the use of R for data wrangling and plotting, the book introduces key concepts of data analysis from a Bayesian and a frequentist tradition. This text is intended for use as a first introduction to statistics for an audience with some affinity towards programming, but no prior exposition to R. Many people have supported this project actively by providing text, examples, code or technical support. Many thanks to (in alphabetic order): Tobias Anton, Florence Bockting, Noa Kallioinen, Minseok Kang, Marcel Klehr, Özge Özenoglu, Maria Pershina, Timo Roettger, Polina Tsvilodub and Inga Wohlert. Work on this web-book has been kindly supported financially by the Lower Saxony Ministry for Science and Culture (project “Innovation plus 2020/21”). "],["general-introduction.html", "1 General Introduction", " 1 General Introduction This chapter lays out the learning goals of this book (Section 1.1) and describes how these goals are to be achieved (Section 1.2). Sections 1.3 and 1.4 detail which technical tools and theoretical concepts are covered in the course, and which are not. There will be some information on the kinds of data sets we will use during the course in Section 1.5. Section 1.6 provides information about how to install the necessary tools for this course. Finally, Section 1.7 provides more information on how a 12-week course can be structured based on the material in this web-book. "],["Chap-01-00-intro-learning-goals.html", "1.1 Learning goals", " 1.1 Learning goals At the end of this course students should: have gained the competence to understand complex data sets, manipulate a data set (using R), so as to plot aspects of it in ways that are useful for answering a given research question understand the general logic of statistical inference, in particular be able to interpret and apply standard analyses from a frequentist and a Bayesian approach be able to independently evaluate statistical analyses based on their adequacy for a given research question and data set be able to critically assess the adequacy of analyses commonly found in the literature Notice that this is, although a lot of hard work already, still rather modest! It doesn’t actually say that we necessarily aim at the competence to do it or even to do it flawlessly! Our main goal is understanding, because that is the foundation of practical success and the foundation of an ability to learn more in the future. We do not teach tricks! We do not share recipes! "],["Chap-01-00-intro-course-structure.html", "1.2 Course structure", " 1.2 Course structure The course consists of five parts. After giving a more detailed overview of the course in this chapter, Part I introduces R, the main programming language that we will use. Part II covers what is often called descriptive statistics. It also gives us room to learn more about R when we massage data into shape, compute summary statistics, and plot different data types in different ways. Part III covers the basic theoretical concepts of Bayesian data analysis. Part IV introduces regression modeling. Part V introduces basic ideas from frequentist data analysis and compares the frequentist and the Bayesian approach. A number of characteristic features distinguishes this course from the bulk of its cousins out there. First, we use a model-centric approach, i.e., we are going to explicitly represent and talk about statistical models as a formalized set of the assumptions which underly a specific analysis. Second, we will use a computational approach, i.e., we foster an understanding of mathematical notions with computer simulations or other variants of helpful code. Third, this course takes a dual approach in that it introduces both the frequentist and the Bayesian approach to statistical inference. We will start off with the Bayesian approach, because it is arguably more intuitive. Yet, a model-centric Bayesian approach also helps with understanding basic concepts from the frequentist paradigm. Fourth, the course focuses on generalized linear models, a class of models that have become the new standard for analyses of experimental data in the social and psychological sciences. They are also very useful for data exploration in other domains (such as machine learning). There are also appendices with additional information: Further useful material (textbooks, manuals, etc.) is provided in Appendix ??. Appendix ?? covers the most important probability distributions used in this book. An excursion providing more information about the important Exponential Family of probability distributions and the Maximum Entropy Principle is given in Appendix ??. The data sets which reoccur throughout the book as “running examples” are succinctly summarized in Appendix ??. Appendix ?? surveys and motivates ideas for good scientific practice and principles of open science. "],["Chap-01-00-intro-tools.html", "1.3 Tools used in this course", " 1.3 Tools used in this course The main programming language used in this course is R (R Core Team 2018). We will make heavy use of the tidyverse package (Wickham 2017), which provides a unified set of functions and conventions that deviate (sometimes: substantially) from basic R. We will also be using the probabilistic programming language WebPPL (Goodman and Stuhlmüller 2014), but only “passively” in order to quickly obtain results from probabilistic calculations that we can experiment with directly in the browser in order to better understand certain ideas or concepts. You will not need to learn to write WebPPL code yourself. We will rely on the R package brms (Burkner 2017) for running Bayesian generalized regression models, which itself relies on the probabilistic programming language Stan (Carpenter et al. 2017). We will, however, not learn to use Stan in this course, but we will take a glimpse at some Stan code to have seen, at least roughly, where the Markov Chain Monte Carlo samples come from which we will use. Section 1.6 gives information about how to install the tools necessary for this course. References "],["Chap-01-00-intro-topics.html", "1.4 Topics covered (and not covered) in the course", " 1.4 Topics covered (and not covered) in the course The main topics that this course will cover are: data preparation: how to clean up and massage a data set into an appropriate shape for plotting and analysis data visualization: how to select relevant aspects of data for informative visualization statistical models: what that is, and why it’s beneficial to think in terms of models, not tests statistical inference: what that is, and how it’s done in frequentist and Bayesian approaches hypothesis testing: how to test assumptions about a model’s parameters generalized regression: how to apply generalized regression models to different types of data sets There is, obviously, a lot that we will not cover in this course. We will, for instance, not dwell at any length on the specifics of algorithms for computing statistical inferences or model fits. We will also only deal with the history of statistics and questions of philosophy of science at the very end of this course and only to the extent that it helps us to better understand the theoretical notions and practical habits that are important in the context of this course. We will also not do extremely heavy math. There are at least two different motivations for data analysis, and it is important to keep them apart. This course focuses on data analysis for explanation, i.e., routines that help us understand reality through inspection of empirical data. We will only glance at the alternative approach, which is data analysis for prediction, i.e., using models to predict future observations, as commonly practiced in machine learning and its applications. In sloppy slogan form, this course treats data science for scientific knowledge gain, not data science as an engineering application. "],["Chap-01-00-intro-data-sets.html", "1.5 Data sets covered", " 1.5 Data sets covered Data analysis can be quite varied because data itself can be quite varied. We try to present some variation, but since this is an introductory course with lots of other ground to cover, we will be slightly conservative in the kind of data that we analyze. The focus is on data from behavioral experiments (like categorical choices or reaction times). There will be no analyses specifically tailored to pictures, sounds, dates or time sequences in this course. Appendix ?? gives an overview of the most important, recurring data sets used in this course. Most of the data sets that we will use repeatedly in this class come from various psychological experiments. To make this even more immersive, these experiments are implemented as browser-based experiments, using _magpie. This makes it possible for students of this course to actually conduct the exact experiments whose data the book will analyze (and maybe generate some more intuitions and some hypotheses). More on data sets used in this book is provided in Appendix ??. "],["Chap-01-00-intro-installation.html", "1.6 Installation", " 1.6 Installation To follow the code in this book, you need a recent version of R (recommended is an R version at least 4.0). We recommend the use of RStudio. The course also has its own R package. The aida package contains convenience functions introduced and used in this book. It can be installed by executing the following code. (More on R, packages and their installation in the next chapter.) install.packages(&#39;remotes&#39;) remotes::install_github(&#39;michael-franke/aida-package&#39;) This course also requires the packages rstan and brms which let R interact with the probabilistic programming language Stan. Installation of these packages can be difficult. If you run into trouble during the installation of these packages, please follow the instructions on the Stan homepage for the most recent recommendations for installation for your OS. "],["Chap-01-00-intro-schedule.html", "1.7 Example schedule (12-week course)", " 1.7 Example schedule (12-week course) It is possible to teach the content of this web-book in a semester (12 weeks). Below is an example schedule of how the material can be chunked. This schedule assumes that sections marked as “excursions” are skipped and that lecturers give clear guidance as to which aspects of each chapter are most important. Table 1.1: Example schedule for a 12-week course based on this web-book. week topic chapter 1 overiew, installation, first steps in R 1 &amp; 2.1 2 basics of R 2 3 data types &amp; wrangling 3 &amp; 4 4 summary statistics &amp; plotting 5 &amp; 6 5 probability basics 7 6 models &amp; parameter inference 8 &amp; 9 7 model comparison &amp; hypothesis testing 10 &amp; 11 8 linear regression 12 &amp; 13 9 categorical predictors &amp; GLM 14 &amp; 15 10 frequentist statistics 16 11 Bayes vs frequentist 17 12 discussion / reflection "],["Chap-01-01-R.html", "2 Basics of R", " 2 Basics of R R is a specialized programming language for data science. Though old, it is heavily supported by an active community. New tools for data handling, visualization, and statistical analysis are provided in the form of packages.1 While other programming languages, like Python or Julia, specialized for scientific computing, also lend themselves beautifully for data analysis, the choice of R in this book is motivated because R’s raison d’être is data analysis. Also, some of the R packages used in this book provide cutting-edge methods that are not as conveniently available in other programming languages (yet). In a manner of speaking, there are two flavors of R. We should distinguish base R from the tidyverse. Base R is what you have when you do not load any packages. We enter the tidyverse by loading the package tidyverse (see below for information on how to do that). The tidyverse consists of several components (which are actually stand-alone packages that can be loaded separately if needed) all of which supply extra functionality for data analysis, based on a unifying philosophy and representation format. While eventually interchangeable, the look-and-feel of base R and the tidyverse is quite different. Figure 2.1 lists a selection of packages from the tidyverse in relation to their role at different stages of the process of data analysis. Figure 2.1: Overview of selected packages from the tidyverse. The image is taken from this introduction to the tidyverse. The official documentation for base R is “An Introduction to R”. The standard reference for using the tidyverse is “R for Data Science (R4DS)”. There are some very useful cheat sheets which you should definitely check out! There are pointers to further material in Appendix ??. The learning goals for this chapter are: become familiar with R, its syntax and basic notions become familiar with the key functionality of the tidyverse understand and write simple R scripts be able to write documents in Rmarkdown Packages live in the official package repository CRAN, or are supplied in less standardized forms, e.g., via open repositories, such as GitHub.↩︎ "],["ch1-first-steps.html", "2.1 First steps", " 2.1 First steps R is an interpreted language. This means that you do not have to compile it. You can just evaluate it line by line, in a so-called session. The session stores the current values of all variables. Usually, code is stored in a script, so one does not have to retype it when starting a new session.2 Try this out by either typing r to open an R session in a terminal or load RStudio.3 You can immediately calculate stuff: 6 * 7 ## [1] 42 Exercise 2.1 Use R to calculate 5 times the result of 659 minus 34. Solution 5 * (659 - 34) ## [1] 3125 2.1.1 Functions R has many built-in functions. The most common situation is that the function is called by its name using prefix notation, followed by round brackets that enclose the function’s arguments (separated by commas if there are multiple arguments). For example, the function round takes a number and, by default, returns the closest integer: # the function `round` takes a number as an argument and # returns the closest integer (default) round(0.6) ## [1] 1 Actually, round allows several arguments. It takes as input the number x to be rounded and another integer number digits which gives the number of digits after the comma to which x should be rounded. We can then specify these arguments in a function call of round by providing the named arguments. # rounds the number `x` to the number `digits` of digits round(x = 0.138, digits = 2) ## [1] 0.14 If all of the parsed arguments are named, then their order does not matter. But all non-named arguments have to be presented in the positions expected by the function after subtracting the named arguments from the ordered list of arguments (to find out the right order one should use help, as explained below in 2.1.6). Here are examples for illustration: round(x = 0.138, digits = 2) # works as intended round(digits = 2, x = 0.138) # works as intended round(0.138, digits = 2) # works as intended round(0.138, 2) # works as intended round(x = 0.138, 2) # works as intended round(digits = 2, 0.138) # works as intended round(2, x = 0.138) # works as intended round(2, 0.138) # does not work as intended (returns 2) Functions can have default values for some or for all of their arguments. In the case of round, the default is digits = 0. There is obviously no default for x in the function round. round(x = 6.138) # returns 6 ## [1] 6 Some functions can take an arbitrary number of arguments. The function sum, which sums up numbers is a point in case. # adds all of its arguments together sum(1, 2, 3) ## [1] 6 Selected functions can also be expressed as operators in infix notation. This applies to frequently recurring operations, such as mathematical operations or logical comparisons. # both of these calls sum 1, 2, and 3 together sum(1, 2, 3) # prefix notation 1 + 2 + 3 # infix notation An expression like 3 + 5 is internally processed as the function `+`(3, 5) which is equivalent to sum(3, 5). Section 2.3 will list some of the most important built-in functions. It will also explain how to define your own functions. 2.1.2 Variables You can assign values to variables using three assignment operators: -&gt;, &lt;- and =, like so: x &lt;- 6 # assigns 6 to variable x 7 -&gt; y # assigns 7 to variable y z = 3 # assigns 3 to variable z x * y / z # returns 6 * 7 / 3 = 14 ## [1] 14 Use of = is discouraged.4 It is good practice to use a consistent naming scheme for variables. This book uses snake_case_variable_names and tends towards using long_and_excessively_informative_names for important variables, and short variable names, like i, j or x, for local variables, indices etc. Exercise 2.2 Create two variables, a and b, and assign the values 103 and 14 to them, respectively. Next, divide variable a by variable b and produce an output with three digits after the comma. Solution a &lt;- 103 b &lt;- 14 round(x = a / b, digits = 3) ## [1] 7.357 2.1.3 Literate coding It is good practice to document code with short but informative comments. Comments in R are demarcated with #. x &lt;- 4711 # a nice number from Cologne Since everything on a line after an occurrence of # is treated as a comment, it is possible to break long function calls across several lines, and to add comments to each line: round( # call the function `round` x = 0.138, # number to be rounded digits = 2 # number of after-comma digits to round to ) In RStudio, you can use Command+Shift+C (on Mac) and Ctrl+Shift+C (on Windows/Linux) to comment or uncomment code, and you can use comments to structure your scripts. Any comment followed by ---- is treated as a (foldable) section in RStudio. # SECTION: variable assignments ---- x &lt;- 6 y &lt;- 7 # SECTION: some calculations ---- x * y Exercise 2.3 Provide extensive comments to all operations in the solution code of the previous exercise. Solution a &lt;- 103 # assign value 103 to variable `a` b &lt;- 14 # assign value 14 to variable `b` round( # produce a rounded number x = a / b, # number to be rounded is a/b digits = 3 # show three digits after the comma ) ## [1] 7.357 2.1.4 Objects Strictly speaking, all entities in R are objects but that is not always apparent or important for everyday practical purposes (see the manual for more information). R supports an object-oriented programming style, but we will not make (explicit) use of this functionality. In fact, this book heavily uses and encourages a functional programming style (see Section 2.4). However, some functions (e.g., optimizers or fitting functions for statistical models) return objects, and we will use this output in various ways. For example, if we run some model on a data set the output is an object. Here, for example, we run a regression model, that will be discussed later on in the book, on a dataset called cars. # you do not need to understand this code model_fit = lm(formula = speed~dist, data = cars) # just notice that the function `lm` returns an object is.object(model_fit) ## [1] TRUE # printing an object on the screen usually gives you summary information print(model_fit) ## ## Call: ## lm(formula = speed ~ dist, data = cars) ## ## Coefficients: ## (Intercept) dist ## 8.2839 0.1656 2.1.5 Packages Much of R’s charm unfolds through the use of packages. CRAN has the official package repository. To install a new package from a CRAN mirror use the install.packages function. For example, to install the package remotes, you would use: install.packages(&quot;remotes&quot;) Once installed, you need to load your desired packages for each fresh session, using a command like the following:5 library(remotes) Once loaded, all functions, data, etc. that ship with a package are available without additional reference to the package name. If you want to be careful or courteous to an admirer of your code, you can reference a function from a package also by explicitly referring to that package. For example, the following code calls the function install_github from the package remotes explicitly.6 remotes::install_github(&quot;SOME-URL&quot;) Indeed, the install_github function allows you to install bleeding-edge packages from GitHub. You can install all packages relevant for this book using the following code (after installing the remotes package): remotes::install_github(&quot;michael-franke/aida-package&quot;) After this installation, you can load all packages for this book simply by using: library(aida) In RStudio, there is a special tab in the pane with information on “files”, “plots” etc. to show all installed packages. This also shows which packages are currently loaded. 2.1.6 Getting help If you encounter a function like lm that you do not know about, you can access its documentation with the help function or just typing ?lm. For example, the following call summons the documentation for lm, the first parts of which are shown in Figure 2.2. help(lm) Figure 2.2: Excerpt from the documentation of the lm function. If you are looking for help on a more general topic, use the function help.search. It takes a regular expression as input and outputs a list of occurrences in the available documentation. A useful shortcut for help.search is just to type ?? followed by the (unquoted) string to search for. For example, calling either of the following lines might produce a display like in Figure 2.3. # two equivalent ways for obtaining help on search term &#39;linear&#39; help.search(&quot;linear&quot;) ??linear Figure 2.3: Result of calling help.search for the term ‘linear’. The top entries in Figure 2.3 are vignettes. These are compact manuals or tutorials on particular topics or functions, and they are directly available in R. If you want to browse through the vignettes available on your machine (which depend on which packages you have installed), go ahead with: browseVignettes() Exercise 2.4 Look up the help page for the command round. As you know about this function already, focus on getting a feeling for how the help text is structured and the most important bits of information are conveyed. Try to understand what the other functions covered in this entry do and when which one would be most useful. Line-by-line execution of code is useful for quick development and debugging. Make sure to learn about keyboard shortcuts to execute single lines or chunks of code in your favorite editor, e.g., check the RStudio Cheat Sheet for information on its keyboard shortcuts.↩︎ You might need to add R to the PATH variables of your operating system to let the terminal know where R was installed (e.g. C:\\Program Files\\R\\R-4.0.3\\bin\\x64). Also, when starting a session in a terminal, you can exit a running R session by typing quit() or q().↩︎ You can produce &lt;- in RStudio with Option-- (on Mac) and Alt-- (on Windows/Linux). For other useful keyboard shortcuts, see here.↩︎ You need to make sure that all packages you need for a session are loaded, so you would need to supply several commands like library(PCKG_NAME) for all required packages.↩︎ Calling functions in this explicit way also dispenses the need to load the package first (though you need to have installed it before), and it might help with naming conflicts when different packages define identically named functions.↩︎ "],["ch1-data-types.html", "2.2 Data types", " 2.2 Data types To learn about a new programming language entails to first learn something about what kinds of objects (elements, first-order citizens) you will have to deal with. Let’s therefore briefly go through the data types that are most important for our later purposes. We will see how to deal with numeric information, Booleans, strings and so forth. In general, we can assess the type of an object stored in variable x with the function typeof(x). Let’s just try this for a bunch of things, just to give you an overview of some of R’s data types (not all of which are important to know about right from the start): typeof(3) # returns type &quot;double&quot; typeof(TRUE) # returns type &quot;logical&quot; typeof(cars) # returns &quot;list&quot; (includes data.frames, tibbles, objects, ...) typeof(&quot;huhu&quot;) # returns &quot;character&quot; (= string) typeof(mean) # returns &quot;closure&quot; (= function) typeof(c) # returns &quot;builtin&quot; (= deep system internal stuff) typeof(round) # returns type &quot;special&quot; (= well, special stuff?) If you really wonder, you can sometimes learn more about an object, if you just print it out as a string: # `lm` is actually a function (&quot;linear model&quot;) # the function `str` casts this function into a string # the result is then printed to screen str(lm) ## function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, model = TRUE, ## x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, ## offset, ...) It is sometimes possible to cast objects of one type into another type XXX using functions as.XXX in base R or as_XXX in the tidyverse. # casting Boolean value `TRUE` into number format as.numeric(TRUE) # returns 1 ## [1] 1 Casting can also happen explicitly. The expressions TRUE and FALSE are built-in variables for the Boolean values “true” and “false”. But when we use them in mathematical expressions, we can do math with them, like so: TRUE + TRUE + FALSE + TRUE + TRUE ## [1] 4 2.2.1 Numeric vectors &amp; matrices R is essentially an array-based language. Arrays are arbitrary but finite-dimensional matrices. We will discuss what is usually referred to as vectors (= one-dimensional arrays), matrices (= two-dimensional arrays), and arrays (= more-than-two-dimensional) in this section with a focus on numeric information. But it is important to keep in mind that arrays can contain objects of other types than numeric information (as long as all objects in the array are of the same type). 2.2.1.1 Numeric information Standard number format in R is double. typeof(3) ## [1] &quot;double&quot; We can also represent numbers as integers and complex. typeof(as.integer(3)) # returns &#39;integer&#39; ## [1] &quot;integer&quot; typeof(as.complex(3)) # returns &#39;complex&#39; ## [1] &quot;complex&quot; 2.2.1.2 Numeric vectors As a generally useful heuristic, expect every numerical information to be treated as a vector (or higher-order: matrix, array, … ; see below), and to expect any (basic, mathematical) operation in R to (most likely) apply to the whole vector, matrix, array, collection.7 This makes it possible to ask for the length of a variable to which we assign a single number, for instance: x &lt;- 7 length(x) ## [1] 1 We can even index such a variable: x &lt;- 7 x[1] # what is the entry in position 1 of the vector x? ## [1] 7 Or assign a new value to a hitherto unused index: x[3] &lt;- 6 # assign the value 6 to the 3rd entry of vector x x # notice that the 2nd entry is undefined, or &quot;NA&quot;, not available ## [1] 7 NA 6 Vectors in general can be declared with the built-in function c(). To memorize this, think of concatenation or combination. x &lt;- c(4, 7, 1, 1) # this is now a 4-place vector x ## [1] 4 7 1 1 There are also helpful functions to generate sequences of numbers: 1:10 # returns 1, 2, 3, ..., 10 seq(from = 1, to = 10, by = 1) # returns 1, 2, 3, ..., 10 seq(from = 1, to = 10, by = 0.5) # returns 1, 1.5, 2, ..., 9.5, 10 seq(from = 0, to = 1 , length.out = 11) # returns 0, 0.1, ..., 0.9, 1 Indexing in R starts with 1, not 0! x &lt;- c(4, 7, 1, 1) # this is now a 4-place vector x[2] ## [1] 7 And now we see what is meant above when we said that (almost) every mathematical operation can be expected to apply to a vector: x &lt;- c(4, 7, 1, 1) # 4-placed vector as before x + 1 ## [1] 5 8 2 2 Exercise 2.5 Create a vector that contains all even numbers from 0 to 20 and assign it to a variable. Now transform the variable such that it contains only odd numbers up to 20 using mathematical operation. Notice that the numbers above 20 should not be included! [Hint: use indexing.] Solution a &lt;- seq(from = 0, to = 20, by = 2) a &lt;- a + 1 a &lt;- a[1:10] a ## [1] 1 3 5 7 9 11 13 15 17 19 2.2.1.3 Numeric matrices Matrices are declared with the function matrix. This function takes, for instance, a vector as an argument. x &lt;- c(4, 7, 1, 1) # 4-placed vector as before (m &lt;- matrix(x)) # cast x into matrix format ## [,1] ## [1,] 4 ## [2,] 7 ## [3,] 1 ## [4,] 1 Notice that the result is a matrix with a single column. This is important. R uses so-called column-major mode.8 This means that it will fill columns first. For example, a matrix with three columns based on a six-placed vector 1, 2, \\(\\dots\\), 6 will be built by filling the first column from top to bottom, then the second column top to bottom, and so on.9 m &lt;- matrix(1:6, ncol = 3) m ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 In line with a column-major mode, vectors are treated as column vectors in matrix operations: x = c(1, 0, 1) # 3-place vector m %*% x # dot product with previous matrix &#39;m&#39; ## [,1] ## [1,] 6 ## [2,] 8 As usual, and independently of a column- or row-major mode, matrix indexing starts with the row index: m[1,] # produces first row of matrix &#39;m&#39; ## [1] 1 3 5 Exercise 2.6 Create a sequence of 9 numbers, equally spaced, starting from 0 and ending with 1. Assign this sequence to a vector called x. Now, create a matrix, stored in variable X, with three columns and three rows that contain the numbers of this vector in the usual column-major fashion. Solution x &lt;- seq(from = 0, to = 1, length.out = 9) X &lt;- matrix(x, ncol = 3) X ## [,1] [,2] [,3] ## [1,] 0.000 0.375 0.750 ## [2,] 0.125 0.500 0.875 ## [3,] 0.250 0.625 1.000 We have not yet covered this, but give it a try and guess what might be a convenient and very short statement to compute the sum of all numbers in matrix X. Solution sum(X) ## [1] 4.5 2.2.1.4 Arrays Arrays are simply higher-dimensional matrices. We will not make (prominent) use of arrays in this book. 2.2.1.5 Names for vectors, matrices and arrays The positions in a vector can be given names. This is extremely useful for good “literate coding” and therefore highly recommended. The names of vector x’s positions are retrieved and set by the names function:10 students &lt;- c(&quot;Jax&quot;, &quot;Jamie&quot;, &quot;Jason&quot;) # names of students grades &lt;- c(1.3, 2.7, 2.0) # a vector of grades names(grades) # retrieve names: with no names so far ## NULL names(grades) &lt;- students # assign names names(grades) # retrieve names again: names assigned ## [1] &quot;Jax&quot; &quot;Jamie&quot; &quot;Jason&quot; grades # output shows names ## Jax Jamie Jason ## 1.3 2.7 2.0 But we can also do this in one swoop, like so: c(Jax = 1.3, Jamie = 2.7, Jason = 2.0) ## Jax Jamie Jason ## 1.3 2.7 2.0 Names for matrices are retrieved or set with functions rownames and colnames. # declare matrix m &lt;- matrix(1:6, ncol = 3) # assign row and column names, using function # `str_c` which is described below rownames(m) &lt;- str_c(&quot;row&quot;, 1:nrow(m), sep = &quot;_&quot;) colnames(m) &lt;- str_c(&quot;col&quot;, 1:ncol(m), sep = &quot;_&quot;) m ## col_1 col_2 col_3 ## row_1 1 3 5 ## row_2 2 4 6 2.2.2 Booleans There are built-in names for Boolean values “true” and “false”, predictably named TRUE and FALSE. Equivalent shortcuts are T and F. If we attempt to do math with Boolean vectors, the outcome is what any reasonable logician would expect: x &lt;- c(T, F, T) 1 - x ## [1] 0 1 0 x + 3 ## [1] 4 3 4 Boolean vectors can be used as index sets to extract elements from other vectors. # vector 1, 2, ..., 5 number_vector &lt;- 1:5 # index of odd numbers set to `TRUE` boolean_vector &lt;- c(T, F, T, F, T) # returns the elements from number vector, for which # the corresponding element in the Boolean vector is true number_vector[boolean_vector] ## [1] 1 3 5 2.2.3 Special values There are a couple of keywords reserved in R for special kinds of objects: NA: “not available”; represent missing values in data NaN: “not a number”; e.g., division zero by zero Inf or -Inf: infinity and negative infinity; returned when a number is too big or divided by zero NULL: the NULL object; often returned when a function is undefined for the provided input 2.2.4 Characters (= strings) Strings are called characters in R. We will be stubborn and call them strings for most of the time here. We can assign a string value to a variable by putting the string in double-quotes: x &lt;- &quot;huhu&quot; typeof(x) ## [1] &quot;character&quot; We can create vectors of characters in the obvious way: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) chr_vector ## [1] &quot;huhu&quot; &quot;hello&quot; &quot;huhu&quot; &quot;ciao&quot; The package stringr from the tidyverse also provides very useful and, in comparison to base R, more uniform functions for string manipulation. The cheat sheet for the stringr package is highly recommended for a quick overview. Below are some examples. Function str_c concatenates strings: str_c(&quot;Hello&quot;, &quot;Hi&quot;, &quot;Hey&quot;, sep = &quot;! &quot;) ## [1] &quot;Hello! Hi! Hey&quot; We can find the indices of matches in a character vector with str_which: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) str_which(chr_vector, &quot;hu&quot;) ## [1] 1 3 Similarly, str_detect gives a Boolean vector of matching: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) str_detect(chr_vector, &quot;hu&quot;) ## [1] TRUE FALSE TRUE FALSE If we want to get the strings matching a pattern, we can use str_subset: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) str_subset(chr_vector, &quot;hu&quot;) ## [1] &quot;huhu&quot; &quot;huhu&quot; Replacing all matches with another string works with str_replace_all: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) str_replace_all(chr_vector, &quot;h&quot;, &quot;B&quot;) ## [1] &quot;BuBu&quot; &quot;Bello&quot; &quot;BuBu&quot; &quot;ciao&quot; For data preparation, we often need to split strings by a particular character. For instance, a set of reaction times could be separated by a character line “|”. We can split this string representation to get individual measurements like so: # three measures of reaction time in a single string reaction_times &lt;- &quot;123|234|345&quot; # notice that we need to doubly (!) escape character | # notice also that the result is a list (see below) str_split(reaction_times, &quot;\\\\|&quot;, n = 3) ## [[1]] ## [1] &quot;123&quot; &quot;234&quot; &quot;345&quot; 2.2.5 Factors Factors are special vectors, which treat their elements as instances of a finite set of categories. To create a factor, we can use the function factor. The following code creates a factor from a character vector. Notice that, when printing, we get information of the kinds of entries (= categories) that occurred in the original character vector: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) factor(chr_vector) ## [1] huhu hello huhu ciao ## Levels: ciao hello huhu For plotting or other representational purposes, it can help to manually specify an ordering on the levels of a factor using the levels argument: # the order of levels is changed manually factor(chr_vector, levels = c(&quot;huhu&quot;, &quot;ciao&quot;, &quot;hello&quot;)) ## [1] huhu hello huhu ciao ## Levels: huhu ciao hello Even though we specified an ordering among factor levels, the last code chunk nonetheless creates what R treats as an unordered factor. There are also genuine ordered factors. An ordered factor is created by setting the argument ordered = T, and optionally also specifying a specific ordering of factor levels, like so: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) factor( chr_vector, # the vector to treat as factor ordered = T, # make sure it&#39;s treated as ordered factor levels = c(&quot;huhu&quot;, &quot;ciao&quot;, &quot;hello&quot;) # specify order of levels by hand ) ## [1] huhu hello huhu ciao ## Levels: huhu &lt; ciao &lt; hello Having both unordered and ordered factors is useful for representing data from experiments, e.g., from categorical or ordinal variables (see Chapter 3). The difference between an unordered factor with explicit ordering information and an ordered factor is subtle and not important in the beginning. (This only matters, for example, in the context of regression modeling.) Factors are trickier to work with than mere vectors because they are rigid about the represented factor levels. Adding an item that does not belong to any of a factor’s levels, leads to trouble: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) my_factor &lt;- factor( chr_vector, # the vector to treat as factor ordered = T, # make sure it&#39;s treated as ordered factor levels = c(&quot;huhu&quot;, &quot;ciao&quot;, &quot;hello&quot;) # specify order of levels ) my_factor[5] &lt;- &quot;huhu&quot; # adding a &quot;known category&quot; is okay my_factor[6] &lt;- &quot;moin&quot; # adding an &quot;unknown category&quot; does not work my_factor ## [1] huhu hello huhu ciao huhu &lt;NA&gt; ## Levels: huhu &lt; ciao &lt; hello The forcats package from the tidyverse helps in dealing with factors. You should check the Cheat Sheet for more helpful functionality. Here is an example of how to expand the levels of a factor: chr_vector &lt;- c(&quot;huhu&quot;, &quot;hello&quot;, &quot;huhu&quot;, &quot;ciao&quot;) my_factor &lt;- factor( chr_vector, # the vector to treat as factor ordered = T, # make sure it&#39;s treated as ordered factor levels = c(&quot;huhu&quot;, &quot;ciao&quot;, &quot;hello&quot;) # specify order of levels ) my_factor[5] &lt;- &quot;huhu&quot; # adding a &quot;known category&quot; is okay my_factor &lt;- fct_expand(my_factor, &quot;moin&quot;) # add new category my_factor[6] &lt;- &quot;moin&quot; # adding new item now works my_factor ## [1] huhu hello huhu ciao huhu moin ## Levels: huhu &lt; ciao &lt; hello &lt; moin It is sometimes useful (especially for plotting) to flexibly reorder the levels of an ordered factor. Here are some useful functions from the forcats package: my_factor # original factor ## [1] huhu hello huhu ciao huhu moin ## Levels: huhu &lt; ciao &lt; hello &lt; moin fct_rev(my_factor) # reverse level order ## [1] huhu hello huhu ciao huhu moin ## Levels: moin &lt; hello &lt; ciao &lt; huhu fct_relevel( # manually supply new level order my_factor, c(&quot;hello&quot;, &quot;ciao&quot;, &quot;huhu&quot;) ) ## [1] huhu hello huhu ciao huhu moin ## Levels: hello &lt; ciao &lt; huhu &lt; moin 2.2.6 Lists, data frames &amp; tibbles Lists are key-value pairs. They are created with the built-in function list. The difference between a list and a named vector is that in the latter, all elements must be of the same type. In a list, the elements can be of arbitrary type. They can also be vectors or even lists themselves. For example: my_list &lt;- list( single_number = 42, chr_vector = c(&quot;huhu&quot;, &quot;ciao&quot;), nested_list = list(x = 1, y = 2, z = 3) ) my_list ## $single_number ## [1] 42 ## ## $chr_vector ## [1] &quot;huhu&quot; &quot;ciao&quot; ## ## $nested_list ## $nested_list$x ## [1] 1 ## ## $nested_list$y ## [1] 2 ## ## $nested_list$z ## [1] 3 To access a list element by its name (= key), we can use the $ sign followed by the unquoted name, double square brackets [[ \"name\" ]] with the quoted name inside, or indices in double brackets, like so: # all of these return the same list element my_list$chr_vector ## [1] &quot;huhu&quot; &quot;ciao&quot; my_list[[&quot;chr_vector&quot;]] ## [1] &quot;huhu&quot; &quot;ciao&quot; my_list[[2]] ## [1] &quot;huhu&quot; &quot;ciao&quot; Lists are very important in R because almost all structured data that belongs together is stored as lists. Objects are special kinds of lists. Data is stored in special kinds of lists, so-called data frames or so-called tibbles. A data frame is base R’s standard format to store data in. A data frame is a list of vectors of equal length. Data sets are instantiated with the function data.frame: # fake experimental data exp_data &lt;- data.frame( trial = 1:5, condition = factor( c(&quot;C1&quot;, &quot;C2&quot;, &quot;C1&quot;, &quot;C3&quot;, &quot;C2&quot;), ordered = T ), response = c(121, 133, 119, 102, 156) ) exp_data ## trial condition response ## 1 1 C1 121 ## 2 2 C2 133 ## 3 3 C1 119 ## 4 4 C3 102 ## 5 5 C2 156 Exercise 2.7 Create a vector a that contains the names of three of your best (imaginary) friends and a vector b with their (imaginary) age. Create a data frame that represents this information (one column with names and one with respective age). Notice that column names should represent the information they contain! Solution a &lt;- c(&quot;M&quot;, &quot;N&quot;, &quot;H&quot;) b &lt;- c(23, 41, 13) best_friends &lt;- data.frame(name = a, age = b) best_friends ## name age ## 1 M 23 ## 2 N 41 ## 3 H 13 We can access columns of a data frame, just like we access elements in a list. Additionally, we can also use index notation, like in a matrix: # gives the value of the cell in row 2, column 3 exp_data[2, 3] # returns 133 ## [1] 133 Exercise 2.8 Display the column of names of your (imaginary) friends from the best_friends data frame. Solution best_friends[&quot;name&quot;] ## name ## 1 M ## 2 N ## 3 H best_friends[1] ## name ## 1 M ## 2 N ## 3 H Now show only the names of friends who are younger than 22 (or some other age that makes sense for your friends and their ages). [Hint: you can write x &lt;= 22 to get a Boolean vector of the same length as x with an entry TRUE at all indices where x is no bigger than 22.] Solution best_friends[best_friends$age &lt;= 22, &quot;name&quot;] ## [1] &quot;H&quot; In RStudio, you can inspect data in data frames (and tibbles (see below)) with the function View. Tibbles are the tidyverse counterpart of data frames. We can cast a data frame into a tibble, using as_tibble. Notice that the information shown for a tibble is much richer than what is provided when printing the content of a data frame. as_tibble(exp_data) ## # A tibble: 5 × 3 ## trial condition response ## &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 1 C1 121 ## 2 2 C2 133 ## 3 3 C1 119 ## 4 4 C3 102 ## 5 5 C2 156 We can also create a tibble directly with the keyword tibble. Indeed, the creation of tibbles is conveniently more flexible than the creation of data frames: the former allows dynamic look-up of previously defined elements. my_tibble &lt;- tibble(x = 1:10, y = x^2) # dynamic construction possible my_dataframe &lt;- data.frame(x = 1:10, y = x^2) # ERROR :/ Another important difference between data frames and tibbles concerns the default treatment of character (= string) vectors. When reading in data from a CSV file as a data frame (using function read.csv), each character vector is treated as a factor by default. But when using read_csv to read CSV data into a tibble character vector are not treated as factors. There is also a very convenient function, called tribble, which allows you to create a tibble by explicitly writing out the information in the rows. hw_points &lt;- tribble( ~hw_nr, ~Jax, ~Jamie, ~Jason, &quot;HW1&quot;, 33, 24, 17, &quot;HW2&quot;, 41, 23, 8 ) hw_points ## # A tibble: 2 × 4 ## hw_nr Jax Jamie Jason ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HW1 33 24 17 ## 2 HW2 41 23 8 Exercise 2.9 Assign to the variable bff a tibble with the following columns (with reasonable names): at least four names of your (imaginary) best friends, their current country of residence, their age, and a Boolean column storing whether they are not older than 23. Ideally, use dynamic construction and the &lt;= operator as in previous exercises. Solution bff &lt;- tibble( name = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), residence = c(&quot;UK&quot;, &quot;JP&quot;, &quot;CH&quot;, &quot;JA&quot;), age = c(24, 45, 72, 12), young = age &lt;= 23 ) bff ## # A tibble: 4 × 4 ## name residence age young ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 A UK 24 FALSE ## 2 B JP 45 FALSE ## 3 C CH 72 FALSE ## 4 D JA 12 TRUE If you are familiar with Python’s scipy and numpy packages, this is R’s default mode of treating numerical information.↩︎ Python, on the other hand, uses the reverse row-major mode.↩︎ It is in this sense that the “first index moves fastest” in column-major mode, which is another frequently given explanation of column-major mode.↩︎ Notice that we can create strings (actually called ‘characters’ in R) with double quotes.↩︎ "],["Chap-01-01-functions.html", "2.3 Functions", " 2.3 Functions 2.3.1 Some important built-in functions Many helpful functions are defined in base R or supplied by packages. We recommend browsing the Cheat Sheets every now and then to pick up more useful stuff for your inventory. Here are some functions that are very basic and generally useful. 2.3.1.1 Standard logic &amp;: “and” |: “or” !: “not” negate(): a pipe-friendly ! (see Section 2.5 for more on piping) all(): returns true of a vector if all elements are T any(): returns true of a vector if at least one element is T 2.3.1.2 Comparisons &lt;: smaller &gt;: greater ==: equal (you can also use near()instead of == e.g. near(3/3,1)returns TRUE) &gt;=: greater or equal &lt;=: less or equal !=: not equal 2.3.1.3 Set theory %in%: whether an element is in a vector union(x, y): union of x and y intersect(x, y): intersection of x and y setdiff(x, y): all elements in x that are not in y 2.3.1.4 Sampling and combinatorics runif(): random number from unit interval [0;1] sample(x, size, replace): take size samples from x (with replacement if replace is T) choose(n, k): number of subsets of size n out of a set of size k (binomial coefficient) 2.3.2 Defining your own functions If you find yourself in a situation in which you would like to copy-paste some code, possibly with minor amendments, this usually means that you should wrap some recurring operations into a custom-defined function. There are two ways of defining your own functions: as a named function, or an anonymous function. 2.3.2.1 Named functions The special operator supplied by base R to create new functions is the keyword function. Here is an example of defining a new function with two input variables x and y that returns a computation based on these numbers. We assign a newly created function to the variable cool_function so that we can use this name to call the function later. Notice that the use of the return keyword is optional here. If it is left out, the evaluation of the last line is returned. # define a new function # - takes two numbers x &amp; y as argument # - returns x * y + 1 cool_function &lt;- function(x, y) { return(x * y + 1) } # apply `cool_function` to some numbers: cool_function(3, 3) # returns 10 cool_function(1, 1) # returns 2 cool_function(1:2, 1) # returns vector [2,3] cool_function(1) # throws error: &#39;argument &quot;y&quot; is missing, with no default&#39; cool_function() # throws error: &#39;argument &quot;x&quot; is missing, with no default&#39; We can give default values for the parameters passed to a function: # same function as before but with # default values for each argument cool_function_2 &lt;- function(x = 2, y = 3) { return(x * y + 1) } # apply `cool_function_2` to some numbers: cool_function_2(3, 3) # returns 10 cool_function_2(1, 1) # returns 2 cool_function_2(1:2, 1) # returns vector [2,3] cool_function_2(1) # returns 4 (= 1 * 3 + 1) cool_function_2() # returns 7 (= 2 * 3 + 1) Exercise 2.10 Create a function called bigger_100 which takes two numbers as input and outputs 0 if their product is less than or equal to 100, and 1 otherwise. (Hint: remember that you can cast a Boolean value to an integer with as.integer.) Solution bigger_100 &lt;- function(x, y) { return(as.integer(x * y &gt; 100)) } bigger_100(40, 3) ## [1] 1 2.3.2.2 Anonymous functions Notice that we can feed functions as parameters to other functions. This is an important ingredient of a functional-style of programming, and something that we will rely on heavily in this book (see Section 2.4). When supplying a function as an argument to another function, we might not want to name the function that is passed. Here’s a (stupid, but hopefully illustrating) example. We first define the named function new_applier_function which takes two arguments as input: an input vector, which is locally called input in the scope of the function’s body, and a function, which is locally called function_to_apply. Our new function new_applier_function first checks whether the input vector has more than one element, throws an error if not, and otherwise applies the argument function function_to_apply to the vector input. # define a function that takes a vector and a function as an argument new_applier_function &lt;- function(input, function_to_apply) { # check if input vector has at least 2 elements if (length(input) &lt;= 1) { # terminate and show informative error message stop(&quot;Error in &#39;new_applier_function&#39;: input vector has length &lt;= 1.&quot;) } # otherwise apply the function to the input vector return(function_to_apply(input)) } We use this new function to show the difference between named and unnamed functions, in particular why the latter can be very handy and elegant. First, we consider a case where we use new_applier_function in connection with the named built-in function sum: # sum vector with built-in &amp; named function new_applier_function( input = 1:3, # input vector function_to_apply = sum # built-in &amp; named function to apply ) # returns 6 If instead of an existing named function, we want to use a new function to supply to new_applier_function, we could define that function first and give it a name, but if we only need it “in situ” for calling new_applier_function once, we can also write this: # Sum vector with anonymous function new_applier_function( input = 1:3, # input vector function_to_apply = function(in_vec) { return(in_vec[1] + in_vec[2]) } ) # returns 3 (as it only sums the first two arguments) Exercise 2.11 How many arguments should you pass to a function that… …tells if the sum of two numbers is even? …applies two different operations on a variable and sums the results? Operations are not fixed in the function. Solution Two arguments. Three arguments. Call the function new_applier_function with input = 1:3 and an anonymous function that returns just the first two elements of the input vector in reverse order (as a vector). Solution new_applier_function( input = 1:3, # input vector function_to_apply = function(in_vec) { return(c(in_vec[c(2,1)])) } ) "],["ch-01-01-loops-and-maps.html", "2.4 Loops and maps", " 2.4 Loops and maps 2.4.1 For-loops For iteratively performing computation steps, R has a special syntax for for loops. Here is an example of an (again, stupid, but illustrative) example of a for loop in R: # fix a vector to transform input_vector &lt;- 1:6 # create output vector for memory allocation output_vector &lt;- integer(length(input_vector)) # iterate over length of input for (i in 1:length(input_vector)) { # multiply by 10 if even if (input_vector[i] %% 2 == 0) { output_vector[i] &lt;- input_vector[i] * 10 } # otherwise leave unchanged else { output_vector[i] &lt;- input_vector[i] } } output_vector ## [1] 1 20 3 40 5 60 Exercise 2.12 Let’s practice for-loops and if/else statements! Create a vector a with 10 random integers from range (1:50). Create a second vector b that has the same length as vector a. Then fill vector b such that the \\(i\\)th entry in b is the mean of a[(i-1):(i+1)]. Do that using a for-loop. Note that missing values are equal to 0 (see example below). Print out the result as a tibble whose columns are a and b. Example: If a has the values [25, 39, 12, 33, 47, 3, 48, 14, 45, 8], then vector b should contain the values [21, 25, 28, 31, 28, 33, 22, 36, 22, 18] when rounded to whole integers. The value in the fourth position of b (value 31), is obtained with (a[3] + a[4] + a[5])/3. The value in the first position of b (value 21) is obtained with (0 + a[1] + a[2])/3 and similarly the last value with (a[9] + a[10] + 0)/3. (Hint: use conditional statements if, if else and else to deal specifically with the edge cases (first and last entry in the vectors).) Solution a &lt;- c(sample((1:50), 10, replace = T)) b &lt;- c(integer(length(a))) for (i in 1:length(a)){ if (i == 1) { b[i] &lt;- (sum(a[i:(i+1)])/3) } else if (i == length(a)) { b[i] &lt;- (sum((a[(i-1):i]))/3) } else { b[i] &lt;- (mean(a[(i-1):(i+1)])) } } tibble(a, b) ## # A tibble: 10 × 2 ## a b ## &lt;int&gt; &lt;dbl&gt; ## 1 36 28 ## 2 48 29.3 ## 3 4 30.3 ## 4 39 16 ## 5 5 15 ## 6 1 14.3 ## 7 37 22.7 ## 8 30 27.7 ## 9 16 16.7 ## 10 4 6.67 2.4.2 Functional iterators Base R provides functional iterators (e.g., apply), but we will use the functional iterators from the purrr package. The main functional operator from purrr is map which takes a vector and a function, applies the function to each element in the vector and returns a list with the outcome. There are also versions of map, written as map_dbl (double), map_lgl (logical) or map_df (data frame), which return a vector of doubles, Booleans or a data frame. The following code repeats the previous example which used a for-loop but now within a functional style using the functional iterator map_dbl: input_vector &lt;- 1:6 map_dbl( input_vector, function(i) { if (input_vector[i] %% 2 == 0) { return(input_vector[i] * 10) } else { return (input_vector[i]) } } ) ## [1] 1 20 3 40 5 60 We can write this even shorter, using purrr’s short-hand notation for functions:11 input_vector &lt;- 1:6 map_dbl( input_vector, ~ ifelse(.x %% 2 == 0, .x * 10, .x) ) ## [1] 1 20 3 40 5 60 The trailing ~ indicates that we define an anonymous function. It, therefore, replaces the usual function(...) call which indicates which arguments the anonymous function expects. To make up for this, after the ~ we can use .x for the first (and only) argument of our anonymous function. To apply a function to more than one input vector, element per element, we can use pmap and its derivatives, like pmap_dbl etc. pmap takes a list of vectors and a function. In short-hand notation, we can define an anonymous function with ~ and integers like ..1, ..2 etc, for the first, second … argument. For example: x &lt;- 1:3 y &lt;- 4:6 z &lt;- 7:9 pmap_dbl( list(x, y, z), ~ ..1 - ..2 + ..3 ) ## [1] 4 5 6 Exercise 2.13 Use map_dbl and an anonymous function to take the following input vector and return a vector whose \\(i\\)th element is the cumulative product of input up to the \\(i\\)th position divided by the cumulative sum of input up to that position. (Hint: the cumulative product up to position \\(i\\) is produced by prod(input[1:i]); notice that you need to “loop over”, so to speak, the index \\(i\\), not the elements of the vector input.) input &lt;- c(12, 6, 18) Solution map_dbl( 1:length(input), function(i) { prod(input[1:i]) / sum(input[1:i]) } ) ## [1] 1 4 36 Just for the record, we can achieve the same result also by ifelse(input_vector %% 2 == 0, input_vector * 10, input_vector).↩︎ "],["Chap-01-01-piping.html", "2.5 Piping", " 2.5 Piping When we use a functional style of programming, piping is your best friend. Consider the standard example of applying functions in what linguists would call “center-embedding”. We start with the input (written inside the inner-most bracketing), then apply the first function round, then the second mean, writing each next function call “around” the previous. # define input input_vector &lt;- c(0.4, 0.5, 0.6) # first round, then take mean mean(round(input_vector)) ## [1] 0.3333333 Things quickly get out of hand when more commands are nested. A common practice is to store intermediate results of computations in new variables which are only used to pass the result into the next step. # define input input_vector &lt;- c(0.4, 0.5, 0.6) # rounded input rounded_input &lt;- round(input_vector) # mean of rounded input mean(rounded_input) ## [1] 0.3333333 Piping lets you pass the result of a previous function call into the next. The magrittr package supplies a special infix operator %&gt;% for piping.12 The pipe %&gt;% essentially takes what results from evaluating the expression on its left-hand side and inputs it as the first argument in the function on its right-hand side. So x %&gt;% f is equivalent to f(x). Or, to continue the example from above, we can now write: input_vector %&gt;% round %&gt;% mean ## [1] 0.3333333 The functions defined as part of the tidyverse are all constructed in such a way that the first argument is the most likely input you would like to pipe into them. But if you want to pipe the left-hand side into another argument slot than the first, you can do that by using the . notation to mark the slot where the left-hand side should be piped into: y %&gt;% f(x, .) is equivalent to f(x, y). Exercise 2.14 A friendly colleague has sent reaction time data in a weird format: weird_RTs &lt;- c(&quot;RT = 323&quot;, &quot;RT = 345&quot;, &quot;RT = 421&quot;, &quot;RT = 50&quot;) Starting with that vector, use a chain of pipes to: extract the numeric information from the string, cast the information into a vector of type numeric, take the log, take the mean, round to 2 significant digits. (Hint: to get the numeric information use stringr::str_sub, which works in this case because the numeric information starts after the exact same number of characters.) Solution weird_RTs %&gt;% stringr::str_sub(start = 6) %&gt;% as.numeric() %&gt;% log %&gt;% mean %&gt;% signif(digits = 2) ## [1] 5.4 2.5.1 Excursion: More on pipes in R When you load the tidyverse package the pipe operator %&gt;% is automatically imported from the magrittr package, but not the whole magrittr package. But the magrittr package has three more useful pipe operators, which are only available if you also explicitly load the magrittr package. library(magrittr) The tree pipe %T&gt;% from the magrittr package passes over to its RHS whatever it was fed on the LHS, thus omitting the output of the current command in the piping chain. This is particularly useful for printing or plotting intermediate results: input_vector &lt;- c(0.4, 0.5, 0.6) input_vector %&gt;% # get the mean mean %T&gt;% # output intermediate result print %&gt;% # do more computations sum(3) ## [1] 0.5 ## [1] 3.5 The exposition pipe %$% from the magrittr package is like the base pipe %&gt;% but makes the names (e.g., columns in a data frame) in the LHS available on the RHS, even when the function on the RHS normally does not allow for this. So, this does not work with the normal pipe: tibble( x = 1:3 ) %&gt;% # normal pipe sum(x) # error: object `x` not found But it works with the exposition pipe: tibble( x = 1:3 ) %$% # exposition pipe makes &#39;x&#39; available sum(x) # works! ## [1] 6 Finally, the assignment pipe %&lt;&gt;% from the magrittr package pipes the LHS into a chain of computations, as usual, but then assigns the final value back to the LHS. x &lt;- c(0.4, 0.5, 0.6) # x is changed in place x %&lt;&gt;% sum(3) %&gt;% mean print(x) ## [1] 4.5 Base R has introduced a native pipe operator |&gt; in version 4.1.0. It differs slightly from the magrittr version, e.g., in that it requires function brackets: 1:10 |&gt; mean # error! 1:10 |&gt; mean() # 5.5 You can read more about the history of the pipe in R in this blog post. 2.5.2 Excursion: Multiple assignments, or “unpacking” The zeallot package can be additionally loaded to obtain a “multiple assignment” operator %&lt;-% which looks like a pipe, but isn’t. library(zeallot) It allows for several variables to be instantiated at the same time: c(x, y) %&lt;-% c(3, &quot;huhu&quot;) print(x) ## [1] &quot;3&quot; print(y) ## [1] &quot;huhu&quot; This is particularly helpful for functions that return several outputs in a list or vector: input_vector &lt;- c(0.4, 0.5, 0.6) some_function &lt;- function(input) { return( list(sum = sum(input), mean = mean(input))) } c(x, y) %&lt;-% some_function(input_vector) print(x) ## [1] 1.5 print(y) ## [1] 0.5 The pipe symbol %&gt;% can be inserted in RStudio with Ctrl+Shift+M (Win/Linux) or Cmd+Shift+M (Mac).↩︎ "],["ch-01-01-Rmarkdown.html", "2.6 Rmarkdown", " 2.6 Rmarkdown Rmarkdown is a simple markup language that embeds R code and its output in order to produce a large variety of document types as output, including HTML or PDF (see Figure 2.4). Figure 2.4: Artwork by allison_horst A strong argument for using something like Rmarkdown in your research pipeline in reproducibility (see Figure 2.5). When you prepare a data set and perform a statistical analysis, for example, you will by necessity make a lot of detailed decisions, not all of which are easy to report in natural language, but all of which combined, contribute to the results you eventually report, e.g., in a research or term paper. Since science aims for objectivity and social structures of scrutinizing previous results, it is important to share with other researchers all of your assumptions and decisions of detail. Ideally, therefore, you can share something like a nicely commented Rmarkdown file that contains every step of the analysis. Figure 2.5: Artwork by allison_horst To get familiar with Rmarkdown, please follow this tutorial. "],["Chap-02-01-data.html", "3 Data, variables &amp; experimental designs", " 3 Data, variables &amp; experimental designs The focus of this course is on data from behavioral experiments, mostly from cognitive psychology or linguistics.13 Data from behavioral experiments are reasonably “well-behaved” data to analyze, in the sense that they require less preprocessing (such as, for instance, data from EEG experiments), and so provide an excellent starting point into data analysis. However, we should not lose sight of the rich and diverse guises of data that are relevant for scientific purposes. After discussing briefly what “data” is in general in Section 3.1, Section 3.2 surveys some of the richness and diversity in which “data” can occur. But it then hones in on some basic distinctions of the kinds of data we will frequently deal with in the cognitive sciences in Section 3.3. We also pick up a few relevant concepts of experimental design in Section 3.4. The learning goals for this chapter are: distinguish different kinds of variables dependent vs. independent nominal vs. ordinal vs. metric get familiar with basic aspects of experimental design factorial designs within- vs. between-subjects design repeated measures randomization, fillers and controls sample size A behavioral experiment is an experiment that records participants’ behavioral choices, such as button clicks or linguistic responses in the form of text or speech. This contrasts with, say, neurological experiments in which participants’ brain activity is recorded, such as with fMRI or EEG, or, e.g., in a psycholinguistic context, processing-related experiments in which secondary measures of cognitive activity are measured, such as eye-movements, pupil dilation or galvanic skin responses.↩︎ "],["Chap-02-01-data-what-is-data.html", "3.1 What is data?", " 3.1 What is data? Some say we live in the data age. But what is data actually? Purist pedants say: “The plural of datum” and add that a datum is just an observation. But when we say “data”, we usually mean a bit more than a bunch of observations. The observation that Jones had apple and banana for breakfast, is maybe interesting but not what we usually call “data”. The Merriam-Webster offers the following definition: Factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation. This is a teleological definition in the sense that it refers to a purpose: data is something that is “used as basis for reasoning, discussion, or calculation”. So, what we mean by “data” is, in large part, defined by what we intend to do with it. Another important aspect of this definition is that we usually consider data to be systematically structured in some way or another. Even when we speak of “raw data”, we expect there to be some structure (maybe labels, categories etc.) that distinguishes data from uninterpretable noise (e.g., the notion of a “variable”, discussed in Section 3.3). In sum, we can say that data is a representation of information stored in a systematic way for the purpose of inference, argument or decision making. Let us consider an example of data from a behavioral experiment, namely the King of France experiment. It is not important to know about this experiment for now. We just want to have a first glimpse at how data frequently looks like. Using R (in ways that we will discuss in the next chapter), we can show the content of part of the data as follows: ## # A tibble: 6 × 4 ## submission_id trial_number trial_type response ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 192 1 practice FALSE ## 2 192 2 practice TRUE ## 3 192 3 practice FALSE ## 4 192 4 practice TRUE ## 5 192 5 practice TRUE ## 6 192 1 filler TRUE We see that the data is represented as a tibble and that there are different kinds of column with different kinds of information. The submission_id is an anonymous identifier for the person whose data is shown here. The trial_number is a consecutive numbering of the different stages of the experiment (at each of which the participant gave one response, listed in the response column). The trial_type tells us which kind of trial each observation is from. There are more columns in this data set, but this is just for a first, rough impression of how “data” might look like. The most important thing to see here is that, following the definition above, data is “information stored in a systematic way”. "],["Chap-02-01-data-kinds-of-data.html", "3.2 Different kinds of data", " 3.2 Different kinds of data There are different kinds of data. Figure 3.1 shows some basic distinctions, represented in a conceptual hierarchy. Figure 3.1: Hierarchy of different kinds of data relevant for ‘data science’. It is easy but wrong to think that data always has to be information based on observations of the world. It is easy to think this because empirical data, i.e., data obtained from empirical observation, is the most common form of data (given that it is, arguably, most relevant for decision making and argument). But it is wrong to think this because we can just as well look at virtual data. For example, virtual data, which is of interest to a data analyst, could be data obtained from computer simulation studies, e.g., from, say, one billion runs of a multi-agent simulation intended to shed light on the nature of cooperative interaction. It makes sense to analyze such data with the same tools as data from an experiment. For instance, we might find out that some parameter constellations in the simulation run are (statistically) most conducive to producing cooperative behavior among our agents. Another example of virtual data is data generated as predictions of a model, which we can use to test whether that model is any good, in so-called model criticism.14 Finally, we should also include logically possible sample data in this list, because of its importance to central ideas of statistical inference (especially \\(p\\)-values, see Section ??). Logically possible sample data are those that were neither observed nor predicted by a model, but something that could have been observed hypothetically, something that it is merely logically possible to observe, even if it would almost never happen in reality or would not be predicted by any serious model. The most frequent form of data, empirical data about the actual world, comes in two major variants. Observational data is data gathered by (passively) observing and recording what would have happened even if we had not been interested in it, so to speak. Examples of observational data are collections of socio-economic variables, like gender, education, income, number of children, etc. In contrast, experimental data is data recorded in a strict regime of manipulation-and-observation, i.e., a scientific experiment. Some pieces of information can only be recorded in an observational study (annual income), and others can only be obtained through experimentation (memory span). Both methods of data acquisition have their own pros and cons. Here are some of the more salient ones: Table 3.1: Comparison of the pros and cons of observational data and experimental data. observational experimental ecologically valid possibly artificial easy/easier to obtain hard/harder to obtain correlation &amp; causation hard to tease apart may yield information on causation vs. correlation No matter what kind of data we have at hand, there are at least two prominent purposes for which data can be useful: explanation and prediction. Though related, it is useful to keep these purposes cleanly apart. Data analysis for explanation uses the data to better understand the source of the data (the world, a computer simulation, a model, etc.). Data analysis for prediction tries to extract regularities from the data gathered so far to make predictions (as accurately as possible) about future or hitherto unobserved data. We will later speak of prior/posterior predictions for this kind of data. Other applicable terms are repeat data or sometimes fake data.↩︎ "],["Chap-02-01-data-variables.html", "3.3 On the notion of “variables”", " 3.3 On the notion of “variables” Data used for data analysis, even if it is “raw data”, i.e., data before preprocessing and cleaning, is usually structured or labeled in some way or other. Even if the whole data we have is a vector of numbers, we would usually know what these numbers represent. For instance, we might just have a quintuple of numbers, but we would (usually/ideally) know that these represent the results of an IQ test. # a simple data vector of IQ-scores IQ_scores &lt;- c(102, 115, 97, 126, 87) Or we might have a Boolean vector with the information of whether each of five students passed an exam. But even then we would (usually/ideally) know the association between names and test results, as in a table like this: # who passed the exam exam_results &lt;- tribble( ~student, ~pass, &quot;Jax&quot;, TRUE, &quot;Jason&quot;, FALSE, &quot;Jamie&quot;, TRUE ) Association of information, as between different columns in a table like the one above, is crucial. Most often, we have more than one kind of observation that we care about. Most often, we care about systematic relationships between different observables in the world. For instance, we might want to look at a relation between, on the one hand, the chance of passing an exam and, on the other hand, the proportion of attendance of the course’s tutorial sessions: # proportion of tutorials attended and exam pass/fail exam_results &lt;- tribble( ~student, ~tutorial_proportion, ~pass, &quot;Jax&quot;, 0.0, TRUE, &quot;Jason&quot;, 0.78, FALSE, &quot;Jamie&quot;, 0.39, TRUE ) exam_results ## # A tibble: 3 × 3 ## student tutorial_proportion pass ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Jax 0 TRUE ## 2 Jason 0.78 FALSE ## 3 Jamie 0.39 TRUE Data of this kind is also called rectangular data, i.e., data that fits into a rectangular table (More on the structure of rectangular data in Section 4.2.). In the example above, every column represents a variable of interest. A (data) variable stores the observations that are of the same kind.15 Different kinds of variables are distinguished based on the structural properties of the kinds of observations that they represent. Common types of variables are, for instance: nominal variable: each observation is an instance of a (finite) set of clearly distinct categories, lacking a natural ordering; binary variable: special case of a nominal variable where there are only two categories; Boolean variable: special case of a binary variable where the two categories are Boolean values “true” and “false”; ordinal variable: each observation is an instance of a (finite) set of clearly distinct and naturally ordered categories, but there is no natural meaning of distance between categories (i.e., it makes sense to say that A is “more” than B but not that A is three times “more” than B); metric variable: each observation is isomorphic to a subset of the reals and interval-scaled (i.e., it makes sense to say that A is three times “more” than B); Examples of some different kinds of variables are shown in Figure 3.2, and Table 3.2 lists common and/or natural ways of representing different kinds of (data) variables in R. Figure 3.2: Examples of different kinds of (data) variables. Artwork by allison_horst. Table 3.2: Common / natural formats for representing data of different kinds in R. variable type representation in R nominal / binary unordered factor Boolean logical vector ordinal ordered factor metric numeric vector In experimental data, we also distinguish the dependent variable(s) from the independent variable(s). The dependent variables are the variables that we do not control or manipulate in the experiment, but the ones that we are curious to record (e.g., whether a patient recovered from an illness within a week). Dependent variables are also called to-be-explained variables. The independent variables are the variables in the experiment that we manipulate (e.g., which drug to administer), usually with the intention of seeing a particular effect on the dependent variables. Independent variables are also called explanatory variables. Exercise 3.1: Variables You are given the following table of observational data: ## # A tibble: 7 × 8 ## name age gender handedness height education has_pets mood ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 A 24 female right 1.74 undergraduate FALSE neutral ## 2 B 32 non-binary right 1.68 graduate TRUE happy ## 3 C 23 male left 1.62 high school TRUE OK ## 4 D 27 male right 1.84 graduate FALSE very happy ## 5 E 26 non-binary left 1.59 undergraduate FALSE very happy ## 6 F 28 female right 1.66 graduate TRUE OK ## 7 G 35 male right 1.68 high school FALSE neutral For each column, decide which type of variable (nominal, binary, etc.) is stored. Solution name: nominal variable age: metric variable gender: nominal variable handedness: binary variable height: metric variable education: ordinal variable has_pets: Boolean variable mood: ordinal variable This sense of “data variable” is not to be confused with the notion of a “random variable”, a concept we will introduce later in Section 7.4. The term “data variable” is not commonly used; the common term is merely “variable”.↩︎ "],["Chap-02-01-data-exp-design.html", "3.4 Basics of experimental design", " 3.4 Basics of experimental design The most basic template for an experiment is to just measure a quantity of interest (the dependent variable), without taking into account any kind of variation in any kind of independent variables. For instance, we measure the time it takes for an object with a specific shape and weight to hit the ground when dropped from a height of exactly 2 meters. To filter out measurement noise, we do not just record one observation, but, ideally, as much as we possibly and practically can. We use the measurements, in our concrete example: time measurements, to test a theory about acceleration and gravity. Data from such a simple measurement experiment would be just a single vector of numbers. A more elaborate kind of experiment would allow for at least one independent variable. Another archetypical example of an empirical experiment would be a medical study, e.g., one in which we are interested in the effect of a particular drug on the blood pressure of patients. We would then randomly allocate each participant to one of two groups. One group, the treatment group, receives the drug in question; the other group, the control group, receives a placebo (and nobody, not even the experimenter, knows who receives what). After a pre-defined exposure to either drug or placebo, blood pressure (for simplicity, just systolic blood pressure) is measured. The interesting question is whether there is a difference between the measurements across groups. This is a simple example of a one-factor design. The factor in question is which group any particular measurement belongs to. Data from such an experiment could look like this: tribble( ~subj_id, ~group, ~systolic, 1, &quot;treatment&quot;, 118, 2, &quot;control&quot;, 132, 3, &quot;control&quot;, 116, 4, &quot;treatment&quot;, 127, 5, &quot;treatment&quot;, 122 ) ## # A tibble: 5 × 3 ## subj_id group systolic ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 treatment 118 ## 2 2 control 132 ## 3 3 control 116 ## 4 4 treatment 127 ## 5 5 treatment 122 For the purposes of this course, which is not a course on experimental design, just a few key concepts of experimental design are important to be aware of. We will go through some of these issues in the following. 3.4.1 What to analyze? – Dependent variables To begin with, it is important to realize that there is quite some variation in what counts as a dependent variable. Not only can there be more than one dependent variable, but each dependent variable can also be of quite a different type (nominal, ordinal, metric, …), as discussed in the previous section. Moreover, we need to carefully distinguish between the actual measurement/observation and the dependent variable itself. The dependent variable is (usually) what we plot, analyze and discuss, but very often, we measure much more or something else. The dependent variable (of analysis) could well just be one part of the measurement. For example, a standard measure of blood pressure has a number for systolic and another for diastolic pressure. Focussing on just one of these numbers is a (hopefully: theoretically motivated; possibly: arbitrary; in the worst case: result-oriented) decision of the analyst. More interesting examples of such data preprocessing frequently arise in the cognitive sciences, for example: eye-tracking: the measured data are triples consisting of a time-point and two spatial coordinates, but what might be analyzed is just the relative proportion of looks at a particular spatial region of interest (some object on the screen) in a particular temporal region of interest (up to 200 ms after the image appeared) EEG: individual measurements obtained by EEG are very noisy, so that the dependent measure in many analyses is an aggregation over the mean voltage recorded by selected electrodes, where averages are taken for a particular subject over many trials of the same condition (repeated measures) that this subject has seen But we do not need to go fancy in our experimental methods to see how issues of data processing affect data analysis at its earliest stages, namely by selecting the dependent variable (that which is to be analyzed). Just take the distinction between closed questions and open questions in text-based surveys. In closed questions, participants select an answer from a finite (usually) small number of choices. In open questions, however, they can write text freely, or they can draw, sing, pronounce, gesture, etc. Open response formats are great and naturalistic, but they, too, often require the analyst to carve out a particular aspect of the (rich, natural) observed reality to enter the analysis. 3.4.2 Conditions, trials, items A factorial design is an experiment with at least two independent variables, all of which are (ordered or unordered) factors.16 Many psychological studies are factorial designs. Whole batteries of analysis techniques have been developed specifically tuned to these kinds of experiments. Factorial designs are often described in terms of short abbreviations. For example, an experiment described as a “\\(n \\times m\\) factorial design” would have two factors of interest, the first of which has \\(n\\) levels, the second of which has \\(m\\) levels. For example, a \\(2 \\times 3\\) factorial design could have one independent variable recording a binary distinction between control and treatment group, and another independent variable representing an orthogonal distinction of gender in categories ‘male’, ‘female’ and ‘non-binary’. For a \\(2 \\times 2 \\times 3\\) factorial design, there are 2 * 2 * 3 = 12 different experimental conditions (also sometimes called design cells). An important distinction in experimental design is whether all participants contribute data to all of the experimental conditions, or whether each only contributes to a part of it. If participants only contribute data to a part of all experimental conditions, this is called a between-subjects design. If all participants contribute data to all experimental conditions, we speak of a within-subjects design. Clearly, sometimes the nature of a design factor determines whether the study can be within-subjects. For example, switching gender for the purpose of a medical study on blood pressure drugs is perhaps a tad much to ask of a participant (though possibly a very enlightening experience). If there is room for the experimenter’s choice of study type, it pays to be aware of some of the clear advantages and drawbacks of either method, as listed in Table 3.3. Table 3.3: Comparison of the pros and cons of between- and within-subjects designs. between-subjects within-subjects no confound between conditions possible cross-contamination between conditions more participants needed fewer participants needed less associated information for analysis more associated data for analysis No matter whether we are dealing with a between- or within-subjects design, another important question is whether each participant gives us only one observation per design cell, or more than one. If participants contribute more than one observation to a design cell, we speak of a repeated-measures design. Such designs are useful as they help separate the signal from the noise (recall the initial example of time measurement from physics). They are also economical because getting several observations worth of relevant data from a single participant for each design cell means that we have to get fewer people to do the experiment (normally). However, exposing a participant repeatedly to the same experimental condition can be detrimental to an experiment’s purpose. Participants might recognize the repetition and develop quick coping strategies to deal with the boredom, for example. For this reason, repeated-measures designs usually include different kinds of trials: Critical trials belong to, roughly put, the actual experiment, e.g., one of the experiment’s design cells. Filler trials are packaged around the critical trials to prevent blatant repetition, predictability or recognition of the experiment’s purpose. Control trials are trials whose data is not used for statistical inference but for checking the quality of the data (e.g., attention checks or tests of whether a participant understood the task correctly). When participants are exposed to several different kinds of trials and even several instances of the same experimental condition, it is also often important to introduce some variability between the instances of the same types of trials. Therefore, psychological experiments often use different items, i.e., different (theoretically exchangeable) instantiations of the same (theoretically important) pattern. For example, if a careful psycholinguist designs a study on the processing of garden-path sentences, she will include not just one example (“The horse raced past the barn fell”) but several (e.g., “Since Jones frequently jogs a mile is a short distance to her”). Item-variability is also important for statistical analyses, as we will see when we talk about hierarchical modeling. In longer experiments, especially within-subjects repeated-measures designs in which participants encounter a lot of different items for each experimental condition, clever regimes of randomization are important to minimize the possible effect of carry-over artifacts, for example. A frequent method is pseudo-randomization, where the trial sequence is not completely arbitrary but arbitrary within certain constraints, such as a particular block design, where each block presents an identical number of trials of each type, but each block shuffles the sequence of its types completely at random. The complete opposite of a within-participants repeated measures design is a so-called single-shot experiment in which any participant gives exactly one data point for one experimental condition. 3.4.3 Sample size A very important question for experimental design is that of the sample size: how many data points do we need (per experimental condition)? We will come back to this issue only much later in this course when we talk about statistical inference. This is because the decision of how many, say, participants to invite for a study should ideally be influenced not by the available time and money, but also by statistical considerations of the kind: how many data points do I need in order to obtain a reasonable level of confidence in the resulting statistical inferences I care about? Exercise 3.2: Experimental Design Suppose that we want to investigate the effect of caffeine ingestion and time of day on reaction times in solving simple math tasks. The following table shows the measurements of two participants: ## # A tibble: 12 × 4 ## subject_id `RT (ms)` caffeine `time of day` ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 43490 none morning ## 2 1 35200 medium morning ## 3 1 33186 high morning ## 4 1 26350 none afternoon ## 5 1 27004 medium afternoon ## 6 1 26492 high afternoon ## 7 2 42904 none morning ## 8 2 36129 medium morning ## 9 2 30340 high morning ## 10 2 28455 none afternoon ## 11 2 40593 medium afternoon ## 12 2 23992 high afternoon Is this experiment a one-factor or a full factorial design? What is/are the factor(s)? How many levels does each factor have? Solution This experiment is a \\(3 \\times 2\\) full factorial design. It has two factors, caffeine (levels: none, medium, high) and time of day (levels: morning, afternoon). How many experimental conditions are there? Solution There are 3 * 2 = 6 different experimental conditions. Is it a between- or within-subjects design? Solution Within-subjects design (each participant contributes data to all experimental conditions). What is the dependent variable, what is/are the independent variable(s)? Solution Dependent variable: RT (the reaction time) Independent variable 1: caffeine (the caffeine dosage) Independent variable 2: time of day Is this experiment a repeated measures design? Explain your answer. Solution No, each participant contributes exactly one data point per design cell. The archetypical medical experiment discussed above is a one-factor design. In contrast, the term ‘factorial design’ is usually used to refer to what is also often called a full factorial design. These are designs with at least two independent variables.↩︎ "],["data-wrangling.html", "4 Data Wrangling", " 4 Data Wrangling The information relevant for our analysis goals is not always directly accessible. Sometimes, we must first uncover it effortfully from an inconvenient representation. Also, sometimes data must be cleaned (ideally: by a priori specified criteria) through removing data points that are deemed of insufficient quality for a particular goal. All of this, and more, is the domain of data wrangling: preprocessing, cleaning, reshaping, renaming etc. Section 4.1 describes how to read data from and write data to files. Section 4.2 introduces the concept of tidy data. We then look at a few common tricks of data manipulation in Section 4.3. We will learn about grouping operations in Section 4.4. Finally, we look at a concrete application in Section 4.5. The learning goals for this chapter are: be able to read from and write data to files understand the notion of tidy data be able to solve common problems of data preprocessing "],["Chap-02-02-data-IO.html", "4.1 Data in, data out", " 4.1 Data in, data out The readr package handles the reading and writing of data stored in text files.17 Here is a cheat sheet on the topic: data I/O cheat sheet. The data sets we will mainly deal with in this course are included in the aida package for convenience. Occasionally, we will also read in data stored in CSV files. Reading a data set from a CSV file works with the read_csv function: fresh_raw_data &lt;- read_csv(&quot;PATH/FILENAME_RAW_DATA.csv&quot;) Writing to a CSV file can be done with the write_csv function: write_csv(processed_data, &quot;PATH/FILENAME_PROCESSED_DATA.csv&quot;) If you want to use a different delimiter (between cells) than a comma, you can use read_delim and write_delim for example, which take an additional argument delim to be set to the delimiter in question. # reading data from a file where cells are (unconventionally) delimited by string &quot;|&quot; data_from_weird_file &lt;- read_delim(&quot;WEIRD_DATA_FILE.TXT&quot;, delim = &quot;|&quot;) Other packages help with reading data from and writing data to other file types, such as excel sheets. Look at the data I/O cheat sheet for more information.↩︎ "],["Chap-02-02-data-tidy-data.html", "4.2 Tidy data", " 4.2 Tidy data The same data can be represented in multiple ways. There is even room for variance in the class of rectangular representations of data. Some manners of representations are more useful for certain purposes than for others. For data analysis (plotting, statistical analyses) we prefer to represent our data as (rectangular) tidy data. A concise rationale for using tidy data is given in Figure 4.1. Figure 4.1: Artwork by allison_horst 4.2.1 Running example Consider the example of student grades for two exams in a course. A compact way of representing the data for visual digestion is the following representation: exam_results_visual &lt;- tribble( ~exam, ~&quot;Rozz&quot;, ~&quot;Andrew&quot;, ~&quot;Siouxsie&quot;, &quot;midterm&quot;, &quot;1.3&quot;, &quot;2.0&quot;, &quot;1.7&quot;, &quot;final&quot; , &quot;2.3&quot;, &quot;1.7&quot;, &quot;1.0&quot; ) exam_results_visual ## # A tibble: 2 × 4 ## exam Rozz Andrew Siouxsie ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 midterm 1.3 2.0 1.7 ## 2 final 2.3 1.7 1.0 This is how such data would frequently be represented, e.g., in tables in a journal. Indeed, Rmarkdown helps us present this data in an appetizing manner, e.g., in Table 4.1, which is produced by the code below: knitr::kable( exam_results_visual, caption = &quot;Fictitious exam results of fictitious students.&quot;, booktabs = TRUE ) Table 4.1: Fictitious exam results of fictitious students. exam Rozz Andrew Siouxsie midterm 1.3 2.0 1.7 final 2.3 1.7 1.0 Though highly perspicuous, this representation of the data is not tidy, in the special technical sense we endorse here. A tidy representation of the course results could be this: exam_results_tidy &lt;- tribble( ~student, ~exam, ~grade, &quot;Rozz&quot;, &quot;midterm&quot;, 1.3, &quot;Andrew&quot;, &quot;midterm&quot;, 2.0, &quot;Siouxsie&quot;, &quot;midterm&quot;, 1.7, &quot;Rozz&quot;, &quot;final&quot;, 2.3, &quot;Andrew&quot;, &quot;final&quot;, 1.7, &quot;Siouxsie&quot;, &quot;final&quot;, 1.0 ) exam_results_tidy ## # A tibble: 6 × 3 ## student exam grade ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Rozz midterm 1.3 ## 2 Andrew midterm 2 ## 3 Siouxsie midterm 1.7 ## 4 Rozz final 2.3 ## 5 Andrew final 1.7 ## 6 Siouxsie final 1 4.2.2 Definition of tidy data Following Wickham (2014), a tidy representation of (rectangular) data is defined as one where: each variable forms a column, each observation forms a row, and each type of observational unit forms a table. Any data set that is not tidy is messy data. Messy data that satisfies the first two constraints, but not the third will be called almost tidy data in this course. We will work, wherever possible, with data that is at least almost tidy. Figure 4.2 shows a graphical representation of the concept of tidy data. Figure 4.2: Organization of tidy data (taken from Wickham and Grolemund (2016)). 4.2.3 Excursion: non-redundant data The final condition in the definition of tidy data is not particularly important for us here (since we will make do with ‘almost tidy data’), but to understand it nonetheless consider the following data set: exam_results_overloaded &lt;- tribble( ~student, ~stu_number, ~exam, ~grade, &quot;Rozz&quot;, &quot;666&quot;, &quot;midterm&quot;, 1.3, &quot;Andrew&quot;, &quot;1969&quot;, &quot;midterm&quot;, 2.0, &quot;Siouxsie&quot;, &quot;3.14&quot;, &quot;midterm&quot;, 1.7, &quot;Rozz&quot;, &quot;666&quot;, &quot;final&quot;, 2.3, &quot;Andrew&quot;, &quot;1969&quot;, &quot;final&quot;, 1.7, &quot;Siouxsie&quot;, &quot;3.14&quot;, &quot;final&quot;, 1.0 ) exam_results_overloaded ## # A tibble: 6 × 4 ## student stu_number exam grade ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Rozz 666 midterm 1.3 ## 2 Andrew 1969 midterm 2 ## 3 Siouxsie 3.14 midterm 1.7 ## 4 Rozz 666 final 2.3 ## 5 Andrew 1969 final 1.7 ## 6 Siouxsie 3.14 final 1 This table is not tidy in an intuitive sense because it includes redundancy. Why list the student numbers twice, once with each observation of exam score? The table is not tidy in the technical sense that not every observational unit forms a table, i.e., the observation of student numbers and the observation of exam scores should be stored independently in different tables, like so: # same as before exam_results_tidy &lt;- tribble( ~student, ~exam, ~grade, &quot;Rozz&quot;, &quot;midterm&quot;, 1.3, &quot;Andrew&quot;, &quot;midterm&quot;, 2.0, &quot;Siouxsie&quot;, &quot;midterm&quot;, 1.7, &quot;Rozz&quot;, &quot;final&quot;, 2.3, &quot;Andrew&quot;, &quot;final&quot;, 1.7, &quot;Siouxsie&quot;, &quot;final&quot;, 1.0 ) # additional table with student numbers student_numbers &lt;- tribble( ~student, ~student_number, &quot;Rozz&quot;, &quot;666&quot;, &quot;Andrew&quot;, &quot;1969&quot;, &quot;Siouxsie&quot;, &quot;3.14&quot; ) Notice that, although the information is distributed over two tibbles, it is linked by the common column student. If we really need to bring all of the information together, the tidyverse has a quick and elegant solution: full_join(exam_results_tidy, student_numbers, by = &quot;student&quot;) ## # A tibble: 6 × 4 ## student exam grade student_number ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Rozz midterm 1.3 666 ## 2 Andrew midterm 2 1969 ## 3 Siouxsie midterm 1.7 3.14 ## 4 Rozz final 2.3 666 ## 5 Andrew final 1.7 1969 ## 6 Siouxsie final 1 3.14 Exercise 4.1: Tidy or Untidy? Lets take a look at this made up data set: data &lt;- tribble( ~subject_id, ~choices, ~reaction_times, 1, &quot;A,B,B&quot;, &quot;312 433 365&quot;, 2, &quot;B,A,B&quot;, &quot;393 491 327&quot;, 3, &quot;B,A,A&quot;, &quot;356 313 475&quot;, 4, &quot;A,B,B&quot;, &quot;292 352 378&quot;) Is this data tidy or untidy? Explain your reasoning. Solution This data is untidy for given reasons: Each row contains more than one observation. Most fields contain more than one value. References "],["Chap-02-02-data-preprocessing-cleaning.html", "4.3 Data manipulation: the basics", " 4.3 Data manipulation: the basics 4.3.1 Pivoting The tidyverse strongly encourages the use of tidy data, or at least almost tidy data. If your data is (almost) tidy, you can be reasonably sure that you can plot and analyze the data without additional wrangling. If your data is not (almost) tidy because it is too wide or too long (see below), what is required is a joyful round of pivoting. There are two directions of pivoting: making data longer, and making data wider. 4.3.1.1 Making too wide data longer with pivot_longer Consider the previous example of messy data again: exam_results_visual &lt;- tribble( ~exam, ~&quot;Rozz&quot;, ~&quot;Andrew&quot;, ~&quot;Siouxsie&quot;, &quot;midterm&quot;, &quot;1.3&quot;, &quot;2.0&quot;, &quot;1.7&quot;, &quot;final&quot; , &quot;2.3&quot;, &quot;1.7&quot;, &quot;1.0&quot; ) exam_results_visual ## # A tibble: 2 × 4 ## exam Rozz Andrew Siouxsie ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 midterm 1.3 2.0 1.7 ## 2 final 2.3 1.7 1.0 This data is “too wide”. We can make it longer with the function pivot_longer from the tidyr package. Check out the example below before we plunge into a description of pivot_longer. exam_results_visual %&gt;% pivot_longer( # pivot every column except the first # (a negative number here means &quot;exclude column with that index number&quot;) cols = - 1, # name of new column which contains the # names of the columns to be &quot;gathered&quot; names_to = &quot;student&quot;, # name of new column which contains the values # of the cells which now form a new column values_to = &quot;grade&quot; ) %&gt;% # optional reordering of columns (to make # the output exactly like `exam_results_tidy`) select(student, exam, grade) ## # A tibble: 6 × 3 ## student exam grade ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Rozz midterm 1.3 ## 2 Andrew midterm 2.0 ## 3 Siouxsie midterm 1.7 ## 4 Rozz final 2.3 ## 5 Andrew final 1.7 ## 6 Siouxsie final 1.0 What pivot_longer does, in general, is take a bunch of columns and gather the values of all cells in these columns into a single, new column, the so-called value column, i.e., the column with the values of the cells to be gathered. If pivot_longer stopped here, we would lose information about which cell values belonged to which original column. Therefore, pivot_longer also creates a second new column, the so-called name column, i.e., the column with the names of the original columns that we gathered together. Consequently, in order to do its job, pivot_longer minimally needs three pieces of information:18 which columns to spin around (function argument cols) the name of the to-be-created new value column (function argument values_to) the name of the to-be-created new name column (function argument names_to) For different ways of selecting columns to pivot around, see Section 4.3.3 below. 4.3.1.2 Making too long data wider with pivot_wider Consider the following example of data which is untidy because it is too long: mixed_results_too_long &lt;- tibble(student = rep(c(&#39;Rozz&#39;, &#39;Andrew&#39;, &#39;Siouxsie&#39;), times = 2), what = rep(c(&#39;grade&#39;, &#39;participation&#39;), each = 3), howmuch = c(2.7, 2.0, 1.0, 75, 93, 33)) mixed_results_too_long ## # A tibble: 6 × 3 ## student what howmuch ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Rozz grade 2.7 ## 2 Andrew grade 2 ## 3 Siouxsie grade 1 ## 4 Rozz participation 75 ## 5 Andrew participation 93 ## 6 Siouxsie participation 33 This data is untidy because it lumps two types of different measurements (a course grade, and the percentage of participation) in a single column. These are different variables, and so should be represented in different columns. To fix a data representation that is too long, we can make it wider with the help of the pivot_wider function from the tidyr package. We look at an example before looking at the general behavior of the pivot_wider function. mixed_results_too_long %&gt;% pivot_wider( # column containing the names of the new columns names_from = what, # column containing the values of the new columns values_from = howmuch ) ## # A tibble: 3 × 3 ## student grade participation ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rozz 2.7 75 ## 2 Andrew 2 93 ## 3 Siouxsie 1 33 In general, pivot_wider picks out two columns, one column of values to distribute into new to-be-created columns, and one vector of names or groups which contains the information about the, well, names of the to-be-created new columns. There are more refined options for pivot_wider, some of which we will encounter in the context of concrete cases of application. 4.3.2 Subsetting rows &amp; columns If a data set contains too much information for your current purposes, you can discard irrelevant (or unhelpful) rows and columns. The function filter takes a Boolean expression and returns only those rows of which the Boolean expression is true: exam_results_tidy %&gt;% # keep only entries with grades better than # or equal to 1.7 filter(grade &lt;= 1.7) ## # A tibble: 4 × 3 ## student exam grade ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Rozz midterm 1.3 ## 2 Siouxsie midterm 1.7 ## 3 Andrew final 1.7 ## 4 Siouxsie final 1 To select rows by an index or a vector of indeces, use the slice function: exam_results_tidy %&gt;% # keep only entries from rows with an even index slice(c(2, 4, 6)) ## # A tibble: 3 × 3 ## student exam grade ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Andrew midterm 2 ## 2 Rozz final 2.3 ## 3 Siouxsie final 1 The function select allows to pick out a subset of columns. Interestingly, it can also be used to reorder columns, because the order in which column names are specified matches the order in the returned tibble. exam_results_tidy %&gt;% # select columns `grade` and `exam` select(grade, exam) ## # A tibble: 6 × 2 ## grade exam ## &lt;dbl&gt; &lt;chr&gt; ## 1 1.3 midterm ## 2 2 midterm ## 3 1.7 midterm ## 4 2.3 final ## 5 1.7 final ## 6 1 final 4.3.3 Tidy selection of column names To select the columns in several functions within the tidyverse, such as pivot_longer or select, there are useful helper functions from the tidyselect package. Here are some examples:19 # bogus code for illustration of possibilities! SOME_DATA %&gt;% select( ... # could be one of the following # all columns indexed 2, 3, ..., 10 2:10 # all columns except the one called &quot;COLNAME&quot; - COLNAME # all columns with names starting with &quot;STRING&quot; starts_with(&quot;STRING&quot;) # all columns with names ending with &quot;STRING&quot; ends_with(&quot;STRING&quot;) # all columns with names containing &quot;STRING&quot; contains(&quot;STRING&quot;) # all columns with names of the form &quot;Col_i&quot; with i = 1, ..., 10 num_range(&quot;Col_&quot;, 1:10) ) 4.3.4 Adding, changing and renaming columns To add a new column, or to change an existing one use the function mutate, like so: exam_results_tidy %&gt;% mutate( # add a new column called &#39;passed&#39; depending on grade # [NB: severe passing conditions in this class!!] passed = grade &lt;= 1.7, # change an existing column; here: change # character column &#39;exam&#39; to ordered factor exam = factor(exam, ordered = T) ) ## # A tibble: 6 × 4 ## student exam grade passed ## &lt;chr&gt; &lt;ord&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 Rozz midterm 1.3 TRUE ## 2 Andrew midterm 2 FALSE ## 3 Siouxsie midterm 1.7 TRUE ## 4 Rozz final 2.3 FALSE ## 5 Andrew final 1.7 TRUE ## 6 Siouxsie final 1 TRUE If you want to rename a column, function rename is what you want: exam_results_tidy %&gt;% # rename existing column &quot;student&quot; to new name &quot;participant&quot; # [NB: rename takes the new name first] rename(participant = student) ## # A tibble: 6 × 3 ## participant exam grade ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Rozz midterm 1.3 ## 2 Andrew midterm 2 ## 3 Siouxsie midterm 1.7 ## 4 Rozz final 2.3 ## 5 Andrew final 1.7 ## 6 Siouxsie final 1 4.3.5 Splitting and uniting columns Here is data from course homework: homework_results_untidy &lt;- tribble( ~student, ~results, &quot;Rozz&quot;, &quot;1.0,2.3,3.0&quot;, &quot;Andrew&quot;, &quot;2.3,2.7,1.3&quot;, &quot;Siouxsie&quot;, &quot;1.7,4.0,1.0&quot; ) This is not a useful representation format. Results of three homework sets are mushed together in a single column. Each value is separated by a comma, but it is all stored as a character vector. To disentangle information in a single column, use the separate function: homework_results_untidy %&gt;% separate( # which column to split up col = results, # names of the new column to store results into = str_c(&quot;HW_&quot;, 1:3), # separate by which character / reg-exp sep = &quot;,&quot;, # automatically (smart-)convert the type of the new cols convert = T ) ## # A tibble: 3 × 4 ## student HW_1 HW_2 HW_3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rozz 1 2.3 3 ## 2 Andrew 2.3 2.7 1.3 ## 3 Siouxsie 1.7 4 1 If you have a reason to perform the reverse operation, i.e., join together several columns, use the unite function. 4.3.6 Sorting a data set If you want to indicate a fixed order of the reoccurring elements in a (character) vector, e.g., for plotting in a particular order, you should make this column an ordered factor. But if you want to order a data set along a column, e.g., for inspection or printing as a table, then you can do that by using the arrange function. You can specify several columns to sort alpha-numerically in ascending order, and also indicate a descending order using the desc function: exam_results_tidy %&gt;% arrange(desc(student), grade) ## # A tibble: 6 × 3 ## student exam grade ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Siouxsie final 1 ## 2 Siouxsie midterm 1.7 ## 3 Rozz midterm 1.3 ## 4 Rozz final 2.3 ## 5 Andrew final 1.7 ## 6 Andrew midterm 2 4.3.7 Combining tibbles There are frequent occasions on which data from two separate variables need to be combined. The simplest case is where two entirely disjoint data sets merely need to be glued together, either horizontally (binding columns together with function cbind) or vertically (binding rows together with function rbind). new_exam_results_tidy &lt;- tribble( ~student, ~exam, ~grade, &quot;Rozz&quot;, &quot;bonus&quot;, 1.7, &quot;Andrew&quot;, &quot;bonus&quot;, 2.3, &quot;Siouxsie&quot;, &quot;bonus&quot;, 1.0 ) rbind( exam_results_tidy, new_exam_results_tidy ) ## # A tibble: 9 × 3 ## student exam grade ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Rozz midterm 1.3 ## 2 Andrew midterm 2 ## 3 Siouxsie midterm 1.7 ## 4 Rozz final 2.3 ## 5 Andrew final 1.7 ## 6 Siouxsie final 1 ## 7 Rozz bonus 1.7 ## 8 Andrew bonus 2.3 ## 9 Siouxsie bonus 1 If two data sets have information in common, and the combination should respect that commonality, the join family of functions is of great help. Consider the case of distributed information again that we looked at to understand the third constraint of the concept of “tidy data”. There are two tibbles, both of which contain information about the same students. They share the column student (this does not necessarily have to be in the same order!) and we might want to join the information from both sources into a single (messy but almost tidy) representation, using full_join. We have seen an example already, which is repeated here: # same as before exam_results_tidy &lt;- tribble( ~student, ~exam, ~grade, &quot;Rozz&quot;, &quot;midterm&quot;, 1.3, &quot;Andrew&quot;, &quot;midterm&quot;, 2.0, &quot;Siouxsie&quot;, &quot;midterm&quot;, 1.7, &quot;Rozz&quot;, &quot;final&quot;, 2.3, &quot;Andrew&quot;, &quot;final&quot;, 1.7, &quot;Siouxsie&quot;, &quot;final&quot;, 1.0 ) # additional table with student numbers student_numbers &lt;- tribble( ~student, ~student_number, &quot;Rozz&quot;, &quot;666&quot;, &quot;Andrew&quot;, &quot;1969&quot;, &quot;Siouxsie&quot;, &quot;3.14&quot; ) full_join(exam_results_tidy, student_numbers, by = &quot;student&quot;) ## # A tibble: 6 × 4 ## student exam grade student_number ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Rozz midterm 1.3 666 ## 2 Andrew midterm 2 1969 ## 3 Siouxsie midterm 1.7 3.14 ## 4 Rozz final 2.3 666 ## 5 Andrew final 1.7 1969 ## 6 Siouxsie final 1 3.14 If two data sets are to be joined by a column that is not exactly shared by both sets (one contains entries in this column that the other doesn’t) then a full_join will retain all information from both. If that is not what you want, check out alternative functions like right_join, semi_join etc. using the data wrangling cheat sheet. Exercise 4.2: Data Wrangling in R We are working with the same example as in the earlier exercise: data &lt;- tribble( ~subject_id, ~choices, ~reaction_times, 1, &quot;A,B,B&quot;, &quot;312 433 365&quot;, 2, &quot;B,A,B&quot;, &quot;393 491 327&quot;, 3, &quot;B,A,A&quot;, &quot;356 313 475&quot;, 4, &quot;A,B,B&quot;, &quot;292 352 378&quot; ) Take a look at the following code snippet. Explain what the individual parts (indicated by the numbers) do. What will the result look like? choice_data &lt;- data %&gt;% #1 select(subject_id, choices) %&gt;% #2 separate( col = choices, into = str_c(&quot;C_&quot;, 1:3), sep = &quot;,&quot;) %&gt;% #3 pivot_longer( cols = -1, names_to = &quot;condition&quot;, values_to = &quot;response&quot;) Solution Selecting two columns (subject_id and choices) out of the data set. In the data set, each cell in the choices column contains more than one value. To separate them, we take this column and divide the strings by the “,”. The names are then given for each line from one to three. Now we are making the data set longer, so that each condition is its own row. We are pivoting each column apart from the first. The names of the columns are combined in a column called condition and the values are put into a column called response. The result: choice_data ## # A tibble: 12 × 3 ## subject_id condition response ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 C_1 A ## 2 1 C_2 B ## 3 1 C_3 B ## 4 2 C_1 B ## 5 2 C_2 A ## 6 2 C_3 B ## 7 3 C_1 B ## 8 3 C_2 A ## 9 3 C_3 A ## 10 4 C_1 A ## 11 4 C_2 B ## 12 4 C_3 B There are alternative possibilities for specifying names of the value and name column, which allow for more dynamic construction of strings. We will not cover all of these details here, but we will use some of these alternative specifications in subsequent examples.↩︎ The helpers from the tidyselect package also accept regular expressions.↩︎ "],["Chap-02-02-data-grouping-nesting.html", "4.4 Grouped operations", " 4.4 Grouped operations A frequently occurring problem in data analysis is to obtain a summary statistic (see Chapter 5) for different subsets of data. For example, we might want to calculate the average score for each student in our class. We could do that by filtering like so (notice that pull gives you the column vector specified): # extracting mean grade for Rozz mean_grade_Rozz &lt;- exam_results_tidy %&gt;% filter(student == &quot;Rozz&quot;) %&gt;% pull(grade) %&gt;% mean mean_grade_Rozz ## [1] 1.8 But then we need to do that two more times. So, as we shouldn’t copy-paste code, we write a function and use map_dbl to add a mean for each student: get_mean_for_student &lt;- function(student_name) { exam_results_tidy %&gt;% filter(student == student_name) %&gt;% pull(grade) %&gt;% mean } map_dbl( exam_results_tidy %&gt;% pull(student) %&gt;% unique, get_mean_for_student ) ## [1] 1.80 1.85 1.35 Also not quite satisfactory, clumsy and error-prone. Enter, grouping in the tidyverse. If we want to apply a particular operation to all combinations of levels of different variables (no matter whether they are encoded as factors or not when we group), we can do this with the function group_by, followed by either a call to mutate or summarise. Check this example: exam_results_tidy %&gt;% group_by(student) %&gt;% summarise( student_mean = mean(grade) ) ## # A tibble: 3 × 2 ## student student_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Andrew 1.85 ## 2 Rozz 1.8 ## 3 Siouxsie 1.35 The function summarise returns a single row for each combination of levels of grouping variables. If we use the function mutate instead, the summary statistic is added (repeatedly) in each of the original rows: exam_results_tidy %&gt;% group_by(student) %&gt;% mutate( student_mean = mean(grade) ) ## # A tibble: 6 × 4 ## # Groups: student [3] ## student exam grade student_mean ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rozz midterm 1.3 1.8 ## 2 Andrew midterm 2 1.85 ## 3 Siouxsie midterm 1.7 1.35 ## 4 Rozz final 2.3 1.8 ## 5 Andrew final 1.7 1.85 ## 6 Siouxsie final 1 1.35 The latter can sometimes be handy, for example when overlaying a plot of the data with grouped means, for instance. It may be important to remember that after a call of group_by, the resulting tibbles retains the grouping information for all subsequent operations. To remove grouping information, use the function ungroup. "],["Chap-02-02-data-case-study-KoF.html", "4.5 Case study: the King of France", " 4.5 Case study: the King of France Let’s go through one case study of data preprocessing and cleaning. We look at the example introduced and fully worked out in Appendix ??. (Please read Section ?? to find out more about where this data set is coming from.) The raw data set is part of the aida package and can be loaded using: data_KoF_raw &lt;- read_csv(&quot;./data_sets/king-of-france_data_raw.csv&quot;) # aida::data_KoF_raw We then take a glimpse at the data: glimpse(data_KoF_raw ) ## Rows: 2,813 ## Columns: 16 ## $ submission_id &lt;dbl&gt; 192, 192, 192, 192, 192, 192, 192, 192, 192, 192, 192, … ## $ RT &lt;dbl&gt; 8110, 35557, 3647, 16037, 11816, 6024, 4986, 13019, 538… ## $ age &lt;dbl&gt; 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57,… ## $ comments &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ item_version &lt;chr&gt; &quot;none&quot;, &quot;none&quot;, &quot;none&quot;, &quot;none&quot;, &quot;none&quot;, &quot;none&quot;, &quot;none&quot;,… ## $ correct_answer &lt;lgl&gt; FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FAL… ## $ education &lt;chr&gt; &quot;Graduated College&quot;, &quot;Graduated College&quot;, &quot;Graduated Co… ## $ gender &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;fema… ## $ languages &lt;chr&gt; &quot;English&quot;, &quot;English&quot;, &quot;English&quot;, &quot;English&quot;, &quot;English&quot;, … ## $ question &lt;chr&gt; &quot;World War II was a global war that lasted from 1914 to… ## $ response &lt;lgl&gt; FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FAL… ## $ timeSpent &lt;dbl&gt; 39.48995, 39.48995, 39.48995, 39.48995, 39.48995, 39.48… ## $ trial_name &lt;chr&gt; &quot;practice_trials&quot;, &quot;practice_trials&quot;, &quot;practice_trials&quot;… ## $ trial_number &lt;dbl&gt; 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1… ## $ trial_type &lt;chr&gt; &quot;practice&quot;, &quot;practice&quot;, &quot;practice&quot;, &quot;practice&quot;, &quot;practi… ## $ vignette &lt;chr&gt; &quot;undefined&quot;, &quot;undefined&quot;, &quot;undefined&quot;, &quot;undefined&quot;, &quot;un… The variables in this data set are: submission_id: unique identifier for each participant RT: the reaction time for each decision age: the (self-reported) age of the participant comments: the (optional) comments each participant may have given item_version: the condition which the test sentence belongs to (only given for trials of type main and special) correct_answer: for trials of type filler and special what the true answer should have been education: the (self-reported) education level with options Graduated College, Graduated High School, Higher Degree gender: (self-reported) gender languages: (self-reported) native languages question: the sentence to be judged true or false response: the answer (“TRUE” or “FALSE”) on each trial trial_name: whether the trial is a main or practice trials (levels main_trials and practice_trials) trial_number: consecutive numbering of each participant’s trial trial_type: whether the trial was of the category filler, main, practice or special, where the latter encodes the “background checks” vignette: the current item’s vignette number (applies only to trials of type main and special) Let’s have a brief look at the comments (sometimes helpful, usually entertaining) and the self-reported native languages: data_KoF_raw %&gt;% pull(comments) %&gt;% unique ## [1] NA ## [2] &quot;I hope I was right most of the time!&quot; ## [3] &quot;My level of education is Some Highschool, not finished. So I couldn&#39;t input what was correct, so I&#39;m leaving a comment here.&quot; ## [4] &quot;It was interesting, and made re-read questions to make sure they weren&#39;t tricks. I hope I got them all correct.&quot; ## [5] &quot;Worked well&quot; ## [6] &quot;A surprisingly tricky study! Thoroughly enjoyed completing it, despite several red herrings!!&quot; ## [7] &quot;n/a&quot; ## [8] &quot;Thank you for the opportunity.&quot; ## [9] &quot;this was challenging&quot; ## [10] &quot;I&#39;m not good at learning history so i might of made couple of mistakes. I hope I did well. :)&quot; ## [11] &quot;Interesting survey - thanks!&quot; ## [12] &quot;no&quot; ## [13] &quot;Regarding the practice question - I&#39;m aware that Alexander Bell invented the telephone, but in reality, it was a collaborative effort by a team of people&quot; ## [14] &quot;Fun study!&quot; ## [15] &quot;Fun stuff&quot; data_KoF_raw %&gt;% pull(languages) %&gt;% unique ## [1] &quot;English&quot; &quot;english&quot; &quot;English, Italian&quot; ## [4] &quot;English/ ASL&quot; &quot;English and Polish&quot; &quot;Chinese&quot; ## [7] &quot;English, Mandarin&quot; &quot;Polish&quot; &quot;Turkish&quot; ## [10] NA &quot;English, Sarcasm&quot; &quot;English, Portuguese&quot; We might wish to exclude people who do not include “English” as one of their native languages in some studies. Here, we do not since we also have strong, more specific filters on comprehension (see below). Since we are not going to use this information later on, we might as well discard it now: data_KoF_raw &lt;- data_KoF_raw %&gt;% select(-languages, -comments, -age, -RT, -education, -gender) But even after pruning irrelevant columns, this data set is still not ideal. We need to preprocess it more thoroughly to make it more intuitively manageable. For example, the information in column trial_name does not give the trial’s name in an intuitive sense, but its type: whether it is a practice or a main trial. But this information, and more, is also represented in the column trial_type. The column item_version contains information about the experimental condition. To see this (mess), the code below prints the selected information from the main trials of only one participant in an order that makes it easier to see what is what. data_KoF_raw %&gt;% # ignore practice trials for the moment # focus on one participant only filter(trial_type != &quot;practice&quot;, submission_id == 192) %&gt;% select(trial_type, item_version, question) %&gt;% arrange(desc(trial_type), item_version) %&gt;% print(n = Inf) ## # A tibble: 24 × 3 ## trial_type item_version question ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 special none The Pope is currently not married. ## 2 special none Germany has volcanoes. ## 3 special none France has a king. ## 4 special none Canada is a democracy. ## 5 special none Belgium has rainforests. ## 6 main 0 The volcanoes of Germany dominate the landscape. ## 7 main 1 Canada has an emperor, and he is fond of sushi. ## 8 main 10 Donald Trump, his favorite nature spot is not the Be… ## 9 main 6 The King of France isn’t bald. ## 10 main 9 The Pope’s wife, she did not invite Angela Merkel fo… ## 11 filler none The Solar System includes the planet Earth. ## 12 filler none Vatican City is the world&#39;s largest country by land … ## 13 filler none Big Ben is a very large building in the middle of Pa… ## 14 filler none Harry Potter is a series of fantasy novels written b… ## 15 filler none Taj Mahal is a mausoleum on the bank of the river in… ## 16 filler none James Bond is a spanish dancer from Madrid. ## 17 filler none The Pacific Ocean is a large ocean between Japan and… ## 18 filler none Australia has a very large border with Brazil. ## 19 filler none Steve Jobs was an American inventor and co-founder o… ## 20 filler none Planet Earth is part of the galaxy ‘Milky Way’. ## 21 filler none Germany shares borders with France, Belgium and Denm… ## 22 filler none Antarctica is a continent covered almost completely … ## 23 filler none The Statue of Liberty is a colossal sculpture on Lib… ## 24 filler none English is the main language in Australia, Britain a… We see that the information in item_version specifies the critical condition. To make this more intuitively manageable, we would like to have a column called condition and it should, ideally, also contain useful information for the cases where trial_type is not main or special. That is why we will therefore remove the column trial_name completely, and create an informative column condition in which we learn of every row whether it belongs to one of the five experimental conditions, and if not whether it is a filler or a “background check” (= special) trial. data_KoF_processed &lt;- data_KoF_raw %&gt;% # drop redundant information in column `trial_name` select(-trial_name) %&gt;% # discard practice trials filter(trial_type != &quot;practice&quot;) %&gt;% mutate( # add a &#39;condition&#39; variable condition = case_when( trial_type == &quot;special&quot; ~ &quot;background check&quot;, trial_type == &quot;main&quot; ~ str_c(&quot;Condition &quot;, item_version), TRUE ~ &quot;filler&quot; ) %&gt;% # make the new &#39;condition&#39; variable a factor factor( ordered = T, levels = c( str_c(&quot;Condition &quot;, c(0, 1, 6, 9, 10)), &quot;background check&quot;, &quot;filler&quot; ) ) ) 4.5.1 Cleaning the data We clean the data in two consecutive steps: Remove all data from any participant who got more than 50% of the answers to the filler material wrong. Remove individual main trials if the corresponding “background check” question was answered wrongly. 4.5.1.1 Cleaning by-participant # look at error rates for filler sentences by subject # mark every subject as an outlier when they # have a proportion of correct responses of less than 0.5 subject_error_rate &lt;- data_KoF_processed %&gt;% filter(trial_type == &quot;filler&quot;) %&gt;% group_by(submission_id) %&gt;% summarise( proportion_correct = mean(correct_answer == response), outlier_subject = proportion_correct &lt; 0.5 ) %&gt;% arrange(proportion_correct) Apply the cleaning step: # add info about error rates and exclude outlier subject(s) d_cleaned &lt;- full_join(data_KoF_processed, subject_error_rate, by = &quot;submission_id&quot;) %&gt;% filter(outlier_subject == FALSE) 4.5.1.2 Cleaning by-trial # exclude every critical trial whose &#39;background&#39; test question was answered wrongly d_cleaned &lt;- d_cleaned %&gt;% # select only the &#39;background question&#39; trials filter(trial_type == &quot;special&quot;) %&gt;% # is the background question answered correctly? mutate( background_correct = correct_answer == response ) %&gt;% # select only the relevant columns select(submission_id, vignette, background_correct) %&gt;% # right join lines to original data set right_join(d_cleaned, by = c(&quot;submission_id&quot;, &quot;vignette&quot;)) %&gt;% # remove all special trials, as well as main trials with incorrect background check filter(trial_type == &quot;main&quot; &amp; background_correct == TRUE) For later reuse, both the preprocessed and the cleaned data set are included in the aida package as well. They are loaded by calling aida::data_KoF_preprocessed and aida::data_KoF_cleaned, respectively. "],["Chap-02-03-summary-statistics.html", "5 Summary statistics", " 5 Summary statistics A summary statistic is a single number that represents one aspect of a possibly much more complex chunk of data. This single number might, for example, indicate the maximum or minimum value of a vector of one billion observations. The large data set (one billion observations) is reduced to a single number which represents one aspect of that data. Summary statistics are, as a general (but violable) rule, many-to-one surjections. They compress complex information into a simpler, compressed representation. Summary statistics are useful for understanding the data at hand, for communication about a data set, but also for subsequent statistical analyses. As we will see later on, many statistical tests look at a summary statistic \\(x\\), which is a single value derived from data set \\(D\\), and compare \\(x\\) to an expectation of what \\(x\\) should be like if the process that generated \\(D\\) really had a particular property. For the moment, however, we use summary statistics only to get comfortable with data: understanding it better and gaining competence to manipulate it. Section 5.1 first uses the Bio-Logic Jazz-Metal data set to look at a very intuitive class of summary statistics for categorical data, namely counts and proportions. Section 5.2 introduces summary statistics for simple, one-dimensional vectors with numeric information. Section 5.3 looks at measures of the relation between two numerical vectors, namely covariance and correlation. These last two sections use the avocado data set. The learning goals for this chapter are: become able to compute counts and frequencies for categorical data understand and be able to compute summary statistics for one-dimensional metric data: measures of central tendency mean, mode, median measures of dispersion variance, standard deviation, quantiles non-parametric estimates of confidence bootstrapped CI of the mean understand and be able to compute for two-dimensional metric data: covariance Bravais-Pearson correlation "],["Chap-02-03-summary-statistics-counts.html", "5.1 Counts and proportions", " 5.1 Counts and proportions Very familiar instances of summary statistics are counts and frequencies. While there is no conceptual difficulty in understanding these numerical measures, we have yet to see how to obtain counts for categorical data in R. The Bio-Logic Jazz-Metal data set provides nice material for doing so. If you are unfamiliar with the data and the experiment that generated it, please have a look at Appendix Chapter ??. 5.1.1 Loading and inspecting the data We load the preprocessed data immediately (see Appendix ?? for details on how this preprocessing was performed). data_BLJM_processed &lt;- read_csv(&quot;./data_sets/bio-logic-jazz-metal-data-processed.csv&quot;) # aida::data_BLJM The preprocessed data lists, for each participant (in column submission_id) the binary choice (in column response) in a particular condition (in column condition). head(data_BLJM_processed) ## # A tibble: 6 × 3 ## submission_id condition response ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 379 BM Beach ## 2 379 LB Logic ## 3 379 JM Metal ## 4 378 JM Metal ## 5 378 LB Logic ## 6 378 BM Beach 5.1.2 Obtaining counts with n, count and tally To obtain counts, the dplyr package offers the functions n, count and tally, among others.20 The function n does not take arguments and is useful for counting rows. It works inside of summarise and mutate and is usually applied to grouped data sets. For example, we can get a count of how many observations the data in data_BLJM_processed contains for each condition by first grouping by variable condition and then calling n (without arguments) inside of summarise: data_BLJM_processed %&gt;% group_by(condition) %&gt;% summarise(nr_observation_per_condition = n()) %&gt;% ungroup() ## # A tibble: 3 × 2 ## condition nr_observation_per_condition ## &lt;chr&gt; &lt;int&gt; ## 1 BM 102 ## 2 JM 102 ## 3 LB 102 Notice that calling n without grouping just gives you the number of rows in the data set: data_BLJM_processed %&gt;% summarize(n_rows = n()) ## # A tibble: 1 × 1 ## n_rows ## &lt;int&gt; ## 1 306 This can also be obtained simply by (although in a different output format!): nrow(data_BLJM_processed) ## [1] 306 Counting can be helpful also when getting acquainted with a data set, or when checking whether the data is complete. For example, we can verify that every participant in the experiment contributed three data points like so: data_BLJM_processed %&gt;% group_by(submission_id) %&gt;% summarise(nr_data_points = n()) ## # A tibble: 102 × 2 ## submission_id nr_data_points ## &lt;dbl&gt; &lt;int&gt; ## 1 278 3 ## 2 279 3 ## 3 280 3 ## 4 281 3 ## 5 282 3 ## 6 283 3 ## 7 284 3 ## 8 285 3 ## 9 286 3 ## 10 287 3 ## # … with 92 more rows The functions tally and count are essentially just convenience wrappers around n. While tally expects that the data is already grouped in the relevant way, count takes a column specification as an argument and does the grouping (and final ungrouping) implicitly. For instance, the following code blocks produce the same output, one using n, the other using count, namely the total number of times a particular response has been given in a particular condition: data_BLJM_processed %&gt;% group_by(condition, response) %&gt;% summarise(n = n()) ## # A tibble: 6 × 3 ## # Groups: condition [3] ## condition response n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 BM Beach 44 ## 2 BM Mountains 58 ## 3 JM Jazz 64 ## 4 JM Metal 38 ## 5 LB Biology 58 ## 6 LB Logic 44 data_BLJM_processed %&gt;% # use function `count` from `dplyr` package dplyr::count(condition, response) ## # A tibble: 6 × 3 ## condition response n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 BM Beach 44 ## 2 BM Mountains 58 ## 3 JM Jazz 64 ## 4 JM Metal 38 ## 5 LB Biology 58 ## 6 LB Logic 44 So, these counts suggest that there is an overall preference for mountains over beaches, Jazz over Metal and Biology over Logic. Who would have known!? These counts are overall numbers. They do not tell us anything about any potentially interesting relationship between preferences. So, let’s have a closer look at the number of people who selected which music-subject pair. We collect these counts in variable BLJM_associated_counts. We first need to pivot the data, using pivot_wider, to make sure each participant’s choices are associated with each other, and then take the counts of interest: BLJM_associated_counts &lt;- data_BLJM_processed %&gt;% pivot_wider(names_from = condition, values_from = response) %&gt;% # drop the Beach vs. Mountain condition select(-BM) %&gt;% dplyr::count(JM, LB) BLJM_associated_counts ## # A tibble: 4 × 3 ## JM LB n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Jazz Biology 38 ## 2 Jazz Logic 26 ## 3 Metal Biology 20 ## 4 Metal Logic 18 We can also produce a table of proportions from this, simply by dividing the column called n by the total number of observations using sum(n). We can also flip the table around into a more convenient (though messy) representation: BLJM_associated_counts %&gt;% # look at relative frequency, not total counts mutate(n = n / sum(n)) %&gt;% pivot_wider(names_from = LB, values_from = n) ## # A tibble: 2 × 3 ## JM Biology Logic ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jazz 0.373 0.255 ## 2 Metal 0.196 0.176 Eye-balling this table of relative frequencies, we might indeed hypothesize that preference for musical style is not independent of preference for an academic subject. The impression is corroborated by looking at the plot in Figure 5.1. More on this later! Figure 5.1: Proportions of jointly choosing a musical style and an academic subfield in the Bio-Logic Jazz-Metal data set. Useful base R functions for obtaining counts are table and prop.table.↩︎ "],["Chap-02-03-summary-statistics-1D.html", "5.2 Central tendency and dispersion", " 5.2 Central tendency and dispersion This section will look at two types of summary statistics: measures of central tendency and measures of dispersion. Measures of central tendency map a vector of observations onto a single number that represents, roughly put, “the center”. Since what counts as a “center” is ambiguous, there are several measures of central tendencies. Different measures of central tendencies can be more or less adequate for one purpose or another. The type of variable (nominal, ordinal or metric, for instance) will also influence the choice of measure. We will visit three prominent measures of central tendency here: (arithmetic) mean, median and mode. Measures of dispersion indicate how much the observations are spread out around, let’s say, “a center”. We will visit three prominent measures of dispersion: the variance, the standard deviation and quantiles. To illustrate these ideas, consider the case of a numeric vector of observations. Central tendency and dispersion together describe a (numeric) vector by giving indicative information about the point around which the observations spread, and how far away from that middle point they tend to lie. Fictitious examples of observation vectors with higher or lower central tendency and higher or lower dispersion are given in Figure 5.2. Figure 5.2: Fictitious data points with higher/lower central tendencies and higher/lower dispersion. NB: The points are ‘jittered’ along the vertical dimension for better visibility; only the horizontal dimension is relevant here. 5.2.1 The data for the remainder of the chapter In the remainder of this chapter, we will use the avocado data set, a very simple and theory-free example in which we can explore two metric variables: the average price at which avocados were sold during specific intervals of time and the total amount of avocados sold. We load the (pre-processed) data into a variable named avocado_data (see Appendix ?? for more information on this data set): avocado_data &lt;- read_csv(&quot;./data_sets/avocado-processed.csv&quot;) # aida::data_avocado We can then take a glimpse: glimpse(avocado_data) ## Rows: 18,249 ## Columns: 7 ## $ Date &lt;date&gt; 2015-12-27, 2015-12-20, 2015-12-13, 2015-12-06, 201… ## $ average_price &lt;dbl&gt; 1.33, 1.35, 0.93, 1.08, 1.28, 1.26, 0.99, 0.98, 1.02… ## $ total_volume_sold &lt;dbl&gt; 64236.62, 54876.98, 118220.22, 78992.15, 51039.60, 5… ## $ small &lt;dbl&gt; 1036.74, 674.28, 794.70, 1132.00, 941.48, 1184.27, 1… ## $ medium &lt;dbl&gt; 54454.85, 44638.81, 109149.67, 71976.41, 43838.39, 4… ## $ large &lt;dbl&gt; 48.16, 58.33, 130.50, 72.58, 75.78, 43.61, 93.26, 80… ## $ type &lt;chr&gt; &quot;conventional&quot;, &quot;conventional&quot;, &quot;conventional&quot;, &quot;con… The columns that will interest us the most in this chapter are: average_price - average price of a single avocado total_volume_sold - total number of avocados sold type - whether the price/amount is for a conventional or an organic avocado In particular, we will look at summary statistics for average_price and total_volume_sold, either for the whole data set or independently for each type of avocado. Notice that both of these variables are numeric. They are vectors of numbers, each representing an observation. 5.2.2 Measures of central tendency 5.2.2.1 The (arithmetic) mean If \\(\\vec{x} = \\langle x_1, \\dots , x_n \\rangle\\) is a vector of \\(n\\) observations with \\(x_i \\in \\mathbb{R}\\) for all \\(1 \\le i \\le n\\), the (arithmetic) mean of \\(x\\), written \\(\\mu_{\\vec{x}}\\), is defined as \\[\\mu_{\\vec{x}} = \\frac{1}{n}\\sum_{i=1}^n x_i\\,.\\] The arithmetic mean can be understood intuitively as the center of gravity. If we place a marble on a wooden board for every \\(x_i\\), such that every marble is equally heavy and the differences between all data measurements are identical to the distances between the marbles, the arithmetic mean is where you can balance the board with the tip of your finger. Example. The mean of the vector \\(\\vec{x} = \\langle 0, 3, 6, 7\\rangle\\) is \\(\\mu_{\\vec{x}} = \\frac{0 + 3 + 6 + 7}{4} = \\frac{16}{4} = 4\\,.\\) The black dots in the graph below show the data observations, and the red cross indicates the mean. Notice that the mean is clearly not the mid-point between the maximum and the minimum (which here would be 3.5). To calculate the mean of a large vector, R has a built-in function mean, which we have in fact used frequently before. Let’s use it to calculate the mean of the variable average_price for different types of avocados: avocado_data %&gt;% group_by(type) %&gt;% summarise( mean_price = mean(average_price) ) ## # A tibble: 2 × 2 ## type mean_price ## &lt;chr&gt; &lt;dbl&gt; ## 1 conventional 1.16 ## 2 organic 1.65 Unsurprisingly, the overall mean of the observed prices is (numerically) higher for organic avocados than for conventional ones. Excursion. It is also possible to conceptualize the arithmetic mean as the expected value when sampling from the observed data. This is useful for linking the mean of a data sample to the expected value of a random variable, a concept we will introduce in Chapter 7. Suppose you have gathered the data \\(\\vec{x} = \\langle 0, 3, 6, 7\\rangle\\). What is the expected value that you think you will obtain if you sample from this data vector once? – Wait! What does that even mean? Expected value? Sampling once? Suppose that some joker from around town invites you for a game. The game goes like this: The joker puts a ball in an urn, one for each data observation. The joker writes the observed value on the ball corresponding to that value. You pay the joker a certain amount of money to be allowed to draw one ball from the urn. The balls are indistinguishable and the process of drawing is entirely fair. You receive the number corresponding to the ball you drew paid out in silver coins. (For simplicity, we assume that all numbers are non-negative, but that is not crucial. If a negative number is drawn, you just have to pay the joker that amount.) How many silver coins would you maximally pay to play one round? Well, of course, no more than four (unless you value gaming on top of silver)! This is because 4 is the expected value of drawing once. This, in turn, is because every ball has a chance of 0.25 of being drawn. So you can expect to earn 0 silver with a 25% chance, 3 with a 25% chance, 6 with a 25% chance and 7 with a 25% chance. In this sense, the mean is the expected value of sampling once from the observed data. 5.2.2.2 The median If \\(\\vec{x} = \\langle x_1, \\dots , x_n \\rangle\\) is a vector of \\(n\\) data observations from an at least ordinal measure and if \\(\\vec{x}\\) is ordered such that for all \\(1 \\le i &lt; n\\) we have \\(x_i \\le x_{i+1}\\), the median is the value \\(x_i\\) such that the number of data observations that are bigger or equal to \\(x_i\\) and the number of data observations that are smaller or equal to \\(x_i\\) are equal. Notice that this definition may yield no unique median. In that case, different alternative strategies are used, depending on the data type at hand (ordinal or metric). (See also the example below.) The median corresponds to the 50% quartile, a concept introduced below. Example. The median of the vector \\(\\vec{x} = \\langle 0, 3, 6, 7 \\rangle\\) does not exist by the definition given above. However, for metric measures, where distances between measurements are meaningful, it is customary to take the two values “closest to where the median should be” and average them. In the example at hand, this would be \\(\\frac{3 + 6}{2} = 4.5\\). The plot below shows the data points in black, the mean as a red cross (as before) and the median as a blue circle. The function median from base R computes the median of a vector. It also takes an ordered factor as an argument. median(c(0, 3, 6, 7 )) ## [1] 4.5 To please the avocados, let’s also calculate the median price of both types of avocados and compare these to the means we calculated earlier already: avocado_data %&gt;% group_by(type) %&gt;% summarise( mean_price = mean(average_price), median_price = median(average_price) ) ## # A tibble: 2 × 3 ## type mean_price median_price ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 conventional 1.16 1.13 ## 2 organic 1.65 1.63 5.2.2.3 The mode The mode is the unique value that occurred most frequently in the data. If there is no unique value with that property, there is no mode. While the mean is only applicable to metric variables, and the median only to variables that are at least ordinal, the mode is only reasonable for variables that have a finite set of different possible observations (nominal or ordinal). There is no built-in function in R to return the mode of a (suitable) vector, but it is easily retrieved by obtaining counts. Exercise 5.1: Mean, median, mode Compute the mean, median and mode of data vector \\(\\vec{x} = \\langle1,2,4,10\\rangle\\). Solution Mean: \\(\\frac{1+2+4+10}{4}=\\frac{17}{4}=4.25\\) Median: \\(\\frac{2+4}{2}=3\\) Mode: all values are equally frequent, so there is no (unique) mode. Now add two numbers to the vector such that the median stays the same, but mode and mean change. Solution Numbers to add: \\(1, 10 \\to \\vec{x} = \\langle1,1,2,4,10,10\\rangle\\) New mean: \\(\\frac{1+1+2+4+10+10}{6}=\\frac{28}{6}\\approx4.67\\) New mode: Both \\(1\\) and \\(10\\) are equally frequent and more frequent than all other numbers. Consequently, there is no (unique) mode. Decide for the following statements whether they are true or false: The mean is a measure of central tendency, which can be quite sensitive to even single outliers in the data. If \\(\\vec{x}\\) is a vector of binary Boolean outcomes, we can retrieve the proportion of occurrences of TRUE in \\(\\vec{x}\\) by the R function mean(x). Solution Both statements are correct. 5.2.3 Measures of dispersion Measures of dispersion indicate how much the observed data is spread out around a measure of central tendency. Intuitively put, they provide a measure for how diverse, variable, clustered, concentrated or smeared out the data observations are. In the following, we will cover three common notions: variance, standard deviation and quantiles. 5.2.3.1 Variance The variance is a common and very useful measure of dispersion for metric data. The variance \\(\\text{Var}(\\vec{x})\\) of a vector of metric observations \\(\\vec{x}\\) of length \\(n\\) is defined as the average of the squared distances from the mean: \\[\\text{Var}(\\vec{x}) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu_{\\vec{x}})^2\\] Example. The variance of the vector \\(\\vec{x} = \\langle 0, 3, 6, 7 \\rangle\\) is computed as: \\[\\text{Var}(\\vec{x}) = \\frac{1}{4} \\ \\left ( (0-4)^2 + (3-4)^2 + (6-4)^2 + (7-4)^2 \\right ) = \\] \\[ \\frac{1}{4} \\ (16 + 1 + 4 + 9) = \\frac{30}{4} = 7.5\\] Figure 5.3 shows a geometric interpretation of the variance for the running example of vector \\(\\vec{x} = \\langle 0, 3, 6, 7 \\rangle\\). Figure 5.3: Geometrical interpretation of variance. Four data points (orange dots) and their mean (red cross) are shown, together with the squares whose sides are the differences between the observed data points and the mean. The numbers in white give the area of each square, which is also indicated by the coloring of each rectangle. We can calculate the variance in R explicitly: x &lt;- c(0, 3, 6, 7) sum((x - mean(x))^2) / length(x) ## [1] 7.5 There is also a built-in function var from base R. Using this we get a different result though: x &lt;- c(0, 3, 6, 7) var(x) ## [1] 10 This is because var computes the variance by a slightly different formula to obtain an unbiased estimator of the variance for the case that the mean is not known but also estimated from the data. The formula for the unbiased estimator that R uses, simply replaces the \\(n\\) in the denominator by \\(n-1\\):21 \\[\\text{Var}(\\vec{x}) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\mu_{\\vec{x}})^2\\] 5.2.3.2 Standard deviation The standard deviation \\(\\text{SD}(\\vec{x})\\) of numeric vector \\(\\vec{x}\\) is just the square root of the variance: \\[ \\text{SD}(\\vec{x}) = \\sqrt{\\text{Var}(\\vec{x})} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu_{\\vec{x}})^2}\\] Let’s calculate the (unbiased) variance and standard deviation for the average_price of different types of avocados: avocado_data %&gt;% group_by(type) %&gt;% summarize( variance_price = var(average_price), stddev_price = sd(average_price), ) ## # A tibble: 2 × 3 ## type variance_price stddev_price ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 conventional 0.0692 0.263 ## 2 organic 0.132 0.364 5.2.3.3 Quantiles For a vector \\(\\vec{x}\\) of at least ordinal measures, we can generalize the concept of a median to an arbitrary quantile. A \\(k\\)% quantile is the element \\(x_i\\) in \\(\\vec{x}\\), such that \\(k\\)% of the data in \\(\\vec{x}\\) lies below \\(x_i\\). If this definition does not yield a unique element for some \\(k\\)% threshold, similar methods to what we saw for the median are applied. We can use the base R function quantile to obtain the 10%, 25%, 50% and 85% quantiles (just arbitrary picks) for the average_price in the avocado data set: quantile( # vector of observations x = avocado_data$average_price, # which quantiles probs = c(0.1, 0.25, 0.5, 0.85) ) ## 10% 25% 50% 85% ## 0.93 1.10 1.37 1.83 This tells us, for instance, that only about ten percent of the data observations had prices lower than $0.93. Exercise 5.2: Variance, standard deviation, quantiles Compute the unbiased variance and standard deviation of data vector \\(\\vec{y} = \\langle4,2,6,8\\rangle\\). Solution \\[ \\begin{align} \\mu_{\\vec{y}} &amp;= \\frac{4+2+6+8}{4}=5 \\\\ Var(\\vec{y}) &amp;= \\frac{1}{n-1}\\sum_{i = 1}^n (y_i-\\mu_{\\vec{y}})^2 \\\\ &amp;= \\frac{1}{4-1}((4-5)^2+(2-5)^2+(6-5)^2+(8-5)^2) \\\\ &amp;= \\frac{1}{3}(1+9+1+9) = \\frac{20}{3} \\approx 6.67 \\\\ SD(\\vec{y}) &amp;= \\sqrt{Var(\\vec{y})} = \\sqrt{6.67} \\approx 2.58 \\end{align} \\] Decide for the following statements whether they are true or false: The median is the 50% quantile. A 10% quantile of 0.2 indicates that 10% of the data observations are above 0.2. The 85% quantile of a vector with unequal numbers always has a larger value than the 25% quantile. Solution Statements a. and c. are correct. 5.2.4 Excursion: Quantifying confidence with bootstrapping Bootstrapping is an elegant way to obtain measures of confidence for summary statistics. These measures of confidence can be used for parameter inference, too. We will discuss parameter inference at length in Chapter 9. In this course, we will not use bootstrapping as an alternative approach to parameter inference. We will, however, follow a common practice (at least in some areas of Cognitive Psychology) to use bootstrapped 95% confidence intervals of the mean as part of descriptive statistics, i.e., in summaries and plots of the data. The bootstrap is a method from a more general class of algorithms, namely so-called resampling methods. The general idea is, roughly put, that we treat the data at hand as the true representation of reality. We then imagine that we run an experiment on that (restricted, hypothetical) reality. We then ask ourselves: What would we estimate (e.g., as a mean) in any such hypothetical experiment? The more these hypothetical measures derived from hypothetical experiments based on a hypothetical reality differ, the less confident we are in the estimate. Sounds weird, but it’s mind-blowingly elegant. An algorithm for constructing a 95% confidence interval of the mean of vector \\(D\\) of numeric data with length \\(k\\) looks as follows: take \\(k\\) samples from \\(D\\) with replacement, call this \\(D^{\\textrm{rep}}\\)22 calculate the mean \\(\\mu(D^{\\textrm{rep}})\\) of the newly sampled data repeat steps 1 and 2 to gather \\(r\\) means of different resamples of \\(D\\); call the result vector \\(\\mu_{\\textrm{sampled}}\\) the boundaries of the 95% inner quantile of \\(\\mu_{\\textrm{sampled}}\\) are the bootstrapped 95% confidence interval of the mean The higher \\(r\\), i.e., the more samples we take, the better the estimate. The higher \\(k\\), i.e., the more observations we have to begin with, the less variable the means \\(\\mu(D^{\\textrm{rep}})\\) of the resampled data will usually be. Hence, usually, the higher \\(k\\), the smaller the bootstrapped 95% confidence interval of the mean. Here is a convenience function that we will use throughout the book to produce bootstrapped 95% confidence intervals of the mean (the functions is also supplied directly as part of the aida package): ## takes a vector of numbers and returns bootstrapped 95% ConfInt ## of the mean, based on `n_resamples` re-samples (default: 1000) bootstrapped_CI &lt;- function(data_vector, n_resamples = 1000) { resampled_means &lt;- map_dbl(seq(n_resamples), function(i) { mean(sample(x = data_vector, size = length(data_vector), replace = T) ) } ) tibble( &#39;lower&#39; = quantile(resampled_means, 0.025), &#39;mean&#39; = mean(data_vector), &#39;upper&#39; = quantile(resampled_means, 0.975) ) } Applying this method to the vector of average avocado prices, we get: bootstrapped_CI(avocado_data$average_price) ## # A tibble: 1 × 3 ## lower mean upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.40 1.41 1.41 Notice that, since average_price has length 18249, i.e., we have \\(k = 18249\\) observations in the data, the bootstrapped 95% confidence interval is rather narrow. Compare this against a case of \\(k = 300\\), obtained by only looking at the first 300 entries in average_price: # first 300 observations of `average price` only smaller_data &lt;- avocado_data$average_price[1:300] bootstrapped_CI(smaller_data) ## # A tibble: 1 × 3 ## lower mean upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.14 1.16 1.17 The mean is different (because we are looking at earlier time points) but, importantly, the interval is larger because with only 300 observations, we have less confidence in the estimate. Exercise 5.3: Bootstrapped Confidence Intervals Explain in your own words how the bootstrapping-algorithm works for obtaining 95% confidence intervals of the mean (2-3 sentences). Solution To get the 95% CI of the mean, we repeatedly take samples from a data vector (with replacement) and calculate the mean of each sample. After taking \\(k\\) samples and calculating each mean \\(\\mu\\), we get a vector of means \\(\\mu_{sampled}\\). The 95% CI ranges between the boundaries of the 95% inner quantile of \\(\\mu_{sampled}\\). Decide for the following statements whether they are true or false: The more samples we take from our data, the larger the 95% confidence interval gets. A larger 95% confidence interval of the mean indicates higher uncertainty regarding the mean. The 95% confidence interval of the mean contains 95% of the values of \\(\\mu_{sampled}\\). Solution Statements b. and c. are correct. 5.2.4.1 Summary functions with multiple outputs, using nested tibbles To obtain summary statistics for different groups of a variable, we can use the function bootstrapped_CI conveniently in concert with nested tibbles, as demonstrated here: avocado_data %&gt;% group_by(type) %&gt;% # nest all columns except grouping-column &#39;type&#39; in a tibble # the name of the new column is &#39;price_tibbles&#39; nest(.key = &quot;price_tibbles&quot;) %&gt;% # collect the summary statistics for each nested tibble # the outcome is a new column with nested tibbles summarise( CIs = map(price_tibbles, function(d) bootstrapped_CI(d$average_price)) ) %&gt;% # unnest the newly created nested tibble unnest(CIs) ## # A tibble: 2 × 4 ## type lower mean upper ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 conventional 1.15 1.16 1.16 ## 2 organic 1.65 1.65 1.66 Using nesting in this case is helpful because we only want to run the bootstrap function once, but we need both of the numbers it returns. The following explains nesting based on this example. To understand what is going on with nested tibbles, notice that the nest function in this example creates a nested tibble with just two rows, one for each value of the variable type, each of which contains a tibble that contains all the data. The column price_tibbles in the first row contains the whole data for all observations for conventional avocados: avocado_data %&gt;% group_by(type) %&gt;% # nest all columns except grouping-column &#39;type&#39; in a tibble # the name of the new column is &#39;price_tibbles&#39; nest(.key = &quot;price_tibbles&quot;) %&gt;% # extract new column with tibble pull(price_tibbles) %&gt;% # peak at the first entry in this vector .[1] %&gt;% head() ## [[1]] ## # A tibble: 9,126 × 6 ## Date average_price total_volume_sold small medium large ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015-12-27 1.33 64237. 1037. 54455. 48.2 ## 2 2015-12-20 1.35 54877. 674. 44639. 58.3 ## 3 2015-12-13 0.93 118220. 795. 109150. 130. ## 4 2015-12-06 1.08 78992. 1132 71976. 72.6 ## 5 2015-11-29 1.28 51040. 941. 43838. 75.8 ## 6 2015-11-22 1.26 55980. 1184. 48068. 43.6 ## 7 2015-11-15 0.99 83454. 1369. 73673. 93.3 ## 8 2015-11-08 0.98 109428. 704. 101815. 80 ## 9 2015-11-01 1.02 99811. 1022. 87316. 85.3 ## 10 2015-10-25 1.07 74339. 842. 64757. 113 ## # … with 9,116 more rows After nesting, we call the custom function bootstrapped_CI on the variable average_price inside of every nested tibble, so first for the conventional, then the organic avocados. The result is a nested tibble. If we now look inside the new column CI, we see that its cells contain tibbles with the output of each call of bootstrapped_CI: avocado_data %&gt;% group_by(type) %&gt;% # nest all columns except grouping-column &#39;type&#39; in a tibble # the name of the new column is &#39;price_tibbles&#39; nest(.key = &quot;price_tibbles&quot;) %&gt;% # collect the summary statistics for each nested tibble # the outcome is a new column with nested tibbles summarise( CIs = map(price_tibbles, function(d) bootstrapped_CI(d$average_price)) ) %&gt;% # extract new column vector with nested tibbles pull(CIs) %&gt;% # peak at the first entry .[1] %&gt;% head() ## [[1]] ## # A tibble: 1 × 3 ## lower mean upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.15 1.16 1.16 Finally, we unnest the new column CIs to obtain the final result (code repeated from above): avocado_data %&gt;% group_by(type) %&gt;% # nest all columns except grouping-column &#39;type&#39; in a tibble # the name of the new column is &#39;price_tibbles&#39; nest(.key = &quot;price_tibbles&quot;) %&gt;% # collect the summary statistics for each nested tibble # the outcome is a new column with nested tibbles summarise( CIs = map(price_tibbles, function(d) bootstrapped_CI(d$average_price)) ) %&gt;% # unnest the newly created nested tibble unnest(CIs) ## # A tibble: 2 × 4 ## type lower mean upper ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 conventional 1.15 1.16 1.16 ## 2 organic 1.65 1.65 1.66 For the current purpose, it is not important what a biased or unbiased estimator is and why this subtle change in the formula matters. We will come back to the issue of estimation in Chapter 9.↩︎ \\(D^{\\textrm{rep}}\\) is short for “repeated data”. We will use this concept more later on. The idea is that we consider “hypothetical data” which we have not perceived, but which we might have. Repeated data is (usually) of the same shape and form as the original, observed data, which is also sometimes noted as \\(D^{\\textrm{obs}}\\) for clarity in comparison to \\(D^{\\textrm{rep}}\\).↩︎ "],["Chap-02-03-summary-statistics-2D.html", "5.3 Covariance and correlation", " 5.3 Covariance and correlation 5.3.1 Covariance Let \\(\\vec{x}\\) and \\(\\vec{y}\\) be two vectors of numeric data of the same length, such that all pairs of \\(x_i\\) and \\(y_i\\) are associated observations. For example, the vectors avocado_data$total_volume_sold and avocado_data$average_price would be such vectors. The covariance between \\(\\vec{x}\\) and \\(\\vec{y}\\) measures, intuitively put, the degree to which changes in one vector correspond with changes in the other. Formally, covariance is defined as follows (notice that we use \\(n-1\\) in the denominator to obtain an unbiased estimator if the means are unknown): \\[\\text{Cov}(\\vec{x},\\vec{y}) = \\frac{1}{n-1} \\ \\sum_{i=1}^n (x_i - \\mu_{\\vec{x}}) \\ (y_i - \\mu_{\\vec{y}})\\] There is a visually intuitive geometric interpretation of covariance. To see this, let’s look at a small contrived example. contrived_example &lt;- tribble( ~x, ~y, 2, 2, 2.5, 4, 3.5, 2.5, 4, 3.5 ) First, notice that the mean of x and y is 3: # NB: `map_df` here iterates over the columns of the tibble in its # first argument slot means_contr_example &lt;- map_df(contrived_example, mean) means_contr_example ## # A tibble: 1 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 3 We can then compute the covariance as follows: contrived_example &lt;- contrived_example %&gt;% mutate( area_rectangle = (x - mean(x)) * (y - mean(y)), covariance = 1 / (n() - 1) * sum((x - mean(x)) * (y - mean(y))) ) contrived_example ## # A tibble: 4 × 4 ## x y area_rectangle covariance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 2 1 0.25 ## 2 2.5 4 -0.5 0.25 ## 3 3.5 2.5 -0.25 0.25 ## 4 4 3.5 0.5 0.25 Similar to what we did with the variance, we can give a geometrical interpretation of covariance. Figure 5.4 shows the four summands contributing to the covariance of the contrived_example. What this graph clearly shows is that summands can have different signs. If \\(x_i\\) and \\(y_i\\) are both bigger than the mean, or if both are smaller than the mean, then the corresponding summand is positive. Otherwise, the corresponding summand is negative. This means that the covariance captures the degree to which pairs of \\(x_i\\) and \\(y_i\\) tend to deviate from the mean in the same general direction. A positive covariance is indicative of a positive general association between \\(\\vec{x}\\) and \\(\\vec{y}\\), while a negative covariance suggests that as you increase \\(x_i\\), the associated \\(y_i\\) becomes smaller. Figure 5.4: Geometrical interpretation of covariance. Four data points (orange dots) and their mean (white dot) are shown, together with the squares whose sides are the differences between the observed data points and the mean. The numbers in white give the area of each square, which is also indicated by the coloring of each rectangle. We can, of course, also calculate the covariance just with the built-in base R function cov: with(contrived_example, cov(x, y)) ## [1] 0.25 And, using this function, we can calculate the covariance between the logarithm of total_volume_sold and average_price in the avocado data:23 with(avocado_data, cov(log(total_volume_sold), average_price)) ## [1] -0.5388084 Interestingly, the negative covariance in this example suggests that across all associated data pairs, the larger total_volume_sold, the lower average_price. It is important that this is a descriptive statistics, and that this is not to be interpreted as evidence of a causal relation between the two measures of interest. Not in this example, not in any other. The covariance describes associated data points; it alone does not provide any evidence for causal relationships. 5.3.2 Correlation Covariance is a very useful notion to show how two variables, well, co-vary. But the problem with this notion of covariance is that it is not invariant under linear transformation. Consider the contrived_example from above once more. The original data had the following covariance: with(contrived_example, cov(x, y)) ## [1] 0.25 But if we just linearly transform, say, vector y to 1000 * y + 500 (e.g., because we switch to an equivalent, but numerically different measuring scale, such as going from Celcius to Fahrenheit), we obtain: with(contrived_example, cov(x, 1000 * y + 500)) ## [1] 250 This is a problem in so far as that we would like to have a measure of how much two variables co-vary that is robust against linear changes, say in measurement scale, like the difference between Celcius and Fahrenheit. To compensate for this problem, we can look at Bravais-Pearson correlation, which is covariance standardized by standard deviations: \\[r_{\\vec{x}\\vec{y}} = \\frac{\\text{Cov}(\\vec{x}, \\vec{y})}{\\text{SD}(\\vec{x}) \\ \\text{SD}(\\vec{y})}\\] Let’s check invariance under linear transformation, using the built-in function cor. The correlation coefficient for the original data is: with(contrived_example, cor(x, y)) ## [1] 0.3 The correlation coefficient for the data with linearly transformed y is: with(contrived_example, cor(x, 1000 * y + 500)) ## [1] 0.3 Indeed, the correlation coefficient is nicely bounded to lie between -1 and 1. A correlation coefficient of 0 is to be interpreted as the absence of any correlation. A correlation coefficient of 1 is a perfect positive correlation (the higher \\(x_i\\), the higher \\(y_i\\)), and -1 indicates a perfect negative correlation (the higher \\(x_i\\), the lower \\(y_i\\)). Again, pronounced positive or negative correlations are not to be confused with strong evidence for a causal relation. It is just a descriptive statistic capturing a property of associated measurements. In the avocado data, the logarithm of total_volume_sold shows a noteworthy correlation with average_price. This is also visible in Figure 5.5. with(avocado_data, cor(log(total_volume_sold), average_price)) ## [1] -0.5834087 Figure 5.5: Scatter plot of avocado prices, plotted against (logarithms of) the total amount sold. The black line is a linear regression line indicating the (negative) correlation between these measures (more on this later). Exercise 5.4: Covariance and Correlation Given two vectors of paired metric measurements \\(\\vec{x}\\) and \\(\\vec{y}\\), you are given the covariance \\(Cov(\\vec{x},\\vec{y}) = 1\\) and the variance of each vector \\(Var(\\vec{x}) = 25\\) and \\(Var(\\vec{y}) = 36\\). Compute Pearson’s correlation coefficient for \\(\\vec{x}\\) and \\(\\vec{y}\\). Solution \\(r_{\\vec{x}\\vec{y}}=\\frac{1}{\\sqrt{25}\\sqrt{36}}=\\frac{1}{30}\\) Decide for the following statements whether they are true or false: The covariance is bounded between -100 and 100. The Pearson correlation coefficient is bounded between 0 and 1. For any (non-trivial) vector \\(\\vec{x}\\) of metric measurements, \\(Cor(\\vec{x},\\vec{x}) = 1\\). Solution Statement c. is correct. We use the logarithm of total_volume_sold because we also used the logarithm for plotting before. Using the logarithm also gives a better linear fit, but what that means we will only see much later when we talk about linear regression.↩︎ "],["Chap-02-02-visualization.html", "6 Data Visualization", " 6 Data Visualization Numerical summaries of complex data always incur information loss. Still lossy, but less so (if done well), is visualization. Any serious data analysis should start with a process in which the analyst becomes intimate with the data at hand. Visualization is an integral part of data-intimacy. Section 6.1 demonstrates how summary statistics can be misleading and how a simple visualization can be much more revealing. Section 6.2 offers some reflection on what makes a data visualization successful. Section 6.3 introduces the basics of data visualization with the ggplot package, an integral part of the tidyverse.24 This first exposition is based on a scatter plot for the avocado price data. Going beyond scatter plots, Section 6.4 looks at some common types of plots and how to realize them using the geom_ family of functions in ggplot. The learning goals for this chapter are: obtain a basic understanding of better/worse plotting understand the idea of hypothesis-driven visualization develop a basic understanding of the ‘grammar of graphs’ get familiar with frequent visualization strategies bar plots, densities, violins, error bars, etc. be able to fine-tune graphs for better visualization It is possible to create ggplot-like graphs with similar syntax in Python, as described here. The Gadfly package for Julia uses very similar ideas (of incremental composition) but a different syntax.↩︎ "],["Chap-02-04-Anscombe-example.html", "6.1 Motivating example: Anscombe’s quartet", " 6.1 Motivating example: Anscombe’s quartet To see how summary statistics can be highly misleading, and how a simple plot can reveal a lot more, consider a famous dataset available in R (Anscombe 1973): anscombe %&gt;% as_tibble ## # A tibble: 11 × 8 ## x1 x2 x3 x4 y1 y2 y3 y4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.7 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.1 8.84 7.04 ## 7 6 6 6 8 7.24 6.13 6.08 5.25 ## 8 4 4 4 19 4.26 3.1 5.39 12.5 ## 9 12 12 12 8 10.8 9.13 8.15 5.56 ## 10 7 7 7 8 4.82 7.26 6.42 7.91 ## 11 5 5 5 8 5.68 4.74 5.73 6.89 There are four pairs of \\(x\\) and \\(y\\) coordinates. Unfortunately, these are stored in long format with two pieces of information buried inside of the column name: for instance, the name x3 contains the information that this column contains the \\(x\\) coordinates for the 3rd pair. This is rather untidy. But, using tools from the dplyr package, we can tidy up quickly: tidy_anscombe &lt;- anscombe %&gt;% as_tibble %&gt;% pivot_longer( ## we want to pivot every column everything(), ## use reg-exps to capture 1st and 2nd character names_pattern = &quot;(.)(.)&quot;, ## assign names to new cols, using 1st part of ## what reg-exp captures as new column names names_to = c(&quot;.value&quot;, &quot;grp&quot;) ) %&gt;% mutate(grp = paste0(&quot;Group &quot;, grp)) tidy_anscombe ## # A tibble: 44 × 3 ## grp x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Group 1 10 8.04 ## 2 Group 2 10 9.14 ## 3 Group 3 10 7.46 ## 4 Group 4 8 6.58 ## 5 Group 1 8 6.95 ## 6 Group 2 8 8.14 ## 7 Group 3 8 6.77 ## 8 Group 4 8 5.76 ## 9 Group 1 13 7.58 ## 10 Group 2 13 8.74 ## # … with 34 more rows Here are some summary statistics for each of the four pairs: tidy_anscombe %&gt;% group_by(grp) %&gt;% summarise( mean_x = mean(x), mean_y = mean(y), min_x = min(x), min_y = min(y), max_x = max(x), max_y = max(y), crrltn = cor(x, y) ) ## # A tibble: 4 × 8 ## grp mean_x mean_y min_x min_y max_x max_y crrltn ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Group 1 9 7.50 4 4.26 14 10.8 0.816 ## 2 Group 2 9 7.50 4 3.1 14 9.26 0.816 ## 3 Group 3 9 7.5 4 5.39 14 12.7 0.816 ## 4 Group 4 9 7.50 8 5.25 19 12.5 0.817 These numeric indicators suggest that each pair of \\(x\\) and \\(y\\) values is very similar. Only the ranges seem to differ. A brilliant example of how misleading numeric statistics can be, as compared to a plot of the data:25 tidy_anscombe %&gt;% ggplot(aes(x, y)) + geom_smooth(method = lm, se = F, color = &quot;darkorange&quot;) + geom_point(color = project_colors[3], size = 2) + scale_y_continuous(breaks = scales::pretty_breaks()) + scale_x_continuous(breaks = scales::pretty_breaks()) + labs( title = &quot;Anscombe&#39;s Quartet&quot;, x = NULL, y = NULL, subtitle = bquote(y == 0.5 * x + 3 ~ (r %~~% .82) ~ &quot;for all groups&quot;) ) + facet_wrap(~grp, ncol = 2, scales = &quot;free_x&quot;) + theme(strip.background = element_rect(fill = &quot;#f2f2f2&quot;, colour = &quot;white&quot;)) Figure 6.1: Anscombe’s Quartet: four different data sets, all of which receive the same correlation score. References "],["Chap-02-04-good-visualization.html", "6.2 Visualization: the good, the bad and the infographic", " 6.2 Visualization: the good, the bad and the infographic Producing good data visualization is very difficult. There are no uncontroversial criteria for what a good visualization should be. There are, unfortunately, quite clear examples of really bad visualizations. We will look at some of these examples in the following. An absolute classic on data visualization is an early book by Edward Tufte (1983) entitled “The Visual Display of Quantitative Information”. A distilled and over-simplified summary of Tufte’s proposal is that we should eliminate chart junk and increase the data-ink ratio, a concept which Tufte defines formally. The more information (= data) a plot conveys, the higher the data-ink ratio. The more ink it requires, the lower it is. However, not all information in the data is equally relevant. Also, spending extra ink to reduce the recipient’s mental effort of retrieving the relevant information can be justified. Essentially, I would here propose to consider a special case of data visualization, common to scientific presentations. I want to speak of hypothesis-driven visualization as a way of communicating a clear message, the message we care most about at the current moment of (scientific) exchange. Though merely a special instance of all the goals one could pursue with data visualization, focusing on this special case is helpful because it allows us to formulate a (defeasible) rule of thumb for good visualization in analogy to how natural language ought to be used in order to achieve optimal cooperative information flow (at least as conceived by authors): The vague &amp; defeasible rule of thumb of good data visualization (according to the author). “Communicate a maximal degree of relevant true information in a way that minimizes the recipient’s effort of retrieving this information.” Interestingly, just like natural language also needs to rely on a conventional medium for expressing ideas which might put additional constraints on what counts as optimal communication (e.g., we might not be allowed to drop a pronoun in English even though it is clearly recoverable from the context, and Italian speakers would happily omit it), so do certain unarticulated conventions in each specific scientific field.26 Here are a few examples of bad plotting.27 To begin with, check out this fictitious data set: large_contrast_data &lt;- tribble( ~group, ~treatment, ~measurement, &quot;A&quot;, &quot;on&quot;, 1000, &quot;A&quot;, &quot;off&quot;, 1002, &quot;B&quot;, &quot;on&quot;, 992, &quot;B&quot;, &quot;off&quot;, 990 ) If we are interested in any potential influence of variables group and treatment on the measurement in question, the following graph is ruinously unhelpful because the large size of the bars renders the relatively small differences between them almost entirely unspottable. large_contrast_data %&gt;% ggplot(aes(x = group, y = measurement, fill = treatment)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) A better visualization would be this: large_contrast_data %&gt;% ggplot(aes( x = group, y = measurement, shape = treatment, color = treatment, group = treatment ) ) + geom_point() + geom_line() + scale_y_continuous(breaks = scales::pretty_breaks()) The following examples use the Bio-Logic Jazz-Metal data set, in particular the following derived table of counts or the derived table of proportions: BLJM_associated_counts ## # A tibble: 4 × 3 ## JM LB n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Jazz Biology 38 ## 2 Jazz Logic 26 ## 3 Metal Biology 20 ## 4 Metal Logic 18 It is probably hard to believe but Figure 6.2 was obtained without further intentional uglification just by choosing a default 3D bar plot display in Microsoft’s Excel. It does actually show the relevant information but it is entirely useless for a human observer without a magnifying glass, professional measuring tools and a calculator. Figure 6.2: Example of a frontrunner for the prize of today’s most complete disaster in the visual communication of information. It gets slightly better with the following pie chart of the same numerical information, also generated with Microsoft’s Excel. Subjectively, Figure 6.3 is pretty much anything but pretty. Objectively, it is better than the previous visualization in terms of 3D bar plots shown in Figure 6.2 but the pie chart is still not useful for answering the question which we care about, namely whether logicians are more likely to prefer Jazz over Metal than biologists. Figure 6.3: Example of a rather unhelpful visual representation of the BLJM data (when the research question is whether logicians are more likely to prefer Jazz over Metal than biologists). We can produce a much more useful representation with the code below. (A similar visualization also appeared as Figure 5.1 in the previous chapter.) BLJM_associated_counts %&gt;% ggplot( aes( x = LB, y = n, color = JM, shape = JM, group = JM ) ) + geom_point(size = 3) + geom_line() + labs( title = &quot;Counts of choices of each music+subject pair&quot;, x = &quot;&quot;, y = &quot;&quot; ) Infographics. Scientific communication with visualized data is different from other modes of communication with visualized data. These other contexts come with different requirements for good data visualization. Good examples of highly successful infographics are produced by the famous illustrator Nigel Holmes, for instance. Figure 6.4 is an example from Holmes’ website showing different amounts of energy consumption for different household appliances. The purpose of this visualization is not (only) to communicate information about which of the listed household appliances is most energy-intensive. Its main purpose is to raise awareness for the unexpectedly large energy consumption of household appliances in general (in standby mode).28 Figure 6.4: Example of an infographic. While possibly considered ‘chart junk’ in a scientific context, the eye-catching and highly memorable (and pretty!) artwork serves a strong secondary purpose in contexts other than scientific ones where hypothesis-driven precise communication with visually presented data is key. References "],["Chap-02-04-ggplot.html", "6.3 Basics of ggplot", " 6.3 Basics of ggplot In this section, we will work towards a first plot with ggplot. It will be a scatter plot (more on different kinds of plots in Section 6.4) for the avocado price data. Check out the ggplot cheat sheet for a quick overview of the nuts and bolts of ggplot. The following paragraphs introduce the key concepts of ggplot: incremental composition: adding elements or changing attributes of a plot incrementally convenience functions &amp; defaults: a closer look at high-level convenience functions (like geom_point) and what they actually do layers: seeing how layers are stacked when we call, e.g. different geom_ functions in sequence grouping: what happens when we use grouping information (e.g., for color, shape or in facets) The section finishes with a first full example of a plot that has different layers, uses grouping, and customizes a few other things. To get started, let’s first load the (preprocessed) avocado data set used for plotting: avocado_data &lt;- read_csv(&quot;./data_sets/avocado-processed.csv&quot;) # aida::data_avocado 6.3.1 Incremental composition of a plot The “gg” in the package name ggplot is short for “grammar of graphs”. It provides functions for describing scientific data plots in a compositional manner, i.e., for dealing with different recurrent elements in a plot in an additive way. As a result of this approach, we will use the symbol + to add more and more elements (or to override the implicit defaults in previously evoked elements) to build a plot. For example, we can obtain a scatter plot for the avocado price data simply by first calling the function ggplot, which just creates an empty plot: incrementally_built_plot &lt;- ggplot() The plot stored in variable incrementally_built_plot is very boring. Take a look: incrementally_built_plot As you can see, you do not see anything except a (white) canvas. But we can add some stuff. Don’t get hung up on the details right now, just notice that we use + to add stuff to our plot:29 incrementally_built_plot + # add a geom of type `point` (=&gt; scatter plot) geom_point( # what data to use data = avocado_data, # supply a mapping (in the form of an &#39;aesthetic&#39; (see below)) mapping = aes( # which variable to map onto the x-axis x = total_volume_sold, # which variable to map onto the y-axis y = average_price ) ) You see that the function geom_point is what makes the points appear. You tell it which data to use and which mapping of variables from the data set to elements in the plot you like. That’s it, at least to begin with. We can also supply the information about the data to use and the aesthetic mapping in the ggplot function call. Doing so will make this information the default for any subsequently added layer. Notice also that the data argument in function ggplot is the first argument, so we will frequently make use of piping, like in the following code which is equivalent to the previous in terms of output: avocado_data %&gt;% ggplot(aes(x = total_volume_sold, y = average_price)) + geom_point() 6.3.2 Elements in the layered grammar of graphs Let’s take a step back. Actually, the function geom_point is a convenience function that does a lot of things automatically for us. It helps to understand subsequent code if we peek under the hood at least for a brief moment initially, if only to just realize where some of the terminology in and around the “grammar of graphs” comes from. The ggplot package defines a layered grammar of graphs (Wickham 2010). This is a structured description language for plots (relevant for data science). It uses a smart system of defaults so that it suffices to often just call a convenience wrapper like geom_point. But underneath, there is the possibility of tinkering with (almost?) all of the (layered) elements and changing the defaults if need be. The process of mapping data onto a visualization essentially follows this route: data -&gt; statistical transformation -&gt; geom. object -&gt; aesthetics You supply (tidy) data. The data is then transformed (e.g., by computing a summary statistic) in some way or another. This could just be an “identity map” in which case you will visualize the data exactly as it is. The resulting data representation is mapped onto some spatial (geometric) appearance, like a line, a dot, or a geometric shape. Finally, there is room to alter the specific aesthetics of this mapping from data to visual object, like adjusting the size or the color of a geometric object, possibly depending on some other properties it has (e.g., whether it is an observation for a conventional or an organically grown avocado). To make explicit the steps which are implicitly carried out by geom_point in the example above, here is a fully verbose but output-equivalent sequence of commands that builds the same plot by defining all the basic components manually: avocado_data %&gt;% ggplot() + # plot consists of layers (more on this soon) layer( # how to map columns onto ingredients in the plot mapping = aes(x = total_volume_sold, y = average_price), # what statistical transformation should be used? - here: none stat = &quot;identity&quot;, # how should the transformed data be visually represented? - here: as points geom = &quot;point&quot;, # should we tinker in any other way with the positioning of each element? # - here: no, thank you! position = &quot;identity&quot; ) + # x and y axes are non-transformed continuous scale_x_continuous() + scale_y_continuous() + # we use a cartesian coordinate system (not a polar or a geographical map) coord_cartesian() In this explicit call, we still need to specify the data and the mapping (which variable to map onto which axis). But we need to specify much more. We tell ggplot that we want standard (e.g., not log-transformed) axes. We also tell it that our axes are continuous, that the data should not be transformed and that the visual shape (= geom) to which the data is to be mapped is a point (hence the name geom_point). It is not important to understand all of these components right now. It is important to have seen them once, and to understand that geom_point is a wrapper around this call which assumes reasonable defaults (such as non-transformed axes, points for representation etc.). 6.3.3 Layers and groups ggplot is the “grammar of layered graphs”. Plots are compositionally built by combining different layers, if need be. For example, we can use another function from the geom_ family of functions to display a different visualization derived from the same data on top of our previous scatter plot.30 avocado_data %&gt;% ggplot( mapping = aes( # notice that we use the log (try without it to understand why) x = log(total_volume_sold), y = average_price ) ) + # add a scatter plot geom_point() + # add a linear regression line geom_smooth(method = &quot;lm&quot;) Notice that layering is really sequential. To see this, just check what happens when we reverse the calls of the geom_ functions in the previous example: avocado_data %&gt;% ggplot( mapping = aes( # notice that we use the log (try without it to understand why) x = log(total_volume_sold), y = average_price ) ) + # FIRST: add a linear regression line geom_smooth(method = &quot;lm&quot;) + # THEN: add a scatter plot geom_point() If you want lower layers to be visible behind layers added later, one possibility is to tinker with opacity, via the alpha parameter. Notice that the example below also changes the colors. The result is quite toxic, but at least you see the line underneath the semi-transparent points. avocado_data %&gt;% ggplot( mapping = aes( # notice that we use the log (try without it to understand why) x = log(total_volume_sold), y = average_price ) ) + # FIRST: add a linear regression line geom_smooth(method = &quot;lm&quot;, color = &quot;darkgreen&quot;) + # THEN: add a scatter plot geom_point(alpha = 0.1, color = &quot;orange&quot;) The aesthetics defined in the initial call to ggplot are global defaults for all layers to follow, unless they are overwritten. This also holds for the data supplied to ggplot. For example, we can create a second layer using another call to geom_point from a second data set (e.g., with a summary statistic), like so: # create a small tibble with the means of both # variables of interest avocado_data_means &lt;- avocado_data %&gt;% summarize( mean_volume = mean(log(total_volume_sold)), mean_price = mean(average_price) ) avocado_data_means ## # A tibble: 1 × 2 ## mean_volume mean_price ## &lt;dbl&gt; &lt;dbl&gt; ## 1 11.3 1.41 avocado_data %&gt;% ggplot( aes(x = log(total_volume_sold), y = average_price) ) + # first layer uses globally declared data &amp; mapping geom_point() + # second layer uses different data set &amp; mapping geom_point( data = avocado_data_means, mapping = aes( x = mean_volume, y = mean_price ), # change shape of element to display (see below) shape = 9, # change size of element to display size = 12, color = &quot;skyblue&quot; ) 6.3.4 Grouping Categorical distinction is frequently important in data analysis. Just think of the different combinations of factor levels in a factorial design, or the difference between conventionally grown and organically grown avocados. ggplot understands grouping very well and acts on appropriately, if you tell it to in the right way. Grouping can be relevant for different aspects of a plot: the color of points or lines, their shape, or even whether to plot everything together or separately. For instance, we might want to display different types of avocados in a different color. We can do this like so: avocado_data %&gt;% ggplot( aes( x = log(total_volume_sold), y = average_price, # use a different color for each type of avocado color = type ) ) + geom_point() Notice that we added the grouping information inside of aes to the call of ggplot. This way the grouping is the global default for the whole plot. Check what happens when we then add another layer, like geom_smooth: avocado_data %&gt;% ggplot( aes( x = log(total_volume_sold), y = average_price, # use a different color for each type of avocado color = type ) ) + geom_point() + geom_smooth(method = &quot;lm&quot;) The regression lines will also be shown in the colors of the underlying scatter plot. We can change this by overwriting the color attribute locally, but then we lose the grouping information: avocado_data %&gt;% ggplot( aes( x = log(total_volume_sold), y = average_price, # use a different color for each type of avocado color = type ) ) + geom_point() + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) To retrieve the grouping information, we can change the explicit keyword group (which just treats data from the relevant factor levels differently without directly changing their appearance): avocado_data %&gt;% ggplot( aes( x = log(total_volume_sold), y = average_price, # use a different color for each type of avocado color = type ) ) + geom_point() + geom_smooth( # tell the smoother to deal with avocados types separately aes(group = type), method = &quot;lm&quot;, color = &quot;black&quot; ) Finally, we see that the lines are not uniquely associable with the avocado type, so we can also change the regression line’s shape attribute conditional on avocado type: avocado_data %&gt;% ggplot( aes( x = log(total_volume_sold), y = average_price, # use a different color for each type of avocado color = type ) ) + geom_point() + geom_smooth( # tell the smoother to deal with avocados types separately aes(group = type, linetype = type), method = &quot;lm&quot;, color = &quot;black&quot; ) 6.3.5 Example of a customized plot If done with the proper mind and heart, plots intended to share (and to communicate a point, following the idea of hypothesis-driven visualization) will usually require a lot of tweaking. We will cover some of the most frequently relevant tweaks in Section 6.6. To nevertheless get a feeling of where the journey is going, at least roughly, here is an example of a plot of the avocado data which is much more tweaked and honed. No claim is intended regarding the false idea that this plot is in any sense optimal. There is not even a clear hypothesis or point to communicate. This just showcases some functionality. Notice, for instance, that this plot uses two layers, invoked by geom_point which shows the scatter plot of points and geom_smooth which layers on top the point cloud regression lines (one for each level in the grouping variable). # pipe data set into function `ggplot` avocado_data %&gt;% # reverse factor level so that horizontal legend entries align with # the majority of observations of each group in the plot mutate( type = fct_rev(type) ) %&gt;% # initialize the plot ggplot( # defined mapping mapping = aes( # which variable goes on the x-axis x = total_volume_sold, # which variable goes on the y-axis y = average_price, # which groups of variables to distinguish group = type, # color and fill to change by grouping variable fill = type, linetype = type, color = type ) ) + # declare that we want a scatter plot geom_point( # set low opacity for each point alpha = 0.1 ) + # add a linear model fit (for each group) geom_smooth( color = &quot;black&quot;, method = &quot;lm&quot; ) + # change the default (normal) of x-axis to log-scale scale_x_log10() + # add dollar signs to y-axis labels scale_y_continuous(labels = scales::dollar) + # change axis labels and plot title &amp; subtitle labs( x = &#39;Total volume sold (on a log scale)&#39;, y = &#39;Average price&#39;, title = &quot;Avocado prices against amount sold&quot;, subtitle = &quot;With linear regression lines&quot; ) Exercise 6.1: Find the match Determine which graph was created with which code: Code 1: code_1 &lt;- ggplot(avocado_data, mapping = aes( x = average_price, y = log(total_volume_sold), color = type ) ) + geom_point() + geom_smooth(method = &quot;lm&quot;) Code 2: code_2 &lt;- ggplot(avocado_data, mapping = aes( x = log(total_volume_sold), y = average_price, color = type ) ) + geom_point() + geom_smooth(method = &quot;lm&quot;) Code 3: code_3 &lt;- ggplot(avocado_data, mapping = aes( x = log(total_volume_sold), y = average_price ) ) + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) + geom_point(alpha = 0.1, color = &quot;blue&quot;) + labs( x = &#39;Total volume sold (on a log scale)&#39;, y = &#39;Average price&#39;, title = &quot;Avocado prices against amount sold&quot; ) Code 4: code_4 &lt;- ggplot(avocado_data, mapping = aes( x = log(total_volume_sold), y = average_price, linetype = type ) ) + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) + geom_point(alpha = 0.1, color = &quot;blue&quot;) + labs( x = &#39;Total volume sold (on a log scale)&#39;, y = &#39;Average price&#39;, title = &quot;Avocado prices against amount sold&quot; ) Plot 1: Plot 2: Plot 3: Solution Plot 1: Code 4 Plot 2: Code 1 Plot 3: Code 2 References "],["Chap-02-04-geoms.html", "6.4 A rendezvous with popular geoms", " 6.4 A rendezvous with popular geoms In the following, we will cover some of the more basic geom_ functions relevant for our present purposes. It might be useful to read this section top-to-bottom at least once, not to think of it as a mere reference list. More information is provided by the ggplot cheat sheet. 6.4.1 Scatter plots with geom_point Scatter plots visualize pairs of associated observations as points in space. We have seen this for the avocado prize data above. Let’s look at some of the further arguments we can use to tweak the presentation by geom_point. The following example changes the shape of the objects displayed to tilted rectangles (sometimes called diamonds, e.g., in LaTeX \\diamond) away from the default circles, the color of the shapes, their size and opacity. avocado_data %&gt;% ggplot(aes(x = log(total_volume_sold), y = average_price)) + geom_point( # shape to display is number 23 (tilted rectangle, see below) shape = 23, # color of the surrounding line of the shape (for shapes 21-24) color = &quot;darkblue&quot;, # color of the interior of each shape fill = &quot;lightblue&quot;, # size of each shape (default is 1) size = 5, # level of opacity for each shape alpha = 0.3 ) How do you know which shape is which number? - By looking at the picture in Figure 6.5, for instance. Figure 6.5: The numerical coding of different shapes in ggplot. Notice that objects 21-24 are sensitive to both color and fill, but the others are only sensitive to color. 6.4.2 Smooth The geom_smooth function operates on two-dimensional metric data and outputs a smoothed line, using different kinds of fitting functions. It is possible to show an indicator of certainty for the fit. We will deal with model fits in later parts of the book. For illustration, just enjoy a few examples here: avocado_data %&gt;% ggplot(aes(x = log(total_volume_sold), y = average_price)) + geom_point( shape = 23, color = &quot;darkblue&quot;, fill = &quot;lightblue&quot;, size = 3, alpha = 0.3 ) + geom_smooth( # fitting a smoothed curve to the data method = &quot;loess&quot;, # display standard error around smoothing curve se = T, color = &quot;darkorange&quot; ) 6.4.3 Line Use geom_line to display a line for your data if that data has associated (ordered) metric values. You can use argument linetype to specify the kind of line to draw. tibble( x = seq(-4, 8, by = 2), y = x^2 ) %&gt;% ggplot(aes(x, y)) + geom_line( linetype = &quot;dashed&quot; ) Sometimes you may want to draw lines between items that are grouped: BLJM_associated_counts %&gt;% ggplot( aes( x = LB, y = n, color = JM, group = JM ) ) + geom_line(size = 3) 6.4.4 Bar plot A bar plot, plotted with geom_bar or geom_col, displays a single number for each of several groups for visual comparison by length. The difference between these two functions is that geom_bar relies on implicit counting, while geom_col expects the numbers that translate into the length of the bars to be supplied for it. This book favors the use of geom_col by first wrangling the data to show the numbers to be visualized, since often this is the cleaner approach and the numbers are useful to have access to independently (e.g., for referring to in the text). Here’s an example of how bar_plot works (implicitly counting numbers of occurrences): tibble( shopping_cart = c( rep(&quot;chocolate&quot;, 2), rep(&quot;ice-cream&quot;, 5), rep(&quot;cookies&quot;, 8) ) ) %&gt;% ggplot(aes(x = shopping_cart)) + geom_bar() To display this data with geom_col we need to count occurrences first ourselves: tibble( shopping_cart = c( rep(&quot;chocolate&quot;, 2), rep(&quot;ice-cream&quot;, 5), rep(&quot;cookies&quot;, 8) ) ) %&gt;% dplyr::count(shopping_cart) %&gt;% ggplot(aes (x = shopping_cart, y = n)) + geom_col() To be clear, geom_col is essentially geom_bar when we overwrite the default statistical transformation of counting to “identity”: tibble( shopping_cart = c( rep(&quot;chocolate&quot;, 2), rep(&quot;ice-cream&quot;, 5), rep(&quot;cookies&quot;, 8) ) ) %&gt;% dplyr::count(shopping_cart) %&gt;% ggplot(aes (x = shopping_cart, y = n)) + geom_bar(stat = &quot;identity&quot;) Bar plots are a frequent sight in psychology papers. They are also controversial. They often fare badly with respect to the data-ink ratio. Especially, when what is plotted are means of grouped variables. For example, the following plot is rather uninformative (even if the research question is a comparison of means): avocado_data %&gt;% group_by(type) %&gt;% summarise( mean_price = mean(average_price) ) %&gt;% ggplot(aes(x = type, y = mean_price)) + geom_col() It makes sense to use the available space for a more informative report about the distribution of data points around the means, e.g., by using geom_violin or geom_histogram etc. But bar plots may also be good enough if there is not more of immediate relevance, such as when we look at counts or proportions. Still, it might help to include a measure of certainty. For instance, using the King of France data set, we can display proportions of ‘true’ answers with 95% bootstrapped confidence intervals like in the plot below. Notice the use of the geom_errorbar function to display the intervals in the following example. We first load the preprocessed data set. data_KoF_processed &lt;- read_csv(&quot;./data_sets/king-of-france_data_processed.csv&quot;) # aida::data_KoF_preprocessed data_KoF_processed %&gt;% # drop unused factor levels droplevels() %&gt;% # get means and 95% bootstrapped CIs for each condition group_by(condition) %&gt;% nest() %&gt;% summarise( CIs = map(data, function(d) bootstrapped_CI(d$response == &quot;TRUE&quot;)) ) %&gt;% unnest(CIs) %&gt;% # plot means and CIs ggplot(aes(x = condition, y = mean, fill = condition)) + geom_col() + geom_errorbar(aes(ymin = lower, ymax = upper, width = 0.2)) + ylim(0, 1) + labs( x = &quot;&quot;, y = &quot;&quot;, title = &quot;Proportion of &#39;TRUE&#39; responses per condition&quot;, subtitle = &quot;Error bars are bootstrapped 95% CIs&quot; ) + theme(legend.position = &quot;none&quot;) + scale_fill_manual(values = project_colors) + theme(axis.text.x = element_text(angle = 30, hjust = 1)) Exercise 6.2: Create a bar plot The data set we will work with in this exercise is currently not part of the aida package. We need to load it like so: url_prefix &lt;- &quot;https://raw.githubusercontent.com/michael-franke/intro-data-analysis/master/data_sets/&quot; WHO_data_url &lt;- str_c(url_prefix, &quot;WHO.csv&quot;) dataWHO &lt;- read_csv(WHO_data_url) Take a look at the data set first, in order to get familiar with it. Create a bar plot, in which Region is on the x-axis and LifeExpectancy_mean is on the y-axis. Each bar should represent the mean life expectancy rate for each region. Solution A minimally functional solution would be this: dataWHO %&gt;% # get the mean for each region group_by(Region) %&gt;% summarise( LifeExpectancy_mean = mean(LifeExpectancy) ) %&gt;% # plot ggplot( aes( x = Region, y = LifeExpectancy_mean, fill = Region ) ) + geom_col() A prettier version suppresses the legend, changes the axis labels, and sligtly tilts the tick labels: dataWHO %&gt;% # get the mean for each region group_by(Region) %&gt;% summarise( LifeExpectancy_mean = mean(LifeExpectancy) ) %&gt;% # plot ggplot( aes( x = Region, y = LifeExpectancy_mean, fill = Region ) ) + geom_col() + # nicer axis labels and a title labs( y = &quot;Mean life expectancy&quot;, title = &quot;Mean life expectancy in different world regions&quot; ) + # hide legend (b/c redundant) theme(legend.position = &quot;none&quot;) + # tilt tick labels by 30 degrees theme(axis.text.x = element_text(angle = 30, hjust = 1)) 6.4.5 Plotting distributions: histograms, boxplots, densities and violins There are different ways for plotting the distribution of observations in a one-dimensional vector, each with its own advantages and disadvantages: the histogram, a box plot, a density plot, and a violin plot. Let’s have a look at each, based on the average_price of different types of avocados. The histogram displays the number of occurrences of observations inside of prespecified bins. By default, the function geom_histogram uses 30 equally spaced bins to display counts of your observations. avocado_data %&gt;% ggplot(aes(x = average_price)) + geom_histogram() If we specify more bins, we get a more fine-grained picture. (But notice that such a high number of bins works for the present data set, which has many observations, but it would not necessarily for a small data set.) avocado_data %&gt;% ggplot(aes(x = average_price)) + geom_histogram(bins = 75) We can also layer histograms but this is usually a bad idea (even if we tinker with opacity) because a higher layer might block important information from a lower layer: avocado_data %&gt;% ggplot(aes(x = average_price, fill = type)) + geom_histogram(bins = 75) An alternative display of distributional metric information is a box plot. Box plots are classics, also called box-and-whiskers plots, and they basically visually report key summary statistics of your metric data. These do work much better than histograms for direct comparison: avocado_data %&gt;% ggplot(aes(x = type, y = average_price)) + geom_boxplot() What we see here is the median for each group (thick black line) and the 25% and 75% quantiles (boxes). The straight lines show the range from the 25% or 75% quantiles to the values given by median + 1.58 * IQR / sqrt(n), where the IQR is the “interquartile range”, i.e., the range between the 25% and 75% quantiles (boxes). To get a better picture of the shape of the distribution, geom_density uses a kernel estimate to show a smoothed line, roughly indicating ranges of higher density of observations with higher numbers. Using opacity, geom_density is useful also for the close comparison of distributions across different groups: avocado_data %&gt;% ggplot(aes(x = average_price, color = type, fill = type)) + geom_density(alpha = 0.5) For many groups to compare, density plots can become cluttered. Violin plots are like mirrored density plots and are better for comparison of multiple groups: avocado_data %&gt;% ggplot(aes(x = type, y = average_price, fill = type)) + geom_violin(alpha = 0.5) A frequently seen method of visualization is to layer a jittered distribution of points under a violin plot, like so: avocado_data %&gt;% ggplot(aes(x = type, y = average_price, fill = type)) + geom_jitter(alpha = 0.3, width = 0.2) + geom_violin(alpha = 0.5) 6.4.6 Rugs Since plotting distributions, especially with high-level abstract smoothing as in geom_density and geom_violin fails to give information about the actual quantity of the data points, rug plots are useful additions to such plots. geom_rug add marks along the axes where different points lie. Here is an example of geom_rug combined with geom_density: avocado_data %&gt;% filter(type == &quot;organic&quot;) %&gt;% ggplot(aes(x = average_price)) + geom_density(fill = &quot;darkorange&quot;, alpha = 0.5) + geom_rug() Here are rugs on a two-dimensional scatter plot: avocado_data %&gt;% filter(type == &quot;conventional&quot;) %&gt;% ggplot(aes(x = total_volume_sold, y = average_price)) + geom_point(alpha = 0.3) + geom_rug(alpha = 0.2) 6.4.7 Annotation It can be useful to add further elements to a plot. We might want to add text, or specific geometrical shapes to highlight aspects of data. The most general function for doing this is annotate. The function annotate takes as a first argument a geom argument, e.g., text or rectangle. It is therefore not a wrapper function in the geom_ family of functions, but the underlying function around which convenience functions like geom_text or geom_rectangle are wrapped. The further arguments that annotate expects depend on the geom it is supposed to realize. Suppose we want to add textual information at a particular coordinate. We can do this with annotate as follows: avocado_data %&gt;% filter(type == &quot;conventional&quot;) %&gt;% ggplot(aes(x = total_volume_sold, y = average_price)) + geom_point(alpha = 0.2) + annotate( geom = &quot;text&quot;, # x and y coordinates for the text x = 2e7, y = 2, # text to be displayed label = &quot;Bravo avocado!&quot;, color = &quot;firebrick&quot;, size = 8 ) We can also single out some data points, like so: avocado_data %&gt;% filter(type == &quot;conventional&quot;) %&gt;% ggplot(aes(x = total_volume_sold, y = average_price)) + geom_point(alpha = 0.2) + annotate( geom = &quot;rect&quot;, # coordinates for the rectangle xmin = 2.1e7, xmax = max(avocado_data$total_volume_sold) + 100, ymin = 0.7, ymax = 1.7, color = &quot;firebrick&quot;, alpha = 0, size = 2 ) "],["Chap-02-04-faceting.html", "6.5 Faceting", " 6.5 Faceting If we have grouping information, sometimes it can just get too much to put all of the information in a single plot, even if we use colors, shapes or line types for disambiguation. Facets are a great way to separately repeat the same kind of plot for different levels of relevant factors. The functions facet_grid and facet_wrap are used for faceting. They both expect a formula-like syntax (we have not yet introduced formulas) using the notation ~ to separate factors. The difference between these functions shows most clearly when we have more than two factors. So let’s introduce a new factor early to the avocado price data, representing whether a recorded measurement was no later than the median date or not. avocado_data_early_late &lt;- avocado_data %&gt;% mutate(early = ifelse(Date &lt;= median(Date), &quot;early&quot;, &quot;late&quot;)) Using facet_grid we get a two-dimensional grid, and we can specify along which axis of this grid the different factor levels are to range by putting the factors in the formula notation like this: row_factor ~ col_factor. avocado_data_early_late %&gt;% ggplot(aes(x = log(total_volume_sold), y = average_price)) + geom_point(alpha = 0.3, color = &quot;skyblue&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;darkorange&quot;) + facet_grid(type ~ early) The same kind of plot realized with facet_wrap looks slightly different. The different factor level combinations are mushed together into a pair. avocado_data_early_late %&gt;% ggplot(aes(x = log(total_volume_sold), y = average_price)) + geom_point(alpha = 0.3, color = &quot;skyblue&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;darkorange&quot;) + facet_wrap(type ~ early) Exercise 6.3: Faceting In your own words, describe what each line of the two code chunks above does. Solution For both: Defining which information should be placed on which axis. A scatter plot is created using geom_point to show data points. Furthermore, the alpha level is chosen and the color of the points is skyblue. A line is added using geom_smooth and the method lm. The color of the line is dark orange. Both geom_point and geom_smooth are currently following the mapping given at the beginning. facet_grid: Now the grid is created with facet_grid, which divides the plot into type and time (early or late). In each part of the plot, you now see the subplot, which contains only the data points that belong to the respective combination. Type and time are placed on different axes. facet_wrap: Now the grid is created with facet_wrap, which divides the plot into type and time (early or late). In each part of the plot, you now see the subplot, which contains only the data points that belong to the respective combination. Here, type and time are combined into pairs. With facet_wrap it is possible to specify the desired number of columns or rows: avocado_data_early_late %&gt;% ggplot(aes(x = log(total_volume_sold), y = average_price)) + geom_point(alpha = 0.3, color = &quot;skyblue&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;darkorange&quot;) + facet_wrap(type ~ early, nrow = 1) "],["Chap-02-04-customization.html", "6.6 Customization etc.", " 6.6 Customization etc. There are many ways in which graphs can (and often: ought to) be tweaked further. The following can only cover a small, but hopefully useful selection. 6.6.1 Themes The general appearance of a plot is governed by its theme. There are many ready-made themes already in the ggplot package, as listed here, and there are more in several other packages. If we store a plot in a variable we can look at how different themes affect it. avocado_grid_plot &lt;- avocado_data_early_late %&gt;% ggplot(aes(x = log(total_volume_sold), y = average_price)) + geom_point(alpha = 0.3, color = &quot;skyblue&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;darkorange&quot;) + facet_grid(type ~ early) avocado_grid_plot + theme_classic() avocado_grid_plot + theme_void() avocado_grid_plot + theme_dark() The plots in this book use the theme hrbrthemes::theme_ipsum from the hrbrthemes package as a default. You can set the default theme for all subsequent plots using a command like this: # set the &#39;void&#39; theme as global default theme_set( theme_void() ) More elaborate tweaking of a plot’s layout can be achieved by the theme function. There are many options. Some let you do crazy things: avocado_grid_plot + theme(plot.background = element_rect(fill = &quot;darkgreen&quot;)) 6.6.2 Guides When using grouped variables (by color, shape, linetype, group, …) ggplot creates a legend automatically. avocado_data %&gt;% ggplot( mapping = aes( x = log(total_volume_sold), y = average_price, color = type ) ) + geom_point(alpha = 0.5) The legend can be suppressed with the guides command. It takes as arguments the different types of grouping variables (like color, group, etc.). avocado_data %&gt;% ggplot( mapping = aes( x = log(total_volume_sold), y = average_price, color = type ) ) + geom_point(alpha = 0.5) + # no legend for grouping by color guides(color = &quot;none&quot;) 6.6.3 Axes, ticks and tick labels If you need to use a non-standard (Cartesian) axis, you can do so, e.g., by changing the \\(x\\)-axis to a log scale (with base 10): avocado_data %&gt;% ggplot( mapping = aes( x = total_volume_sold, y = average_price, color = type ) ) + geom_point(alpha = 0.5) + scale_x_log10() The scales package has a number of nice convenience functions for tweaking axis ticks (the places where axes are marked and possibly labeled) and tick labels (the labels applied to the tick marks). For example, we can add dollar signs to the price information, like so: avocado_data %&gt;% ggplot( mapping = aes( x = total_volume_sold, y = average_price, color = type ) ) + geom_point(alpha = 0.5) + scale_x_log10() + scale_y_continuous(labels = scales::dollar) 6.6.4 Labels To change any other kind of labeling information (aside from tick mark labels on axes), the labs function can be used. It is rather self-explanatory: avocado_data %&gt;% ggplot( mapping = aes( x = total_volume_sold, y = average_price, color = type ) ) + geom_point(alpha = 0.5) + scale_x_log10() + scale_y_continuous(labels = scales::dollar) + # change axis labels and plot title &amp; subtitle labs( x = &#39;Total volume sold (on a log scale)&#39;, y = &#39;Average price&#39;, title = &quot;Avocado prices plotted against the amount sold per type&quot;, subtitle = &quot;With linear regression lines&quot;, caption = &quot;This plot shows the total volume of avocados sold against the average price for many different points in time.&quot; ) 6.6.5 Combining &amp; arranging plots Presenting visual information in a tightly packed spatial arrangement can be helpful for the spectator. Everything is within a single easy saccade, so to speak. Therefore it can be useful to combine different plots into a single combined plot. The cowplot package helps with this, in particular the function cowplot::plot_grid as shown here: # create an avocado plot avocado_plot &lt;- avocado_data %&gt;% ggplot(aes(x = total_volume_sold, y = average_price)) + geom_point(alpha = 0.5) # create a BLJM bar plot BLJM_plot &lt;- data_BLJM_processed %&gt;% ggplot(aes(x = response)) + geom_bar() # combine both into one cowplot::plot_grid( # plots to combine avocado_plot, BLJM_plot, # number columns ncol = 1 ) 6.6.6 LaTeX expressions in plot labels If you are enthusiastic about LaTeX, you can also use it inside of plot labels. The latex2exp package is useful here, which provides the function latex2exp::TeX to allow you to include LaTeX formulas. Just make sure that you double all backslashes, as in the following example: avocado_data %&gt;% ggplot(aes(x = total_volume_sold, y = average_price)) + geom_point(alpha = 0.5) + labs(title = latex2exp::TeX(&quot;We can use $\\\\LaTeX$ here: $\\\\sum_{i = 0}^n \\\\alpha^i$&quot;)) Exercise 6.4: Customization Feel free to play around with customizing your previously created plots or plots that you find in this book. Try to make annotations or try out different themes and colors. It will help you understand these kinds of plots a little better. print(sessionInfo()) ## R version 4.1.1 (2021-08-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 20.04.3 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0 ## ## locale: ## [1] LC_CTYPE=C.UTF-8 LC_NUMERIC=C LC_TIME=C.UTF-8 ## [4] LC_COLLATE=C.UTF-8 LC_MONETARY=C.UTF-8 LC_MESSAGES=C.UTF-8 ## [7] LC_PAPER=C.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] zeallot_0.1.0 magrittr_2.0.1 scales_1.1.1 cowplot_1.1.1 ## [5] latex2exp_0.5.0 naniar_0.6.1 boot_1.3-28 extraDistr_1.9.1 ## [9] rcartocolor_2.0.0 gridExtra_2.3 ggsignif_0.6.3 brms_2.16.1 ## [13] Rcpp_1.0.7 forcats_0.5.1 stringr_1.4.0 dplyr_1.0.7 ## [17] purrr_0.3.4 readr_2.0.2 tidyr_1.1.4 tibble_3.1.5 ## [21] ggplot2_3.3.5 tidyverse_1.3.1 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 ## [4] igraph_1.2.6 splines_4.1.1 crosstalk_1.1.1 ## [7] rstantools_2.1.1 inline_0.3.19 digest_0.6.28 ## [10] htmltools_0.5.2 rsconnect_0.8.24 fansi_0.5.0 ## [13] checkmate_2.0.0 tzdb_0.1.2 modelr_0.1.8 ## [16] RcppParallel_5.1.4 matrixStats_0.61.0 vroom_1.5.5 ## [19] xts_0.12.1 prettyunits_1.1.1 colorspace_2.0-2 ## [22] rvest_1.0.1 haven_2.4.3 xfun_0.26 ## [25] callr_3.7.0 crayon_1.4.1 jsonlite_1.7.2 ## [28] lme4_1.1-27.1 zoo_1.8-9 glue_1.4.2 ## [31] gtable_0.3.0 V8_3.4.2 distributional_0.2.2 ## [34] pkgbuild_1.2.0 rstan_2.21.2 abind_1.4-5 ## [37] mvtnorm_1.1-2 DBI_1.1.1 miniUI_0.1.1.1 ## [40] xtable_1.8-4 bit_4.0.4 stats4_4.1.1 ## [43] StanHeaders_2.21.0-7 DT_0.19 htmlwidgets_1.5.4 ## [46] httr_1.4.2 threejs_0.3.3 posterior_1.1.0 ## [49] ellipsis_0.3.2 pkgconfig_2.0.3 loo_2.4.1 ## [52] farver_2.1.0 sass_0.4.0 dbplyr_2.1.1 ## [55] utf8_1.2.2 labeling_0.4.2 tidyselect_1.1.1 ## [58] rlang_0.4.11 reshape2_1.4.4 later_1.3.0 ## [61] munsell_0.5.0 cellranger_1.1.0 tools_4.1.1 ## [64] cli_3.0.1 generics_0.1.0 broom_0.7.9 ## [67] ggridges_0.5.3 evaluate_0.14 fastmap_1.1.0 ## [70] yaml_2.2.1 bit64_4.0.5 processx_3.5.2 ## [73] knitr_1.36 fs_1.5.0 visdat_0.5.3 ## [76] nlme_3.1-152 mime_0.12 projpred_2.0.2 ## [79] xml2_1.3.2 compiler_4.1.1 bayesplot_1.8.1 ## [82] shinythemes_1.2.0 rstudioapi_0.13 gamm4_0.2-6 ## [85] curl_4.3.2 reprex_2.0.1 bslib_0.3.0 ## [88] stringi_1.7.5 highr_0.9 ps_1.6.0 ## [91] Brobdingnag_1.2-6 lattice_0.20-45 Matrix_1.3-4 ## [94] nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 ## [97] tensorA_0.36.2 vctrs_0.3.8 pillar_1.6.3 ## [100] lifecycle_1.0.1 jquerylib_0.1.4 bridgesampling_1.1-2 ## [103] httpuv_1.6.3 R6_2.5.1 bookdown_0.24.1 ## [106] promises_1.2.0.1 codetools_0.2-18 colourpicker_1.1.1 ## [109] MASS_7.3-54 gtools_3.9.2 assertthat_0.2.1 ## [112] withr_2.4.2 shinystan_2.5.0 mgcv_1.8-37 ## [115] parallel_4.1.1 hms_1.1.1 grid_4.1.1 ## [118] coda_0.19-4 minqa_1.2.4 rmarkdown_2.11 ## [121] shiny_1.7.1 lubridate_1.7.10 base64enc_0.1-3 ## [124] dygraphs_1.1.1.6 "],["Chap-03-01-probability.html", "7 Basics of Probability Theory", " 7 Basics of Probability Theory Probability is the basic ingredient of statistical inference. In this chapter, we will cover the very basics of probability theory. We will visit its axiomatic definition and some common interpretations in Section 7.1, where we also start with the main mental exercise of this section: seeing how probability distributions can be approximately represented by samples. We will cover important concepts such as joint and marginal probability in Section 7.2. This paves the way for learning about conditional probability and Bayes rule in Section 7.3. Section 7.4 introduces the notion of a random variable. Finally, Section 7.5 briefly covers how information about common probability distributions can be accessed in R. The learning goals for this chapter are: become familiar with the notion of probability and also: its axiomatic definition the notion of joint, marginal and conditional probability understand and apply Bayes rule get comfortable with random variables understand how probability distributions are approximately represented by samples become able to use R’s built-in probability distributions "],["Chap-03-01-probability-basics.html", "7.1 Probability", " 7.1 Probability Intuitively put, a probability distribution is a formal construct that captures an agent’s belief state. In Bayesian data analysis, that agent of interest is the analyst themselves or a hypothetical model of the analyst. More concretely, a probability distribution assigns numerical values (conveniently scaled to lie between 0 and 1) to a number of different contingencies, i.e., different ways the world could be. These numbers can be interpreted as the weight of belief (also referred to as “degree of credence” in the philosophical literature) that the agent assigns to each contingency: the higher the number assigned to a contingency, the more likely the agent considers this way the world could be. 7.1.1 Outcomes, events, observations To define the notion of probability, we first consider the space of relevant contingencies (ways the world could be) \\(\\Omega\\) containing all elementary outcomes \\(\\omega_1, \\omega_2, \\dots \\in \\Omega\\) of a process or an event whose execution is (partially) random or unknown. Elementary outcomes are mutually exclusive (\\(\\omega_i \\neq \\omega_j \\; \\text{for} \\,\\forall i \\neq j\\)). The set \\(\\Omega\\) exhausts all possibilities.31 Example. The set of elementary outcomes of a single coin flip is \\(\\Omega_{\\text{coin flip}} = \\left \\{ \\text{heads}, \\text{tails} \\right \\}\\). The elementary outcomes of tossing a six-sided die are \\(\\Omega_{\\text{standard die}} = \\{\\)⚀, ⚁, ⚂, ⚃, ⚄, ⚅ \\(\\}\\).32 An event \\(A\\) is a subset of \\(\\Omega\\). Think of an event as a (possibly partial) observation. We might observe, for instance, not the full outcome of tossing a die, but only that there is a dot in the middle. This would correspond to the event \\(A = \\{\\) ⚀, ⚂, ⚄ \\(\\}\\), i.e., observing an odd-numbered outcome. The trivial observation \\(A = \\Omega\\) and the impossible observation \\(A = \\emptyset\\) are counted as events, too. The latter is included for technical reasons that we don’t need to know for our purpose. For any two events \\(A, B \\subseteq \\Omega\\), standard set operations correspond to logical connectives in the usual way. For example, the conjunction \\(A \\cap B\\) is the observation of both \\(A\\) and \\(B\\); the disjunction \\(A \\cup B\\) is the observation that it is either \\(A\\) or \\(B\\); the negation of \\(A\\), \\(\\overline{A} = \\left \\{ \\omega \\in \\Omega \\mid \\omega \\not \\in A \\right \\}\\), is the observation that it is not \\(A\\). 7.1.2 Probability distributions A probability distribution \\(P\\) over \\(\\Omega\\) is a function \\(P \\ \\colon \\ \\mathfrak{P}(\\Omega) \\rightarrow \\mathbb{R}\\)33 that assigns to all events \\(A \\subseteq \\Omega\\) a real number, such that the following (so-called Kolmogorov axioms) are satisfied: A1. \\(0 \\le P(A) \\le 1\\) A2. \\(P(\\Omega) = 1\\) A3. \\(P(A_1 \\cup A_2 \\cup A_3 \\cup \\dots) = P(A_1) + P(A_2) + P(A_3) + \\dots\\) whenever \\(A_1, A_2, A_3, \\dots\\) are mutually exclusive34 Occasionally, we encounter the notation \\(P \\in \\Delta(\\Omega)\\) to express that \\(P\\) is a probability distribution over \\(\\Omega\\). (E.g., in physics, theoretical economics or game theory. Less so in psychology or statistics.) If \\(\\omega \\in \\Omega\\) is an elementary event, we often write \\(P(\\omega)\\) as a shorthand for \\(P(\\left \\{ \\omega \\right \\})\\). In fact, if \\(\\Omega\\) is finite, it suffices to assign probabilities to elementary outcomes. A number of rules follow immediately from the definition: C1. \\(P(\\emptyset) = 0\\) C2. \\(P(\\overline{A}) = 1 - P(A)\\) C3. \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) for any \\(A, B \\subseteq \\Omega\\) Exercise 7.1 [optional] Prove C1, C2 and C3 using A1, A2 and A3. Solution C1: \\(P(\\Omega \\cup \\emptyset) = P(\\Omega) + P(\\emptyset) \\Leftrightarrow P(\\Omega) = P(\\Omega) + P(\\emptyset) \\Leftrightarrow 0 = P(\\emptyset)\\) following A3 since \\(\\Omega\\) and \\(\\emptyset\\) are mutually exclusive. C2: \\(P(\\Omega) = P(A \\cup \\overline{A}) = P(A) + P(\\overline{A}) = 1\\). C3: \\(P(A \\cup B) = P((A-B) \\cup (A \\cap B) \\cup (B-A)) = P(A-B) + P(A \\cap B) + P(B-A) = \\\\ (P(A \\cup B) - P(B)) + P(A \\cap B) + (P(A \\cup B) - P(A)) = 2 P(A \\cup B) - P(A) - P(B) + P(A \\cap B) \\\\ \\Leftrightarrow P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) 7.1.3 Interpretations of probability It is reasonably safe to think of probability, as defined above, as a handy mathematical primitive which is useful for certain applications. There are at least three ways of thinking about where this primitive probability might come from: Frequentist: Probabilities are generalizations of intuitions/facts about frequencies of events in repeated executions of a random event. Subjectivist: Probabilities are subjective beliefs of a rational agent who is uncertain about the outcome of a random event. Realist: Probabilities are the property of an intrinsically random world. While trying to stay away from philosophical quibbles, we will adopt a subjectivist interpretation of probabilities, but note that frequentist considerations should affect what a rational agent should believe. 7.1.4 Distributions as samples No matter what your metaphysics of probability are, it is useful to realize that probability distributions can be approximately represented by sampling. Think of an urn as a container with balls of different colors with different proportions (see Figure 7.1). In the simplest case, there is a number of \\(N &gt; 1\\) balls of which \\(k &gt; 0\\) are black and \\(N-k &gt; 0\\) are white. (There are at least one black and one white ball.) For a single random draw from our urn we have: \\(\\Omega_{\\text{our urn}} = \\left \\{ \\text{white}, \\text{black} \\right \\}\\). We now draw from this urn with replacement. That is, we shake the urn, draw one ball, observe its color, take note of the color, and put it back into the urn. Each ball has the same chance of being sampled. If we imagine an infinite sequence of single draws from our urn with replacement, the limiting proportion with which we draw a black ball is \\(\\frac{k}{N}\\). This statement about frequency is what motivates saying that the probability of drawing a black ball on a single trial is (or should be35) \\(P(\\text{black}) = \\frac{k}{N}\\). Figure 7.1: An urn with seven black balls and three white balls. Imagine shaking this container, and then drawing blindly a single ball from it. If every ball has an equal probability of being drawn, what is the probability of drawing a black ball? That would be 0.7. The plot below shows how the proportion of black balls drawn from an urn like in Figure 7.1 with \\(k = 7\\) black balls and \\(N = 10\\) balls in total, gravitates to the probability 0.7 when we keep drawing and drawing. To sum this up concisely, we have a random process (drawing once from the urn) whose outcome is uncertain, and we convinced ourselves that the probability of an outcome corresponds to the relative frequency it occurs, in the limit of repeatedly executing the random process (i.e., sampling from the urn). From here, it requires only a small step to a crucial but ultimately very liberating realization. If the probability of an event occurring can be approximated by its frequency in a large sample, then we can approximately represent (say: internally in a computer) a probability distribution as one of two things: a large set of (what is called: representative) samples; or even better as an oracle (e.g., in the form of a clever algorithm) that quickly returns a representative sample. This means that, for approximately computing with probability, we can represent distributions through samples or a sample-generating function. We do not need to know precise probability or be able to express them in a mathematical formula. Samples or sampling is often enough to approximate probability distributions. Exercise 7.2 Explore how taking more or less samples affects the proportion of draws from an urn with the WebPPL code below. You can enter the number of black balls and the total number of balls for your urn. You can also enter the number of times you want to draw from your urn (with replacement - meaning that after every draw, the ball you just picked is placed back into the urn). You should execute the code several times in sequence with the same parameter values. This is because each time you run the code, another different random result will be shown. By inspecting what happens across several runs (each drawing nr_draws times from the urn), you can check the effect of varying the variable nr_draws. E.g., what happens with a low sample size, e.g., nr_draws = 20, as opposed to a large sample size, e.g., nr_draws = 100000? // how many balls are black? how many in total? var nr_black = 7 var nr_total = 10 // how many draws from the urn (with replacement)? var nr_draws = 20 ///fold: var model = function() { flip(nr_black/nr_total) == 1 ? \"black\" : \"white\" } display('Proportion of balls sampled') Infer({method: \"forward\", samples : nr_draws}, model) /// Solution With a small sample size, there is a lot of variation in the observed proportion. As the sample size gets larger and larger, the result converges to nr_black / nr_total. For simplicity of exposure, we gloss over subtleties arising when dealing with infinite sets \\(\\Omega\\). We make up for this when we define probability density functions for continuous random variables, which have an uncountably infinite number of elementary outcomes. We will usually be concerned with continuous random variables within applied statistics.↩︎ Think of \\(\\Omega\\) as a partition of the space of all possible ways in which the world could be, where we lump together into one partition cell (one elementary outcome) all ways in which the world could be that are equivalent regarding those aspects of reality that we are interested in. We do not care whether the coin lands in the mud or in the sand. It only matters whether it came up heads or tails. Each elementary event can be realized in myriad ways. \\(\\Omega\\) is our, the modelers’, first crude simplification of nature, abstracting away aspects we currently do not care about.↩︎ For any of you who are interested in the precise mathematical description of probability space, \\(\\mathfrak{P}(\\Omega)\\) is called Borel set. It is important since probabilities can only be defined for measurable sets.↩︎ A3 is the axiom of countable additivity. Finite additivity may be enough for finite or countable sets \\(\\Omega\\), but infinite additivity is necessary for full generality in the uncountable case.↩︎ If probabilities are subjective beliefs, a rational agent is, in a sense, normatively required to assign exactly this probability.↩︎ "],["Chap-03-01-probability-marginal.html", "7.2 Structured events &amp; marginal distributions", " 7.2 Structured events &amp; marginal distributions The single urn scenario of the last section is a very basic first example. To pave the way for learning about conditional probability and Bayes rule in the next sections, let us consider a slightly more complex example. We call it the flip-and-draw scenario. 7.2.1 Probability table for a flip-and-draw scenario Suppose we have two urns. Both have \\(N=10\\) balls. Urn 1 has \\(k_1=2\\) black and \\(N-k_1 = 8\\) white balls. Urn 2 has \\(k_2=4\\) black and \\(N-k_2=6\\) white balls. We sometimes draw from urn 1, sometimes from urn 2. To decide from which urn a ball should be drawn, we flip a fair coin. If it comes up heads, we draw from urn 1; if it comes up tails, we draw from urn 2. The process is visualized in Figure 7.2 below. An elementary outcome of this two-step process of flip-and-draw is a pair \\(\\langle \\text{outcome-flip}, \\text{outcome-draw} \\rangle\\). The set of all possible such outcomes is: \\[\\Omega_{\\text{flip-and-draw}} = \\left \\{ \\langle \\text{heads}, \\text{black} \\rangle, \\langle \\text{heads}, \\text{white} \\rangle, \\langle \\text{tails}, \\text{black} \\rangle, \\langle \\text{tails}, \\text{white} \\rangle \\right \\}\\,.\\] The probability of event \\(\\langle \\text{heads}, \\text{black} \\rangle\\) is given by multiplying the probability of seeing “heads” on the first flip, which happens with probability \\(0.5\\), and then drawing a black ball, which happens with probability \\(0.2\\), so that \\(P(\\langle \\text{heads}, \\text{black} \\rangle) = 0.5 \\times 0.2 = 0.1\\). The probability distribution over \\(\\Omega_{\\text{flip-draw}}\\) is consequently as in Table 7.1. (If in doubt, start flipping &amp; drawing and count your outcomes or use the WebPPL code box in the exercise below to simulate flips-and-draws.) Table 7.1: Joint probability table for the flip-and-draw scenario heads tails black \\(0.5 \\times 0.2 = 0.1\\) \\(0.5 \\times 0.4 = 0.2\\) white \\(0.5 \\times 0.8 = 0.4\\) \\(0.5 \\times 0.6 = 0.3\\) Figure 7.2: The flip-and-draw scenario, with transition and full path probabilities. 7.2.2 Structured events and joint-probability distributions Table 7.1 is an example of a joint probability distribution over a structured event space, which here has two dimensions. Since our space of outcomes is the Cartesian product of two simpler outcome spaces, namely \\(\\Omega_{flip\\text{-}\\&amp;\\text{-}draw} = \\Omega_{flip} \\times \\Omega_{draw}\\),36 we can use notation \\(P(\\text{heads}, \\text{black})\\) as shorthand for \\(P(\\langle \\text{heads}, \\text{black} \\rangle)\\). More generally, if \\(\\Omega = \\Omega_1 \\times \\dots \\Omega_n\\), we can think of \\(P \\in \\Delta(\\Omega)\\) as a joint probability distribution over \\(n\\) subspaces. 7.2.3 Marginalization If \\(P\\) is a joint probability distribution over event space \\(\\Omega = \\Omega_1 \\times \\dots \\Omega_n\\), the marginal distribution over subspace \\(\\Omega_i\\), \\(1 \\le i \\le n\\) is the probability distribution that assigns to all \\(A_i \\subseteq \\Omega_i\\) the probability (where notation \\(P(\\dots, \\omega, \\dots )\\) is shorthand for \\(P(\\dots, \\{\\omega \\}, \\dots)\\)):37 \\[ \\begin{align*} P(A_i) &amp; = \\sum_{\\omega_1 \\in \\Omega_{1}} \\sum_{\\omega_2 \\in \\Omega_{2}} \\dots \\sum_{\\omega_{i-1} \\in \\Omega_{i-1}} \\sum_{\\omega_{i+1} \\in \\Omega_{i+1}} \\dots \\sum_{\\omega_n \\in \\Omega_n} P(\\omega_1, \\dots, \\omega_{i-1}, A_{i}, \\omega_{i+1}, \\dots \\omega_n) \\end{align*} \\] For example, the marginal distribution of draws derivable from Table 7.1 has \\(P(\\text{black}) = P(\\text{heads, black}) + P(\\text{tails, black}) = 0.3\\) and \\(P(\\text{white}) = 0.7\\).38 The marginal distribution of coin flips derivable from the joint probability distribution in Table 7.1 gives \\(P(\\text{heads}) = P(\\text{tails}) = 0.5\\), since the sum of each column is exactly \\(0.5\\). Exercise 7.3 Given the following joint probability table, compute the probability that a student does not attend the lecture, i.e., \\(P(\\text{miss})\\). attend miss rainy 0.1 0.6 dry 0.2 0.1 Solution \\(P(\\text{miss}) = P(\\text{miss, rainy}) + P(\\text{miss, dry}) = 0.6 + 0.1 = 0.7\\) Play around with the following WebPPL implementation of the flip-and-draw scenario. Change the ‘input values’ of the coin’s bias and the probabilities of sampling a black ball from either urn. Inspect the resulting joint probability tables and the marginal distribution of observing “black”. Try to find at least three different parameter settings that result in the marginal probability of black being 0.7. // you can play around with the values of these variables var coin_bias = 0.5 // coin bias var prob_black_urn_1 = 0.2 // probability of drawing \"black\" from urn 1 var prob_black_urn_2 = 0.4 // probability of drawing \"black\" from urn 2 ///fold: // convenience function for showing nicer tables var condProb2Table = function(condProbFct, row_names, col_names, precision){ var matrix = map(function(row) { map(function(col) { _.round(Math.exp(condProbFct.score({\"coin\": row, \"ball\": col})),precision)}, col_names)}, row_names) var max_length_col = _.max(map(function(c) {c.length}, col_names)) var max_length_row = _.max(map(function(r) {r.length}, row_names)) var header = _.repeat(\" \", max_length_row + 2)+ col_names.join(\" \") + \"\\n\" var row = mapIndexed(function(i,r) { _.padEnd(r, max_length_row, \" \") + \" \" + mapIndexed(function(j,c) { _.padEnd(matrix[i][j], c.length+2,\" \")}, col_names).join(\"\") + \"\\n\" }, row_names).join(\"\") return header + row } // flip-and-draw scenario model var model = function() { var coin_flip = flip(coin_bias) == 1 ? \"heads\" : \"tails\" var prob_black_selected_urn = coin_flip == \"heads\" ? prob_black_urn_1 : prob_black_urn_2 var ball_color = flip(prob_black_selected_urn) == 1 ? \"black\" : \"white\" return({coin: coin_flip, ball: ball_color}) } // infer model and display as (custom-made) table var inferred_model = Infer({method: 'enumerate'}, model) display(\"Joint probability table\") display(condProb2Table(inferred_model, [\"tails\", \"heads\"], [\"white\", \"black\"], 3)) display(\"\\nMarginal probability of ball color\") viz(marginalize(inferred_model, function(x) {return x.ball})) /// Solution Three possibilities for obtaining a value of 0.7 for the marginal probability of “black”: prob_black_urn_1 = prob_black_urn_2 = 0.7 coin_bias = 1 and prob_black_urn_1 = 0.7 coin_bias = 0.5, prob_black_urn_1 = 0.8 and prob_black_urn_2 = 0.6 With \\(\\Omega_{\\text{flip}} = \\left \\{ \\text{heads}, \\text{tails} \\right \\}\\) and \\(\\Omega_{\\text{draw}} = \\left \\{ \\text{black}, \\text{white} \\right \\}\\).↩︎ This notation, using \\(\\sum\\), assumes that subspaces are countable. In other cases, a parallel definition with integrals can be used.↩︎ The term “marginal distribution” derives from such probability tables, where traditionally the sum of each row/column was written in the margins.↩︎ "],["Chap-03-01-probability-conditional.html", "7.3 Conditional probability", " 7.3 Conditional probability Let us assume probability distribution \\(P \\in \\Delta(\\Omega)\\) and that events \\(A,B \\subseteq \\Omega\\) are given. The conditional probability of \\(A\\) given \\(B\\), written as \\(P(A \\mid B)\\), gives the probability of \\(A\\) on the assumption that \\(B\\) is true.39 It is defined like so: \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\] Conditional probabilities are only defined when \\(P(B) &gt; 0\\).40 Example. If a die is unbiased, each of its six faces has equal probability to come up after a toss. The probability of event \\(B = \\{\\) ⚀, ⚂, ⚄ \\(\\}\\) that the tossed number is odd has probability \\(P(B) = \\frac{1}{2}\\). The probability of event \\(A = \\{\\) ⚂, ⚃, ⚄, ⚅ \\(\\}\\) that the tossed number is bigger than two is \\(P(A) = \\frac{2}{3}\\). The probability that the tossed number is bigger than two and odd is \\(P(A \\cap B) = P(\\{\\) ⚂, ⚄ \\(\\}) = \\frac{1}{3}\\). The conditional probability of tossing a number that is bigger than two, when we know that the toss is odd, is \\(P(A \\mid B) = \\frac{1 / 3}{1 / 2} = \\frac{2}{3}\\). Algorithmically, conditional probability first rules out all events in which \\(B\\) is not true and then simply renormalizes the probabilities assigned to the remaining events in such a way that their relative probabilities remain unchanged. Given this, another way of interpreting conditional probability is that \\(P(A \\mid B)\\) is what a rational agent should believe about \\(A\\) after observing (nothing more than) that \\(B\\) is true. The agent rules out, possibly hypothetically, that \\(B\\) is false, but otherwise does not change opinion about the relative probabilities of anything that is compatible with \\(B\\). This is also explained in the video embedded below. 7.3.1 Bayes rule Looking back at the joint-probability distribution in Table 7.1, the conditional probability \\(P(\\text{black} \\mid \\text{heads})\\) of drawing a black ball, given that the initial coin flip showed heads, can be calculated as follows: \\[ P(\\text{black} \\mid \\text{heads}) = \\frac{P(\\text{black} , \\text{heads})}{P(\\text{heads})} = \\frac{0.1}{0.5} = 0.2 \\] This calculation, however, is quite excessive. We can read out the conditional probability directly already from the way the flip-and-draw scenario was set up. After flipping heads, we draw from urn 1, which has \\(k=2\\) out of \\(N=10\\) black balls, so clearly: if the initial flip comes up heads, then the probability of a black ball is \\(0.2\\). Indeed, in a step-wise random generative process like the flip-and-draw scenario, some conditional probabilities are very clear, and sometimes given by definition. These are, usually, the conditional probabilities that define how the process unfolds forward in time, so to speak. Bayes rule is a way of expressing, in a manner of speaking, conditional probabilities in terms of the “reversed” conditional probabilities: \\[P(B \\mid A) = \\frac{P(A \\mid B) \\times P(B)}{P(A)}\\] Bayes rule is a straightforward corollary of the definition of conditional probabilities, according to which \\(P(A \\cap B) = P(A \\mid B) \\times P(B)\\), so that: \\[ P(B \\mid A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{P(A \\mid B) \\times P(B)}{P(A)} \\] Bayes rule allows for reasoning backward from observed causes to likely underlying effects. When we have a feed-forward model of how unobservable effects probabilistically constrain observable outcomes, Bayes rule allows us to draw inferences about latent/unobservable variables based on the observation of their downstream effects. Consider yet again the flip-and-draw scenario. But now assume that Jones flipped the coin and drew a ball. We see that it is black. What is the probability that it was drawn from urn 1, or equivalently, that the coin landed heads? It is not \\(P(\\text{heads}) = 0.5\\), the so-called prior probability of the coin landing heads. It is a conditional probability, also called the posterior probability,41 namely \\(P(\\text{heads} \\mid \\text{black})\\). But it is not as easy and straightforward to write down as the reverse probability \\(P(\\text{black} \\mid \\text{heads})\\) of which we said above that it is an almost trivial part of the set up of the flip-and-draw scenario. It is here that Bayes rule has its purpose: \\[ P(\\text{heads} \\mid \\text{black}) = \\frac{P(\\text{black} \\mid \\text{heads}) \\times P(\\text{heads})}{P(\\text{black})} = \\frac{0.2 \\times 0.5}{0.3} = \\frac{1}{3} \\] This result is quite intuitive. Drawing a black ball from urn 2 (i.e., after seeing tails) is twice as likely as drawing a black ball from urn 1 (i.e., after seeing heads). Consequently, after seeing a black ball drawn, with equal probabilities of heads and tails, the probability that the coin landed tails is also twice as large as that it landed heads. Exercise 7.4 Play around with the following WebPPL implementation of the flip-and-draw scenario, which calculates the posterior distribution over coin flip outcomes given that we observed the draw of a black ball. Change the parameters of the scenario and try to build intuitions about how your changes will affect the resulting posterior distribution. // you can play around with the values of these variables var coin_bias = 0.5 // coin bias var prob_black_urn_1 = 0.2 // probability of drawing \"black\" from urn 1 var prob_black_urn_2 = 0.4 // probability of drawing \"black\" from urn 2 ///fold: // flip-and-draw scenario model var model = function() { var coin_flip = flip(coin_bias) == 1 ? \"heads\" : \"tails\" var prob_black_selected_urn = coin_flip == \"heads\" ? prob_black_urn_1 : prob_black_urn_2 var ball_color = flip(prob_black_selected_urn) == 1 ? \"black\" : \"white\" condition(ball_color == \"black\") return({coin: coin_flip}) } // infer model and display as (custom-made) table var inferred_model = Infer({method: 'enumerate'}, model) viz(inferred_model) /// Solution Three possibilities for obtaining a value of 0.7 for the marginal probability of “black”: prob_black_urn_1 = prob_black_urn_2 = 0.7 coin_bias = 1 and prob_black_urn_1 = 0.7 coin_bias = 0.5, prob_black_urn_1 = 0.8 and prob_black_urn_2 = 0.6 Suppose that we know that around 6% of the population has statisticositis, a rare disease that makes you allergic to fallacious statistical reasoning. A new test has been developed to diagnose statisticositis but it is not infallible. The specificity of the test (the test result is negative when the subject really does not have statisticositis) is 98%. The sensitivity of the test (the test result is positive when the subject really does have statisticositis) is 95%. When you take this test and it gives a negative test result, how likely is it that you do not have statisticositis? Solution First, let’s abbreviate the test result being negative or positive as \\(\\overline{T}\\) and \\(T\\) and actual statisticositis as \\(\\overline{S}\\) and \\(S\\). We want to calculate \\(P(\\overline{S} \\mid \\overline{T})\\). According to Bayes rule, \\(P(\\overline{S} \\mid \\overline{T}) = \\frac{P(\\overline{T} \\mid \\overline{S}) P(\\overline{S})} {P(\\overline{T})}\\). We are given that \\(P(\\overline{T} \\mid \\overline{S}) = 0.98\\), \\(P(\\overline{T} \\mid S) = 1 - P(T \\mid S) = 0.05\\) and \\(P(\\overline{S}) = 1 - P(S) = 0.94\\). Furthermore, \\(P(\\overline{T}) = P(\\overline{T},S) + P(\\overline{T},\\overline{S}) = P(\\overline{T} \\mid S) P(S) + P(\\overline{T} \\mid \\overline{S}) P(\\overline{S}) = 0.9242\\). Putting this all together, we get \\(P(\\overline{S} \\mid \\overline{T}) \\approx 99.7 \\%\\). So, given a negative test result, you can be pretty certain that you do not have statisticositis. Check out this website for more details on these calculations in the context of a more serious application. Excursion: Bayes rule for data analysis In later chapters, we will use Bayes rule for data analysis. The flip-and-draw scenario structurally “reflects” what will happen later. Think of the color of the ball drawn as the data \\(D\\) which we observe. Think of the coin as a latent parameter \\(\\theta\\) of a statistical model. Bayes rule for data analysis then looks like this: \\[P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) \\times P(\\theta)}{P(D)}\\] We will discuss this at length in Chapter 8 and thereafter. 7.3.2 Stochastic (in-)dependence Event \\(A\\) is stochastically independent of \\(B\\) if, intuitively speaking, learning \\(B\\) does not change one’s beliefs about \\(A\\), i.e., \\(P(A \\mid B) = P(A)\\). If \\(A\\) is stochastically independent of \\(B\\), then \\(B\\) is stochastically independent of \\(A\\) because: \\[ \\begin{aligned} P(B \\mid A) &amp; = \\frac{P(A \\mid B) \\ P(B)}{P(A)} &amp;&amp; \\text{[Bayes rule]} \\\\ &amp; = \\frac{P(A) \\ P(B)}{P(A)} &amp;&amp; \\text{[by ass. of independence]} \\\\ &amp; = P(B) &amp;&amp; \\text{[cancellation]} \\\\ \\end{aligned} \\] For example, imagine a flip-and-draw scenario where the initial coin flip has a bias of \\(0.8\\) towards heads, but each of the two urns has the same number of black balls, namely \\(3\\) black and \\(7\\) white balls. Intuitively and formally, the probability of drawing a black ball is then independent of the outcome of the coin flip; learning that the coin landed heads, does not change our beliefs about how likely the subsequent draw will result in a black ball. The probability table for this example is in Table 7.2. Table 7.2: Joint probability table for a flip-and-draw scenario where the coin has a bias of \\(0.8\\) towards heads and where each of the two urns holds \\(3\\) black and \\(7\\) white balls. heads tails \\(\\Sigma\\) rows black \\(0.8 \\times 0.3 = 0.24\\) \\(0.2 \\times 0.3 = 0.06\\) 0.3 white \\(0.8 \\times 0.7 = 0.56\\) \\(0.2 \\times 0.7 = 0.14\\) 0.7 \\(\\Sigma\\) columns 0.8 0.2 1.0 Independence shows in Table 7.2 in the fact that the probability in each cell is the product of the two marginal probabilities. This is a direct consequence of stochastic independence: Proposition 7.1 (Probability of conjunction of stochastically independent events) For any pair of events \\(A\\) and \\(B\\) with non-zero probability: \\[P(A \\cap B) = P(A) \\ P(B) \\, \\ \\ \\ \\ \\text{[if } A \\text{ and } B \\text{ are stoch. independent]} \\] Show proof. Proof. By assumption of independence, it holds that \\(P(A \\mid B) = P(A)\\). But then: \\[ \\begin{aligned} P(A \\cap B) &amp; = P(A \\mid B) \\ P(B) &amp;&amp; \\text{[def. of conditional probability]} \\\\ &amp; = P(A) \\ P(B) &amp;&amp; \\text{[by ass. of independence]} \\end{aligned} \\]   References "],["Chap-03-01-probability-random-variables.html", "7.4 Random variables", " 7.4 Random variables So far, we have defined a probability distribution as a function that assigns a probability to each subset of the space \\(\\Omega\\) of elementary outcomes. We saw that rational beliefs should conform to certain axioms, reflecting a “logic of rational beliefs”. But in data analysis, we are often interested in a space of numeric outcomes. You probably know stuff like the “normal distribution” which is a distribution that assigns a probability to each real number. In keeping with our previous definition of probability as targeting a measurable set \\(\\Omega\\), we introduce what we could sloppily call “probability distributions over numbers” using the concept of random variables. Caveat: random variables are very useful concepts and offer highly versatile notation, but both concept and notation can be elusive in the beginning. Formally, a random variable is a function \\(X \\ \\colon \\ \\Omega \\rightarrow \\mathbb{R}\\) that assigns to each elementary outcome a numerical value. It is reasonable to think of this number as a summary statistic: a number that captures one aspect of relevance of what is actually a much more complex chunk of reality. Example. For a single coin flip, we have \\(\\Omega_{\\text{coin flip}} = \\left \\{ \\text{heads}, \\text{tails} \\right \\}\\). A usual way of mapping this onto numerical outcomes is to define \\(X_{\\text{coin flip}} \\ \\colon \\ \\text{heads} \\mapsto 1; \\text{tails} \\mapsto 0\\). Less trivially, consider flipping a coin two times. Elementary outcomes should be individuated by the outcome of the first flip and the outcome of the second flip, so that we get: \\[ \\Omega_{\\text{two flips}} = \\left \\{ \\langle \\text{heads}, \\text{heads} \\rangle, \\langle \\text{heads}, \\text{tails} \\rangle, \\langle \\text{tails}, \\text{heads} \\rangle, \\langle \\text{tails}, \\text{tails} \\rangle \\right \\} \\] Consider the random variable \\(X_{\\text{two flips}}\\) that counts the total number of heads. Crucially, \\(X_{\\text{two flips}}(\\langle \\text{heads}, \\text{tails} \\rangle) = 1 = X_{\\text{two flips}}(\\langle \\text{tails}, \\text{heads} \\rangle)\\). We assign the same numerical value to different elementary outcomes since the order is not relevant if we are only interested in a count of the number of heads. 7.4.1 Notation &amp; terminology Traditionally, random variables are represented by capital letters, like \\(X\\). The numeric values they take on are written as small letters, like \\(x\\). We write \\(P(X = x)\\) as a shorthand for the probability \\(P(\\left \\{ \\omega \\in \\Omega \\mid X(\\omega) = x \\right \\})\\), that an event \\(\\omega\\) occurs which is mapped onto \\(x\\) by the random variable \\(X\\). For example, if our coin is fair, then \\(P(X_{\\text{two flips}} = x) = 0.5\\) for \\(x=1\\) and \\(0.25\\) for \\(x \\in \\{0,2\\}\\). Similarly, we can also write \\(P(X \\le x)\\) for the probability of observing any event that \\(X\\) maps to a number not bigger than \\(x\\). If the range of \\(X\\) is countable (not necessarily finite), we say that \\(X\\) is discrete. For ease of exposition, we may say that if the range of \\(X\\) is an interval of real numbers, \\(X\\) is called continuous. 7.4.2 Cumulative distribution functions, mass &amp; density For a discrete random variable \\(X\\), the cumulative distribution function \\(F_X\\) associated with \\(X\\) is defined as: \\[ F_X(x) = P(X \\le x) = \\sum_{x&#39; \\in \\left \\{ \\text{Rng}(X) \\mid x&#39; \\le x \\right \\}} P(X = x) \\] The probability mass function \\(f_x\\) associated with \\(X\\) is defined as: \\[ f_X(x) = P(X = x) \\] Example. Suppose we flip a coin with a bias of \\(\\theta\\) towards heads \\(n\\) times. What is the probability that we will see heads \\(k\\) times? If we map the outcome of heads to 1 and tails to 0, this probability is given by the Binomial distribution, as follows: \\[ \\text{Binom}(K = k ; n, \\theta) = \\binom{n}{k} \\, \\theta^{k} \\, (1-\\theta)^{n-k} \\] Here \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) is the binomial coefficient, which gives the number of possibilities of drawing an unordered subset with \\(k\\) elements from a set with a total of \\(n\\) elements. Figure 7.3 gives examples of the Binomial distribution, concretely its probability mass functions, for two values of the coin’s bias, \\(\\theta = 0.25\\) or \\(\\theta = 0.5\\), when flipping the coin \\(n=24\\) times. Figure 7.4 gives the corresponding cumulative distributions. Figure 7.3: Examples of the Binomial distribution. The \\(y\\)-axis gives the probability of seeing \\(k\\) heads when flipping a coin \\(n=24\\) times with a bias of either \\(\\theta = 0.25\\) or \\(\\theta = 0.5\\). Figure 7.4: Examples of the cumulative distribution of the Binomial distribution. The \\(y\\)-axis gives the probability of seeing \\(k\\) or fewer outcomes of heads when flipping a coin \\(n=24\\) times with a bias of either \\(\\theta = 0.25\\) or \\(\\theta = 0.5\\). For a continuous random variable \\(X\\), the probability \\(P(X = x)\\) will usually be zero: it is virtually impossible that we will see precisely the value \\(x\\) realized in a random event that can realize uncountably many numerical values of \\(X\\). However, \\(P(X \\le x)\\) does usually take non-zero values and so we define the cumulative distribution function \\(F_X\\) associated with \\(X\\) as: \\[ F_X(x) = P(X \\le x) \\] Instead of a probability mass function, we derive a probability density function from the cumulative function as: \\[ f_X(x) = F&#39;(x) \\] A probability density function can take values greater than one, unlike a probability mass function. Example. The Gaussian (Normal) distribution characterizes many natural distributions of measurements which are symmetrically spread around a central tendency. It is defined as: \\[ \\mathcal{N}(X = x ; \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}} \\exp \\left ( - \\frac{(x-\\mu)^2}{2 \\sigma^2} \\right) \\] where parameter \\(\\mu\\) is the mean, the central tendency, and parameter \\(\\sigma\\) is the standard deviation. Figure 7.5 gives examples of the probability density function of two normal distributions. Figure 7.6 gives the corresponding cumulative distribution functions. Figure 7.5: Examples of the Normal distribution. In both cases \\(\\mu = 0\\), once with \\(\\sigma = 1\\) and once with \\(\\sigma = 4\\). Figure 7.6: Examples of the cumulative normal distribution corresponding to the previous probability density functions. 7.4.3 Expected value &amp; variance The expected value of a random variable \\(X\\) is a measure of central tendency. It tells us, like the name suggests, which average value of \\(X\\) we can expect when repeatedly sampling from \\(X\\). If \\(X\\) is discrete, the expected value is: \\[ \\mathbb{E}_X = \\sum_{x} x \\times f_X(x) \\] If \\(X\\) is continuous, it is: \\[ \\mathbb{E}_X = \\int x \\times f_X(x) \\ \\text{d}x \\] The expected value is also frequently called the mean. The variance of a random variable \\(X\\) is a measure of how much likely values of \\(X\\) are spread or clustered around the expected value. If \\(X\\) is discrete, the variance is: \\[ \\text{Var}(X) = \\sum_x (\\mathbb{E}_X - x)^2 \\times f_X(x) = \\mathbb{E}_{X^2} -\\mathbb{E}_X^2 \\] If \\(X\\) is continuous, it is: \\[ \\text{Var}(X) = \\int (\\mathbb{E}_X - x)^2 \\times f_X(x) \\ \\text{d}x = \\mathbb{E}_{X^2} -\\mathbb{E}_X^2 \\] Example. If we flip a coin with bias \\(\\theta = 0.25\\) a total of \\(n=24\\) times, we expect on average to see \\(n \\times\\theta = 24 \\times 0.25 = 6\\) outcomes showing heads.42 The variance of a binomially distributed variable is \\(n \\times\\theta \\times(1-\\theta) = 24 \\times 0.25 \\times 0.75 = \\frac{24 \\times 3}{16} = \\frac{18}{4} = 4.5\\). The expected value of a normal distribution is just its mean \\(\\mu\\) and its variance is \\(\\sigma^2\\). Exercise 7.5 Compute the expected value and variance of a fair die. Solution expected_value &lt;- 1*(1/6) + 2*(1/6) + 3*(1/6) + 4*(1/6) + 5*(1/6) + 6*(1/6) variance &lt;- 1^2*(1/6) + 2^2*(1/6) + 3^2*(1/6) + 4^2*(1/6) + 5^2*(1/6) + 6^2*(1/6) - expected_value^2 print(expected_value) ## [1] 3.5 variance ## [1] 2.916667 Below, you see several normal distributions with differing means \\(\\mu\\) and standard deviations \\(\\sigma\\). The red, unnumbered distribution is the so-called standard normal distribution; it has a mean of 0 and a standard deviation of 1. Compare each distribution below (1-4) to the standard normal distribution and think about how the parameters of the standard normal were changed. Also, think about which distribution (1-4) has the smallest/largest mean and the smallest/largest standard deviation. Solution Distribution 1 (\\(\\mu\\) = 5, \\(\\sigma\\) = 1): larger mean, same standard deviation Distribution 2 (\\(\\mu\\) = 0, \\(\\sigma\\) = 3): same mean, larger standard deviation Distribution 3 (\\(\\mu\\) = 6, \\(\\sigma\\) = 2): larger mean, larger standard deviation Distribution 4 (\\(\\mu\\) = -6, \\(\\sigma\\) = 0.5): smaller mean, smaller standard deviation 7.4.4 Composite random variables Composite random variables are random variables generated by mathematical operations conjoining other random variables. For example, if \\(X\\) and \\(Y\\) are random variables, then we can define a new derived random variable \\(Z\\) using notation like: \\[Z = X + Y\\] This notation looks innocuous but is conceptually tricky yet ultimately very powerful. On the face of it, we are doing as if we are using + to add two functions. But a sampling-based perspective makes this quite intuitive. We can think of \\(X\\) and \\(Y\\) as large samples, representing the probability distributions in question. Then we build a sample by just adding elements in \\(X\\) and \\(Y\\). (If samples are of different size, just add a random element of \\(Y\\) to each \\(X\\).) Consider the following concrete example. \\(X\\) is the probability distribution of rolling a fair dice with six sides. \\(Y\\) is the probability distribution of flipping a biased coin that lands heads (represented as number 1) with probability 0.75. The derived probability distribution \\(Z = X + Y\\) can be approximately represented by samples derived as follows: n_samples &lt;- 1e6 # `n_samples` rolls of a fair dice samples_x &lt;- sample( 1:6, size = n_samples, replace = T ) # `n_samples` flips of a biased coin samples_y &lt;- sample( c(0, 1), prob = c(0.25, 0.75), size = n_samples, replace = T ) samples_z &lt;- samples_x + samples_y tibble(outcome = samples_z) %&gt;% dplyr::count(outcome) %&gt;% mutate(n = n / sum(n)) %&gt;% ggplot(aes(x = outcome, y = n)) + geom_col() + labs(y = &quot;proportion&quot;) This is not immediately obvious from our definition, but it is intuitive and you can derive it.↩︎ "],["Chap-03-01-probability-R.html", "7.5 Probability distributions in R", " 7.5 Probability distributions in R Appendix ?? covers a number of common probability distributions that are relevant for the purposes of this course. Appendix ?? furthermore provides additional theoretical background on the exponential family, an important class of probability distributions widely used in statistics. R has built-in functions for most common probability distributions. Further distributions are covered in additional packages. If mydist is the name of a probability distribution, then R routinely offers four functions for mydist, distinguished by the first letter: dmydist(x, ...) the density function gives the probability (mass/density) \\(f(x)\\) for x pmydist(x, ...) the cumulative probability function gives the cumulative distribution function \\(F(x)\\) for x qmydist(p, ...) the quantile function gives the value x for which p = pmydist(x, ...) rmydist(n, ...) the random sample function returns n samples from the distribution For example, the family of functions for the normal distribution has the following functions: # density of standard normal at x = 1 dnorm(x = 1, mean = 0, sd = 1) ## [1] 0.2419707 # cumulative density of standard normal at q = 0 pnorm(q = 0, mean = 0, sd = 1) ## [1] 0.5 # point where the cumulative density of standard normal is p = 0.5 qnorm(p = 0.5, mean = 0, sd = 1) ## [1] 0 # n = 3 random samples from a standard normal rnorm(n = 3, mean = 0, sd = 1) ## [1] 0.8894700 1.5003542 0.3971384 Exercise 7.6 Use R to compute the median of the exponential distribution with rate \\(\\lambda = 1\\). Remember that the median is the 50% quantile. The quantile function of the exponential distribution can be accessed with qexp in R. Solution qexp(0.5, rate = 1) ## [1] 0.6931472 Use R’s function for the cumulative normal distribution (see above) to compute this integral, i.e., the area under the density function of a standard normal distribution ranging from -1 to 2: \\[ \\int_{-1}^{2} \\mathcal{N}(x, \\mu = 0, \\sigma = 1) \\text{d}x \\] Solution pnorm(2, mean = 0, sd = 1) - pnorm(-1, mean = 0, sd = 1) ## [1] 0.8185946 "],["Chap-03-03-models.html", "8 Statistical models", " 8 Statistical models Uninterpreted data is uninformative. We cannot generalize, draw inferences or attempt to make predictions unless we make (however minimal) assumptions about the data at hand: what it represents, how it came into existence, which parts relate to which other parts, etc. One way of explicitly acknowledging these assumptions is to engage in model-based data analysis. A statistical model is a conventionally condensed formal representation of the assumptions we make about what the data is and how it might have been generated. In this way, model-based data analysis is more explicit about the analyst’s assumptions than other approaches, such as test-based approaches, which we will encounter in Chapter ??. There is room for divergence in how to think about a statistical model, the assumptions it encodes and the truth. Some will want to reason with models using language like “if we assume that model \\(M\\) is true, then …” or “this shows convincingly that \\(M\\) is likely to be the true model”. Others feel very uncomfortable with such language. In times of heavy discomfort they might repeat their soothing mantra: All models are wrong, but some are useful. — Box (1979) To become familiar with model-based data analysis, Section 8.1 introduces the concept of a probabilistic statistical model. Section 8.2 expands on the notation, both formulaic and graphical, which we will use in this book to communicate about models. Finally, Section 8.3 enlarges on the crucial aspects of parameters and priors. The learning goals for this chapter are: become familiar with the notion of a (Bayesian) statistical model understand the key ingredients of a model: likelihood function, parameters, prior, prior distribution understand notation to communicate models formulas &amp; graphs References "],["Chap-03-03-models-general.html", "8.1 Statistical models", " 8.1 Statistical models In its most common natural sense, a “model” is a model of something. It intends to represent something else in a condensed, abstract, and more practical form; where what is practical is conditioned by a given purpose. For any given purpose, a good model will try to represent some aspects of reality and abstract away from irrelevant features that might otherwise blur our vision. The most common purpose of a statistical model is to either learn something about reality by drawing inferences from data - possibly with the ulterior goal of making an informed practical decision - or to make predictions about unknown events (future, present or past unknowns). A statistical model \\(M\\) is a model of a random process \\(R\\) that could have generated some kind of observable data that we are interested in.43 The model \\(M\\) is then a formally precise formulation of our assumptions about this random process \\(R\\). Often, we want to explain some part of our data observations, the dependent variable(s) \\(D_{\\text{DV}}\\), in terms of some other observations, the independent variables \\(D_{\\text{IV}}\\) (see Chapter 3.3 for more on the notion of (in-)dependent variables). But it is also possible that there are no independent variables in terms of which we would like to model the dependent variable \\(D_{\\text{DV}}\\). A model \\(M\\) for data \\(D\\) fixes a likelihood function for \\(D_\\text{DV}\\). The likelihood function determines how likely any potential data observation \\(D_\\text{DV}\\) is, given the corresponding observations in \\(D_\\text{IV}\\). Most often, the likelihood function also has free parameters, represented by a parameter vector \\(\\theta\\). The basic (and yet rather uninformative) notation for a likelihood function of model \\(M\\) for data \\(D\\) with parameter vector \\(\\theta\\) is therefore:44 \\[ P_M(D_\\text{DV} \\mid D_\\text{IV}, \\theta) \\] Bayesian models have an additional component, namely a prior distribution over parameter values, commonly written as: \\[ P_M(\\theta) \\]. The Bayesian prior over parameter values can be used to regularize inference and/or to represent any motivated and justifiable a priori assumptions about parameter values that are plausible given our knowledge so far. Section 8.3 elaborates on parameters and priors. But first, we should take a look at an example, which we will use in the remainder of this chapter for further illustration. Example: Binomial Model. The data we are interested in comes from a sequence of flips of a coin with bias \\(\\theta_c \\in [0;1]\\). We have observed that \\(k\\) of the \\(N\\) flips turned out to be heads. We know \\(N\\) and \\(k\\), but we do not know \\(\\theta_c\\). We will use the Binomial Model in later sections to infer the latent (= not directly observable) coin bias \\(\\theta_c\\). The coin’s bias \\(\\theta_c\\) is the only parameter of this model. The dependent variable is \\(k\\). \\(N\\) is another data observation (treated here as an independent variable45). The likelihood function for this model is the Binomial distribution: \\[ P_M(k \\mid \\theta_c, N) = \\text{Binomial}(k, N, \\theta_c) = \\binom{N}{k}\\theta_c^k(1-\\theta_c)^{N-k} \\] For reasons that will become clear later, we use a Beta distribution for the prior of \\(\\theta_c\\). For example, we can use parameters so that the ensuing distribution is flat (a so-called “uninformative prior”; more on this below): \\[ P_M(\\theta_c) = \\text{Beta}(\\theta_c, 1, 1) \\] There are three main uses for models in statistical data analysis: Prediction: Models can be used to make predictions about future or hypothetical data observations. We will see an example of this in Section 8.3 in this chapter. Parameter estimation: Based on model \\(M\\) and data \\(D\\), we try to infer which value of the parameter vector \\(\\theta\\) we should believe in or work with (e.g., base our decision on). Parameter estimation can also serve knowledge gain, especially if (some component of) \\(\\theta\\) is theoretically interesting. We will deal with parameter estimation in Chapter 9. Model comparison: If we formulate at least two alternative models, we can ask which model better explains or better predicts some data. In some of its guises, model comparison helps with the question of whether a given data set provides evidence in favor of one model and against another other, and if so, how much. Model comparison is the topic of Chapter 10. In most common parlance, however, we often speak of “a model of the data” or of “modeling the data”, but this is only sloppy slang for “a model of (what we assume is) a random process that could generate data of the relevant kind”.↩︎ Since in many contexts the meaning will be clear enough, we follow the common practice and write \\(P(D \\mid \\theta)\\) as a shortcut for the, strictly speaking, correct but cumbersome \\(P(\\mathcal{D} = D \\mid \\Theta = \\theta)\\). In this latter notation, \\(\\mathcal{D}\\) is the class of all relevant observable data and \\(\\Theta\\) is the range of a possibly high-dimensional vector of parameter values. We diverge from the common practice of using capital roman letters for random variables and small roman letters for values from these random variables because parameter vectors are traditionally written as \\(\\theta\\) and the small letter \\(\\textrm{d}\\) (albeit non-italic) is reserved for differentials.↩︎ It is fair to treat \\(N\\) as an independent variable if it was determined at the beginning of the experiment (= sequence of flips), so that the only dependent measure is the number \\(k\\) of head outcomes for fixed \\(N\\).↩︎ "],["Chap-03-03-models-representation.html", "8.2 Notation &amp; graphical representation", " 8.2 Notation &amp; graphical representation If it is important to communicate the assumptions underlying a statistical argument, and if models are means of making these assumptions formally explicit, then it follows that efficient communication of models is important too. We here follow the common practice of representing models using a special purpose formulaic notation and, where useful, a graph-based visual display in which probabilistic dependencies are lucidly represented. Recall that the Binomial Model has a binomial likelihood function: \\[ P_M(k \\mid \\theta_c, N) = \\text{Binomial}(k, N, \\theta_c) = \\binom{N}{k}\\theta_c^k(1-\\theta_c)^{N-k} \\] And a Beta distribution as a prior, e.g., with shape parameters set so that all values of \\(\\theta_c\\) are equally likely. \\[ P_M(\\theta_c) = \\text{Beta}(\\theta_c, 1, 1) \\] 8.2.1 Formula notation To concisely represent models, we use a special notation, which is very intuitive when we think about sampling. Instead of the above notation for the prior we write: \\[ \\theta_c \\sim \\text{Beta}(1,1) \\] The symbol “\\(\\sim\\)” is often read as “is distributed as”. You can also think of it as meaning that \\(\\theta_c\\) is sampled from a \\(\\text{Beta}(1,1)\\) distribution. Similarly, for the likelihood function, we just write: \\[k \\sim \\text{Binomial}(\\theta_c, N).\\] 8.2.2 Graphical notation When models get very complex and incorporate many parameters, it can be difficult to tease out the relations between all of the model’s components. In such a situation a graphical notation of a model is helpful. We here adopt the conventions described by Lee and Wagenmakers (2014). We represent every relevant variable as a node in a directed acyclic graph structure (a probabilistic network). The graph structure is used to indicate dependencies between the variables, with children depending on their parents. In visualizing this, we use the following general conventions: known or unknown (= latent) variable shaded nodes: observed variables unshaded nodes: unobserved / latent variables kind of variable: circular nodes: continuous variables square nodes: discrete variables kind of dependency: single line: stochastic dependency double line: deterministic dependency For the Binomial Model this results in the relevant variables: number of trials (\\(N\\)) number of success (\\(k\\)) probability for success (\\(\\theta_c\\)) Of these, \\(N\\) and \\(k\\) are observed and discrete variables, and \\(\\theta_c\\) is a latent continuous variable. Clearly, the number of heads \\(k\\) depends on the coin bias \\(\\theta_c\\) as well as on the number of trials \\(N\\). This yields a graphical and formulaic notation as in Figure 8.1. Figure 8.1: The Binomial Model. Notice that any specific Beta prior shape would yield what we here call a Binomial Model, which is why there are no concrete shape parameters given in this graph. References "],["Chap-03-03-models-parameters-priors.html", "8.3 Parameters, priors, and prior predictions", " 8.3 Parameters, priors, and prior predictions We defined a Bayesian model as a pair consisting of a parameterized likelihood function and a prior distribution over parameter values: \\[ \\begin{aligned} &amp; \\text{Likelihood: } &amp; P_M(D \\mid \\theta) \\\\ &amp; \\text{Prior: } &amp; P_M(\\theta) \\end{aligned} \\] In this section, we dive deeper into what a parameter is, what a prior distribution \\(P_M(\\theta)\\) is, and how we can use a model to make predictions about data. The running example for this section is the Binomial Model as introduced above. As a concrete example of data, we consider a case with \\(N=24\\) coin flips and \\(k=7\\) head outcomes. 8.3.1 What’s a model parameter? A model parameter is a value that the likelihood depends on. In the graphical notation we introduced in Section 8.2, parameters usually (but not necessarily) show up as white nodes, because they are unknowns. For example, the single parameter \\(\\theta_c\\) in the Binomial Model shapes or fine-tunes the likelihood function. Remember that the likelihood function for the Binomial Model is: \\[ P_M(k \\mid \\theta_c, N) = \\text{Binomial}(k, N, \\theta_c) = \\binom{N}{k}\\theta_c^k(1-\\theta_c)^{N-k} \\] To understand the role of the parameter \\(\\theta_c\\), we can plot the likelihood of the observed data (here: \\(k=7\\) and \\(N=24\\)) as a function of \\(\\theta_c\\). This is what is shown in Figure 8.2. For each logically possible value of \\(\\theta_c \\in [0;1]\\) on the horizontal axis, Figure 8.2 plots the resulting likelihood of the observed data on the vertical axis. What this plot shows is how the likelihood function depends on its parameter \\(\\theta_c\\). Different values of \\(\\theta_c\\) make the data we observed more or less likely. Figure 8.2: Likelihood function for the Binomial Model, for \\(k=7\\) and \\(N=24\\). Exercise 8.1 Use R to calculate how likely it is to get \\(k=22\\) heads when tossing a coin with bias \\(\\theta_c = 0.5\\) a total of \\(N=100\\) times. Solution dbinom(22, size = 100, prob = 0.5, log = FALSE) ## [1] 5.783981e-09 Which parameter value, \\(\\theta_c = 0.4\\) or \\(\\theta_c = 0.6\\), makes the data from the previous part of this exercise (\\(N=100\\) and \\(k=22\\)) more likely? - Give a reason for your intuitive guess and use R to check your intuition. Solution The number of heads \\(k=22\\) is (far) less than half of the total number of coin flips \\(N=100\\). This should be more likely for a bias towards tails than for a bias towards heads. So, we might assume that \\(\\theta_c=0.4\\) makes the data more likely than \\(\\theta_c = 0.6\\). dbinom(22, size = 100, prob = 0.4, log = FALSE) ## [1] 6.402414e-05 dbinom(22, size = 100, prob = 0.6, log = FALSE) ## [1] 8.815222e-15 8.3.2 Priors over parameters The prior distribution over parameter values \\(P_M(\\theta)\\) is an integral part of a model when we adopt a Bayesian approach to data analysis. This entails that two (Bayesian) models can share the same likelihood function, and yet ought to be considered as different models. (This also means that, when we say “Binomial Model” we really mean a whole class of models, all varying in the prior on \\(\\theta\\).) In Bayesian data analysis, priors \\(P_M(\\theta)\\) are most saliently interpreted as encoding the modeler’s prior beliefs about the parameters in question. Ideally, the beliefs that support the specification of a prior should be supported by an argument, results of previous research, or other justifiable motivations. However, informed subjective priors are just one of the ways to justify priors over parameters. There are three main types of motivations for priors \\(P_M(\\theta)\\); though the choice of a particular prior for a particular application might have mixed motives. Subjective priors capture the modeler’s genuine subjective beliefs in the sense described above. Practical priors are priors that are used pragmatically because of their specific usefulness, e.g., because they simplify a mathematical calculation or a computer simulation, or because they help in statistical reasoning, such as when skeptical priors are formulated that work against a particular conclusion. Objective priors are priors that, as some argue, should be adopted for a given likelihood function to avoid conceptually paradoxical consequences. We will not deal with objective priors in this introductory course beyond mentioning them here for completeness. Orthogonal to the kind of motivation given for a prior, we can distinguish different priors based on how strongly they commit the modeler to a particular range of parameter values. The most extreme case of ignorance are uninformative priors which assign the same level of credence to all parameter values. Uninformative priors are also called flat priors because they express themselves as flat lines for discrete probability distributions and continuous distributions defined over an interval with finite lower and upper bounds.46 Informative priors, on the other hand, can be weakly informative or strongly informative, depending on how much commitment they express. The most extreme case of commitment would be expressed in a point-valued prior, which puts all probability (mass or density) on a single value of a parameter. Since this is no longer a respectable probability distribution, although it satisfies the definition, we speak of a degenerate prior here. Figure 8.3 shows examples of uninformative, weakly or strongly informative priors, as well as point-valued priors for the Binomial Model. The priors shown here (resulting in four different Bayesian models all falling inside the family of Binomial Models) are as follows: uninformative : \\(\\theta_c \\sim \\text{Beta}(1,1)\\) weakly informative : \\(\\theta_c \\sim \\text{Beta}(5,2)\\) strongly informative : \\(\\theta_c \\sim \\text{Beta}(50,20)\\) point-valued : \\(\\theta_c \\sim \\text{Beta}(\\alpha, \\beta)\\) with \\(\\alpha, \\beta \\rightarrow \\infty\\) and \\(\\frac{\\alpha}{\\beta} = \\frac{5}{2}\\) Figure 8.3: Examples of different kinds of Bayesian priors for coin bias \\(\\theta_c\\) in the Binomial Model. 8.3.3 Prior predictions How should priors be specified for a Bayesian model? Several aspects might inform this decision. Practical considerations may matter (maybe the model can only be implemented and run with common software for certain priors). If subjective beliefs play a role, it may be hard to specify an exact shape of the prior distribution over some or all parameters, especially when these parameters are not easily interpretable in an intuitive way. Therefore, two principles for the specification of priors are important: Sensitivity analysis: Researchers should always check diligently whether or how much their results depend on the specific choices of priors, e.g., by running the same analysis with a wide range of different priors. Inspecting the prior predictive distribution: It is one thing to ask whether a particular value for some parameter makes intuitive or conceptual sense. It is another at least as important question whether the predictions that the model makes about the data are intuitively or conceptually reasonable from an a priori perspective.47 Indeed, by specifying priors over parameter values, Bayesian models make predictions about how likely a particular data outcome is, even before having seen any data at all. The (Bayesian) prior predictive distribution of model \\(M\\) is a probability distribution over future or hypothetical data observations, written here as \\(D_{\\text{pred}}\\) for “predicted data”: \\[ \\begin{aligned} P_M(D_{\\text{pred}}) &amp; = \\sum_{\\theta} P_M(D_{\\text{pred}} \\mid \\theta) \\ P_M(\\theta) &amp;&amp; \\text{[discrete parameter space]} \\\\ P_M(D_{\\text{pred}}) &amp; = \\int P_M(D_{\\text{pred}} \\mid \\theta) \\ P_M(\\theta) \\ \\text{d}\\theta &amp;&amp; \\text{[continuous parameter space]} \\end{aligned} \\] The formula above is obtained by marginalization over parameter values (represented here as an integral for the continuous case). We can think of the prior predictive distribution also in terms of samples. We want to know how likely a given logically possible data observation \\(D_{\\text{pred}}\\) is, according to the model with its a priori distribution over parameters. So we sample, repeatedly, parameter vectors \\(\\theta\\) from the prior distribution. For each sampled \\(\\theta\\), we then sample a potential data observation \\(D_{\\text{pred}}\\). The prior predictive distribution captures how likely it is under this sampling process to see each logically possible data observation \\(D_{\\text{pred}}\\). Notice that this sampling process corresponds exactly to the way in which we write down models using the conventions laid out in Section 8.2, underlining once more how a model is really a representation of a random process that could have generated the data. In the case of the Binomial Model when we use a Beta prior over \\(\\theta\\), the prior predictive distribution is so prominent that it has its own name and fame. It’s called the Beta-binomial distribution. Figure 8.4 shows the prior predictions for the four kinds of priors from Figure 8.3 when \\(N = 24\\). Figure 8.4: Prior predictive distributions for Binomial Models with the Beta-priors from the previous figure. It is possible to use uninformative priors also for continuous distributions defined over an unbounded interval, in which case we speak of improper priors (to remind ourselves that, mathematically, we are doing something tricky).↩︎ Obviously, priors should not be chosen after having seen the data in such a way that they engineer in the conclusions that a researcher wants to reach because of prior conviction, pride or ambition.↩︎ "],["ch-03-04-parameter-estimation.html", "9 Bayesian parameter estimation", " 9 Bayesian parameter estimation Based on a model \\(M\\) with parameters \\(\\theta\\), parameter estimation addresses the question of which values of \\(\\theta\\) are good estimates, given some data \\(D\\). This chapter deals specifically with Bayesian parameter estimation. Given a Bayesian model \\(M\\), we can use Bayes rule to update prior beliefs about \\(\\theta\\) to obtain so-called posterior beliefs \\(P_M(\\theta \\mid D)\\), which represent the new beliefs after observing \\(D\\) and updating in a conservative, rational manner based on the assumptions spelled out in \\(M\\). We will see two different methods of computing posterior distributions \\(P_M(\\theta \\mid D)\\), a precise mathematical derivation with limited applicability in terms of so-called conjugate priors, and an efficient but approximate sampling method based on so-called Markov Chain Monte Carlo algorithms. The chapter also introduces common point-valued and interval-ranged estimates for parameters, in particular the Bayesian measures of posterior mean and credible intervals. We will also learn about the posterior predictive distribution and how to draw inferences about hypotheses about specific parameter values. The learning goals for this chapter are: understand how Bayes rule applies to parameter estimation role of prior and likelihood understand the notion of conjugate prior understand and compute point-valued and interval-range estimators MLE, MAP, posterior mean (Bayesian) credible intervals understand the basic ideas behind MCMC sampling algorithms understand the notion of a posterior predictive distribution learn how to use Bayesian parameter inference to test hypotheses about parameter values "],["ch-03-03-estimation-bayes.html", "9.1 Bayes rule for parameter estimation", " 9.1 Bayes rule for parameter estimation 9.1.1 Definitions and terminology Fix a Bayesian model \\(M\\) with likelihood \\(P(D \\mid \\theta)\\) for observed data \\(D\\) and prior over parameters \\(P(\\theta)\\). We then update our prior beliefs \\(P(\\theta)\\) to obtain posterior beliefs by Bayes rule:48 \\[P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) \\ P(\\theta)}{P(D)}\\] The ingredients of this equation are: the posterior distribution \\(P(\\theta \\mid D)\\) - our posterior beliefs about how likely each value of \\(\\theta\\) is given \\(D\\); the likelihood function \\(P(D \\mid \\theta)\\) - how likely each observation of \\(D\\) is for a fixed \\(\\theta\\); the prior distribution \\(P(\\theta)\\) - our initial (prior) beliefs about how likely each value of \\(\\theta\\) might be; the marginal likelihood \\(P(D) = \\int P(D \\mid \\theta) \\ P(\\theta) \\ \\text{d}\\theta\\) - how likely an observation of \\(D\\) is under our prior beliefs about \\(\\theta\\) (a.k.a., the prior predictive probability of \\(D\\) according to \\(M\\)) A frequently used shorthand notation for probabilities is this: \\[\\underbrace{P(\\theta \\, | \\, D)}_{posterior} \\propto \\underbrace{P(\\theta)}_{prior} \\ \\underbrace{P(D \\, | \\, \\theta)}_{likelihood}\\] where the “proportional to” sign \\(\\propto\\) indicates that the probabilities on the LHS are defined in terms of the quantity on the RHS after normalization. So, if \\(F \\colon X \\rightarrow \\mathbb{R}^+\\) is a positive function of non-normalized probabilities (assuming, for simplicity, finite \\(X\\)), \\(P(x) \\propto F(x)\\) is equivalent to \\(P(x) = \\frac{F(x)}{\\sum_{x&#39; \\in X} F(x&#39;)}\\). 9.1.2 The effects of prior and likelihood on the posterior The shorthand notation for the posterior \\(P(\\theta \\, | \\, D) \\propto P(\\theta) \\ P(D \\, | \\, \\theta)\\) makes it particularly clear that the posterior distribution is a “mix” of prior and likelihood. Let’s first explore this “mixing property” of the posterior before worrying about how to compute posteriors concretely. We consider the case of flipping a coin with unknown bias \\(\\theta\\) a total of \\(N\\) times and observing \\(k\\) heads (= successes). This is modeled with the Binomial Model (see Section 8.1), using priors expressed with a Beta distribution, giving us a model specification as: \\[ \\begin{aligned} k &amp; \\sim \\text{Binomial}(N, \\theta) \\\\ \\theta &amp; \\sim \\text{Beta}(a, b) \\end{aligned} \\] To study the impact of the likelihood function, we compare two data sets. The first one is the contrived “24/7” example where \\(N = 24\\) and \\(k = 7\\). The second example uses a much larger naturalistic data set stemming from the King of France example, namely \\(k = 109\\) for \\(N = 311\\). These numbers are the number of “true” responses and the total number of responses for all conditions except Condition 1, which did not involve a presupposition. data_KoF_cleaned &lt;- aida::data_KoF_cleaned data_KoF_cleaned %&gt;% filter(condition != &quot;Condition 1&quot;) %&gt;% group_by(response) %&gt;% dplyr::count() ## # A tibble: 2 × 2 ## # Groups: response [2] ## response n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 202 ## 2 TRUE 109 The likelihood function for both data sets is plotted in Figure 9.1. The most important thing to notice is that the more data we have (as in the KoF example), the narrower the range of parameter values that make the data likely. Intuitively, this means that the more data we have, the more severely constrained the range of a posteriori plausible parameter values will be, all else equal. Figure 9.1: Likelihood for two examples of binomial data. The first example has \\(k = 7\\) and \\(N = 24\\). The second has \\(k = 109\\) and \\(N = 311\\). Picking up the example from Section 8.3.2, we will consider the four types of priors show below in Figure 9.2. Figure 9.2: Examples of different kinds of Bayesian priors for the Binomial Model. Combining the four different priors and the two different data sets, we see that the posterior is indeed a mix of prior and likelihood. In particular, we see that the weakly informative prior has only little effect if there are many data points (the KoF data), but does affect the posterior of the 24/7 case (compared against the uninformative prior). Figure 9.3: Posterior beliefs over bias parameter \\(\\theta\\) under different priors and different data sets. We see that strongly informative priors have more influence on the posterior than weakly informative priors, and that the influence of the prior is stronger for less data than for more. Exercise 9.1 Use the WebPPL code below to explore the effects of priors and different observations in the Binomial Model in order to be able to answer the questions in the second part below. Ask yourself how you need to change parameters in such a way as to: make the contribution of the likelihood function stronger make the prior more informative // select your parameters here var k = 7 // observed successes (heads) var N = 24 // total flips of a coin var a = 1 // first shape parameter of beta prior var b = 1 // second shape parameter of beta prior var n_samples = 50000 // number of samples for approximation ///fold: display(\"Prior distribution\") var prior = function() { beta(a, b) } viz(Infer({method: \"rejection\", samples: n_samples}, prior)) display(\"\\nPosterior distribution\") var posterior = function() { beta(k + a, N - k + b) } viz(Infer({method: \"rejection\", samples: n_samples}, posterior)) /// Solution To make the influence of the likelihood function stronger, we need more data. Try increasing variables N and k without changing their ratio. To make the prior more strongly informative, you should increase the shape parameters a and b. Based on your explorations of the WebPPL code, which of the following statements do you think is true? The prior always influences the posterior more than the likelihood. The less informative the prior, the more the posterior is influenced by it. The posterior is more influenced by the likelihood the less informative the prior is. The likelihood always influences the posterior more than the prior. The likelihood has no influence on the posterior in case of a point-valued prior (assuming a single-parameter model). Solution False False True False True 9.1.3 Computing Bayesian posteriors with conjugate priors Bayesian posterior distributions can be hard to compute. Almost always, the prior \\(P(\\theta)\\) is easy to compute (otherwise, we might choose a different one for practicality). Usually, the likelihood function \\(P(D \\mid \\theta)\\) is also fast to compute. Everything seems innocuous when we just write: \\[\\underbrace{P(\\theta \\, | \\, D)}_{posterior} \\propto \\underbrace{P(\\theta)}_{prior} \\ \\underbrace{P(D \\, | \\, \\theta)}_{likelihood}\\] But the real pain is the normalizing constant, i.e., the marginalized likelihood a.k.a. the “integral of doom”, which to compute can be intractable, especially if the parameter space is large and not well-behaved: \\[P(D) = \\int P(D \\mid \\theta) \\ P(\\theta) \\ \\text{d}\\theta\\] Section 9.3 will, therefore, enlarge on methods to compute or approximate the posterior distribution efficiently. Fortunately, the computation of Bayesian posterior distributions can be quite simple in special cases. If the prior and the likelihood function cooperate, so to speak, the computation of the posterior can be as simple as sleep. The nature of the data often prescribes which likelihood function is plausible. But we have more wiggle room in the choice of the priors. If prior \\(P(\\theta)\\) and posterior \\(P(\\theta \\, | \\, D)\\) are of the same family, i.e., if they are the same kind of distribution albeit possibly with different parameterizations, we say that they conjugate. In that case, the prior \\(P(\\theta)\\) is called conjugate prior for the likelihood function \\(P(D \\, | \\, \\theta)\\) from which the posterior \\(P(\\theta \\, | \\, D)\\) is derived. Theorem 9.1 The Beta distribution is the conjugate prior of binomial likelihood. For \\(\\theta \\sim \\text{Beta}(a,b)\\) as prior and data \\(k\\) and \\(N\\), the posterior is \\(\\theta \\sim \\text{Beta}(a+k, b+ N-k)\\). Show proof. Proof. By construction, the posterior is: \\[P(\\theta \\mid \\langle{k, N \\rangle}) \\propto \\text{Binomial}(k ; N, \\theta) \\ \\text{Beta}(\\theta \\, | \\, a, b) \\] We extend the RHS by definitions, while omitting the normalizing constants: \\[ \\begin{aligned} \\text{Binomial}(k ; N, \\theta) \\ \\text{Beta}(\\theta \\, | \\, a, b) &amp; \\propto \\theta^{k} \\, (1-\\theta)^{N-k} \\, \\theta^{a-1} \\, (1-\\theta)^{b-1} \\\\ &amp; = \\theta^{k + a - 1} \\, (1-\\theta)^{N-k +b -1} \\end{aligned} \\] This latter expression is the non-normalized Beta-distribution for parameters \\(a + k\\) and \\(b + N - k\\), so that we conclude with what was to be shown: \\[ \\begin{aligned} P(\\theta \\mid \\langle k, N \\rangle) &amp; = \\text{Beta}(\\theta \\, | \\, a + k, b+ N-k) \\end{aligned} \\]   Exercise 9.2 Fill in the blanks in the code below to get a plot of the posterior distribution for the coin flip scenario with \\(k=20\\), \\(N=24\\), making use of conjugacy and starting with a uniform Beta prior. theta = seq(0, 1, length.out = 401) as_tibble(theta) %&gt;% mutate(posterior = ____ ) %&gt;% ggplot(aes(___, posterior)) + geom_line() Solution theta &lt;- seq(0, 1, length.out = 401) as_tibble(theta) %&gt;% mutate(posterior = dbeta(theta, 21, 5)) %&gt;% ggplot(aes(theta, posterior)) + geom_line() Suppose that Jones flipped a coin with unknown bias 30 times. She observed 20 heads. She updates her beliefs rationally with Bayes rule. Her posterior beliefs have the form of a beta distribution with parameters \\(\\alpha = 25\\), \\(\\beta = 15\\). What distribution and what parameter values of that distribution capture Jones’ prior beliefs before updating her beliefs with this data? Solution \\(\\text{Beta}(5,5)\\) 9.1.4 Excursion: Sequential updating Ancient wisdom has coined the widely popular proverb: “Today’s posterior is tomorrow’s prior.” Suppose we collected data from an experiment, like \\(k = 7\\) in \\(N = 24\\). Using uninformative priors at the outset, our posterior belief after the experiment is \\(\\theta \\sim \\text{Beta}(8,18)\\). But now consider what happened at half-time. After half the experiment, we had \\(k = 2\\) and \\(N = 12\\), so our beliefs followed \\(\\theta \\sim \\text{Beta}(3, 11)\\) at this moment in time. But using these beliefs as priors, and observing the rest of the data would consequently result in updating the prior \\(\\theta \\sim \\text{Beta}(3, 11)\\) with another set of observations \\(k = 5\\) and \\(N = 12\\), giving us the same posterior belief as what we would have gotten if we updated in one swoop. Figure 9.4 shows the steps through the belief space, starting uninformed and observing one piece of data at a time (going right for each outcome of heads, down for each outcome of tails). Figure 9.4: Beta distributions for different parameters. Starting from an uninformative prior (top left), we arrive at the posterior distribution in the bottom left, in any sequence of sequentially updating with the data. This sequential updating is not a peculiarity of the Beta-Binomial case or of conjugacy. It holds in general for Bayesian inference. Sequential updating is a very intuitive property, but it is not shared by all other forms of inference from data. That Bayesian inference is sequential and commutative follows from the commutativity of multiplication of likelihoods (and the definition of Bayes rule). Theorem 9.2 Bayesian posterior inference is sequential and commutative in the sense that for a data set \\(D\\) which is comprised of two mutually exclusive subsets \\(D_1\\) and \\(D_2\\) such that \\(D_1 \\cup D_2 = D\\), we have: \\[ P(\\theta \\mid D ) \\propto P(\\theta \\mid D_1) \\ P(D_2 \\mid \\theta) \\] Show proof. Proof. \\[ \\begin{aligned} P(\\theta \\mid D) &amp; = \\frac{P(\\theta) \\ P(D \\mid \\theta)}{ \\int P(\\theta&#39;) \\ P(D \\mid \\theta&#39;) \\text{d}\\theta&#39;} \\\\ &amp; = \\frac{P(\\theta) \\ P(D_1 \\mid \\theta) \\ P(D_2 \\mid \\theta)}{ \\int P(\\theta&#39;) \\ P(D_1 \\mid \\theta&#39;) \\ P(D_2 \\mid \\theta&#39;) \\text{d}\\theta&#39;} &amp; \\text{[from multiplicativity of likelihood]} \\\\ &amp; = \\frac{P(\\theta) \\ P(D_1 \\mid \\theta) \\ P(D_2 \\mid \\theta)}{ \\frac{k}{k} \\int P(\\theta&#39;) \\ P(D_1 \\mid \\theta&#39;) \\ P(D_2 \\mid \\theta&#39;) \\text{d}\\theta&#39;} &amp; \\text{[for random positive k]} \\\\ &amp; = \\frac{\\frac{P(\\theta) \\ P(D_1 \\mid \\theta)}{k} \\ P(D_2 \\mid \\theta)}{\\int \\frac{P(\\theta&#39;) \\ P(D_1 \\mid \\theta&#39;)}{k} \\ P(D_2 \\mid \\theta&#39;) \\text{d}\\theta&#39;} &amp; \\text{[rules of integration; basic calculus]} \\\\ &amp; = \\frac{P(\\theta \\mid D_1) \\ P(D_2 \\mid \\theta)}{\\int P(\\theta&#39; \\mid D_1) \\ P(D_2 \\mid \\theta&#39;) \\text{d}\\theta&#39;} &amp; \\text{[Bayes rule with } k = \\int P(\\theta) P(D_1 \\mid \\theta) \\text{d}\\theta ]\\\\ \\end{aligned} \\]   9.1.5 Posterior predictive distribution We already learned about the prior predictive distribution of a model in Chapter 8.3.3. Remember that the prior predictive distribution of a model \\(M\\) captures how likely hypothetical data observations are from an a priori point of view. It was defined like this: \\[ \\begin{aligned} P_M(D_{\\text{pred}}) &amp; = \\sum_{\\theta} P_M(D_{\\text{pred}} \\mid \\theta) \\ P_M(\\theta) &amp;&amp; \\text{[discrete parameter space]} \\\\ P_M(D_{\\text{pred}}) &amp; = \\int P_M(D_{\\text{pred}} \\mid \\theta) \\ P_M(\\theta) \\ \\text{d}\\theta &amp;&amp; \\text{[continuous parameter space]} \\end{aligned} \\] After updating beliefs about parameter values in the light of observed data \\(D_{\\text{obs}}\\), we can similarly define the posterior predictive distribution, which is analogous to the prior predictive distribution, except that it relies on the posterior over parameter values \\(P_{M(\\theta \\mid D_{\\text{obs}})}\\) instead of the prior \\(P_M(\\theta)\\): \\[ \\begin{aligned} P_M(D_{\\text{pred}} \\mid D_{\\text{obs}}) &amp; = \\sum_{\\theta} P_M(D_{\\text{pred}} \\mid \\theta) \\ P_M(\\theta \\mid D_{\\text{obs}}) &amp;&amp; \\text{[discrete parameter space]} \\\\ P_M(D_{\\text{pred}} \\mid D_{\\text{obs}}) &amp; = \\int P_M(D_{\\text{pred}} \\mid \\theta) \\ P_M(\\theta \\mid D_{\\text{obs}}) \\ \\text{d}\\theta &amp;&amp; \\text{[continuous parameter space]} \\end{aligned} \\] Since parameter estimation is only about one model, it is harmless to omit the index \\(M\\) in the probability notation.↩︎ "],["ch-03-04-parameter-estimation-points-intervals.html", "9.2 Point-valued and interval-ranged estimates", " 9.2 Point-valued and interval-ranged estimates Let’s consider the “24/7” example with a flat prior again, concisely repeated in Figure 9.5. Figure 9.5: Prior (uninformative), likelihood and posterior for the 24/7 example. The posterior probability distribution in Figure 9.5 contains rich information. It specifies how likely each value of \\(\\theta\\) is, obtained by updating the original prior beliefs with the observed data. Such rich information is difficult to process and communicate in natural language. It is therefore convenient to have conventional means of summarizing the rich information carried in a probability distribution like in Figure 9.5. Customarily, we summarize in terms of a point-estimate and/or an interval estimate. The point estimate gives information about a “best value”, i.e., a salient point, and the interval estimate gives, usually, an indication of how closely other “good values” are scattered around the “best value”. The most frequently used Bayesian point estimate is the mean of the posterior distribution, and the most frequently used Bayesian interval estimate is the credible interval. We will introduce both below, alongside some alternatives (namely the maximum a posteriori, the maximum likelihood estimate and the inner-quantile range). 9.2.1 Point-valued estimates A common Bayesian point estimate of parameter vector \\(\\theta\\) is the mean of the posterior distribution over \\(\\theta\\). It gives the value of \\(\\theta\\) which we would expect to see when basing out expectations on the posterior distribution: \\[ \\begin{aligned} \\mathbb{E}_{P(\\theta \\mid D)} = \\int \\theta \\ P(\\theta \\mid D) \\ \\text{d}\\theta \\end{aligned} \\] Taking the Binomial Model as example, if we start with flat beliefs, the expected value of \\(\\theta\\) after \\(k\\) successes in \\(N\\) flips can be calculated rather easily as \\(\\frac{k+1}{n+2}\\).49 For our example case, we calculate the expected value of \\(\\theta\\) as \\(\\frac{8}{26} \\approx 0.308\\) (see also Figure 9.5). Another salient point-estimate to summarize a Bayesian posterior distribution is the maximum a posteriori, or MAP, for short. The MAP is the parameter value (tuple) that maximizes the posterior distribution: \\[ \\text{MAP}(P(\\theta \\mid D)) = \\arg \\max_\\theta P(\\theta \\mid D) \\] While the mean of the posterior is “holistic” in the sense that it depends on the whole distribution, the MAP does not. The mean is therefore more faithful to the Bayesian ideal of taking the full posterior distribution into account. Moreover, depending on how Bayesian posteriors are computed/approximated, the estimation of a mean can be more reliable than that of a MAP. The maximum likelihood estimate (MLE) is a point estimate based on the likelihood function alone. It specifies the value of \\(\\theta\\) for which the observed data is most likely. We often use the notation \\(\\hat{\\theta}\\) to denote the MLE of \\(\\theta\\): \\[ \\begin{aligned} \\hat{\\theta} = \\arg \\max_{\\theta} P(D \\mid \\theta) \\end{aligned} \\] By ignoring the prior information entirely, the MLE is not a Bayesian notion, but a frequentist one (more on this in later chapters). For the binomial likelihood function, the maximum likelihood estimate is easy to calculate as \\(\\frac{k}{N}\\), yielding \\(\\frac{7}{24} \\approx 0.292\\) for the running example. Figure 9.6 shows a graph of the non-normalized likelihood function and indicates the maximum likelihood estimate (the value that maximizes the curve). Figure 9.6: Non-normalized likelihood function for the observation of \\(k=7\\) successes in \\(N=24\\) flips, including maximum likelihood estimate. Exercise 9.3 Can you think of a situation where MLE and MAP are the same? HINT: Think which prior eliminates the difference between them! Solution MLE is a special case of MAP with a uniform prior. 9.2.2 Interval-ranged estimates A common Bayesian interval estimate of the coin bias parameter \\(\\theta\\) is a credible interval.50 An interval \\([l;u]\\) is a \\(\\gamma\\%\\) credible interval for a random variable \\(X\\) if two conditions hold, namely \\[ \\begin{aligned} P(l \\le X \\le u) = \\frac{\\gamma}{100} \\end{aligned} \\] and, secondly, for every \\(x \\in[l;u]\\) and \\(x&#39; \\not \\in[l;u]\\) we have \\(P(X=x) &gt; P(X = x&#39;)\\). Intuitively, a \\(95\\%\\) credible interval gives the range of values in which we believe with relatively high certainty that the true value resides. Figure 9.5 indicates the \\(95\\%\\) credible interval, based on the posterior distribution \\(P(\\theta \\mid D)\\) of \\(\\theta\\), for the 24/7 example.51 Instead of credible intervals, sometimes posteriors are also summarized in terms of the \\(\\gamma\\%\\) inner-quantile region. This is the interval \\([l;u]\\) such that \\[P(l \\le X) = P(X \\le u) = 0.5 \\cdot (1 - \\frac{\\gamma}{100})\\] For example, a 95% inner-quantile region contains all values except the smallest and largest values what each comprise 2.5% of the probability mass/density. The inner-quantile range is easier to compute and does not have trouble with multi-modality. This is why it is frequently used to approximate Bayesian credible intervals. However, care must be taken because the inner-quantile range is not as intuitive a measure of the “best values” as credible intervals. While credible intervals and inner quantile regions coincide for distributions with a symmetric distribution around a single maximum, and so tend to coincide for large sample size when posteriors tend to converge to normal distributions, there are cases of clear divergence. Figure 9.7 shows such a case. While the inner-quantile region does not include the most likely values, the credible interval does. Figure 9.7: Difference between a 95% credible interval and a 95% inner-quantile region. 9.2.3 Computing Bayesian estimates As mentioned, the most common (and arguably best) summaries to report for a Bayesian posterior are the posterior mean and a credible interval. The aida package which accompanies this book has a convenience function called aida::summarize_sample_vector() that gives the mean and 95% credible interval for a vector of samples. You can use it like so: # take samples from a posterior (24/7 example with flat priors) posterior_samples &lt;- rbeta(100000, 8, 18) # get summaries aida::summarize_sample_vector( # vector of samples samples = posterior_samples, # name of output column name = &quot;theta&quot; ) ## # A tibble: 1 × 4 ## Parameter `|95%` mean `95%|` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 theta 0.139 0.308 0.483 9.2.4 Excursion: Computing MLEs and MAPs in R Computing the maximum or minimum of a function, such as an MLE or MAP estimate, is a common problem. R has a built-in function optim that is useful for finding the minimum of a function. (If a maximum is needed, just multiply by \\(-1\\) and search the minimum with optim.) We can use the optim function to retrieve an MLE for 24/7 data and the Binomial Model (with flat priors) using conjugacy like so: # perform optimization MLE &lt;- optim( # starting value for optimization par = 0.2, # funtion to minimize (= optimize) fn = function(par){ -dbeta(par, 8, 18) }, # method of optimization (for 1-d cases) method = &quot;Brent&quot;, # lower and upper bound of possible parameter values lower = 0, upper = 1 ) # retrieve MLE MLE$par ## [1] 0.2916667 Indeed, the value obtained by computationally approximating the maximum likelihood estimate for this likelihood function coincides with the true value of \\(\\frac{7}{24}\\). This is also known as Laplace’s rule or the rule of succession.↩︎ Also frequently called “highest-density intervals”, even when we are dealing not with density but probability mass.↩︎ Not all random variables have a credible interval for a given \\(\\gamma\\), according to this definition. A bimodal distribution might not, for example. A bi-modal distribution has two regions of high probability. We can, therefore, generalize the concept to a finite set of disjoint convex credible regions, all of which have the second property of the definition above and all of which conjointly are realized with \\(\\gamma\\%\\) probability. Unfortunately, common parlor uses the term “credible interval” to refer to credible regions as well. The same disaster occurs with alternative terms, such as “\\(\\gamma\\%\\) highest-density intervals”, which also often refers to what should better be called “highest-density regions”.↩︎ "],["Ch-03-03-estimation-algorithms.html", "9.3 Approximating the posterior", " 9.3 Approximating the posterior There are several methods of computing approximations of Bayesian posteriors. Variational inference, for example, hinges on the fact that under very general conditions, Bayesian posterior distributions are well approximated by (multi-variate) normal distributions. The more data, the better the approximation. We can then reduce the approximation of a Bayesian posterior to a problem of optimizing parameter values: we simply look for the parameter values that yield the “best” parametric approximation to the Bayesian posterior. (Here, “best” is usually expressed in terms of minimizing a measure of divergence between probability distributions, such as Kullback-Leibler divergence.) Another prominent method of approximating Bayesian posteriors is rejection sampling. The most prominent class of methods to approximate Bayesian posteriors are Markov Chain Monte Carlo methods. We will describe the most basic version of such MCMC algorithms below. For most applications in the context of this introductory book, it suffices to accept that there are black boxes (with some knobs for fine-tuning) that, if you supply a model description, priors and data, will return samples from the posterior distribution. 9.3.1 Of apples and trees: Markov Chain Monte Carlo sampling Beginning of each summer, Nature sends out the Children to distribute the apples among the trees. It is custom that bigger trees ought to receive more apples. Indeed, every tree is supposed to receive apples in proportion to how many leaves it has. If Giant George (an apple tree!) has twice as many leaves as Thin Finn (another apple tree!), Giant George is to receive twice as many apples as Thin Finn. This means that if there are \\(n_a\\) apples to distribute in total, and \\(L(t)\\) is the number of leaves of tree \\(t\\), every tree should receive \\(A(t)\\) apples, where: \\[ A(t) = \\frac{L(t)}{\\sum_{t&#39;} L(t&#39;)} \\ n_a \\] The trouble is that Nature does not know the number of leaves of all the trees: Nature does not care about numbers. The Children, however, can count. But they cannot keep in mind the number of leaves for many trees for a long time. And no single Child could ever visit all the trees before the winter. This is why the Children distribute apples in a way that approximates Nature’s will. The more apples to distribute, the better the approximation. Nature is generally fine with approximate but practical solutions. When a Child visits a tree, it affectionately hangs an apple into its branches. It also writes down the name of the tree in a list next to the number of the apple it has just delivered. It then looks around and selects a random tree in the neighborhood. If the current tree \\(t_c\\), where the Child is at present, has fewer leaves than this other tree \\(t_o\\), i.e., if \\(L(t_c) &lt; L(t_o)\\), the Child visits \\(t_o\\). If instead \\(L(t_c) \\ge L(t_o)\\), the Child flips a coin and visits \\(t_o\\) with a probability proportional to \\(\\frac{L(t_o)}{L(t_c)}\\). In other words, the Child will always visit a tree with more leaves, and it will visit a tree with fewer leaves depending on the proportion of leaves. When a large number of apples are distributed, and Nature looks at the list of trees each Child has visited. This list of tree names is a set of representative samples from the probability distribution: \\[P(t) \\propto L(t)\\] These samples were obtained without the knowledge of the normalizing constant. The Children only had \\(L(t)\\) at their disposal. When trees are parameter tuples \\(\\theta\\) and the number of leaves is the product \\(P(D \\mid \\theta) \\ P(\\theta)\\), the Children would deliver samples from the posterior distribution without knowledge of the normalizing constant (a.k.a. the integral of doom). The sequence of trees visited by a single Child is a sample chain. Usually, Nature sends out at least 2-4 Children. The first tree a Child visits is the initialization of the chain. Sometimes Nature selects initial trees strategically for each Child. Sometimes Nature lets randomness rule. In any case, a Child might be quite far away from the meadow with lush apple trees, the so-called critical region (where to dwell makes the most sense). It might take many tree hops before a Child reaches this meadow. Nature, therefore, allows each Child to hop from tree to tree for a certain time, the warm-up period, before the Children start distributing apples and taking notes. If each Child only records every \\(k\\)-th tree it visits, Nature calls \\(k\\) a thinning factor. Thinning generally reduces autocorrelation (think: the amount to which subsequent samples do not carry independent information about the distribution). Since every next hop depends on the current tree (and only on the current tree), the whole process is a Markov process. It is light on memory and parallelizable but also affected by autocorrelation. Since we are using samples, a so-called Monte Carlo method, the whole affair is a Markov Chain Monte Carlo algorithm. It is one of many. It’s called Metropolis-Hastings. More complex MCMC algorithms exist. One class of such MCMC algorithms is called Hamiltonian Monte Carlo, and these approaches use gradients to optimize the proposal function, i.e., the choice of the next tree to consider going to. They use the warm-up period to initialize certain tuning parameters, making them much faster and more reliable (at least if the distribution of leaves among neighboring trees is well-behaved). How could Nature be sure that the plan succeeded? If not even Nature knows the distribution \\(P(t)\\), how can we be sure that the Children’s list gives representative samples to work with? Certainty is petty. The reduction of uncertainty is key! Since we send out several Children in parallel, and since each Child distributed many apples, we can compare the list of trees delivered by each Child (= the set of samples in each chain). For that purpose, we can use statistics and ask: is it plausible that the set of samples in each chain has been generated from the same probability distribution? - The answer to this question can help reduce uncertainty about the quality of the sampling process. #target_box { width: 250px; height: 80px; border: 1px solid #aaaaaa; overflow: auto; } #start_box { width: 300px; height: auto; padding: 10px; margin: 2em 5em 0 0; float: right; } .drag { height: auto; padding: 10px; margin: 0 0 0 0; } Exercise 9.6 On the right, there is a shuffled list of the steps that occur in the MH algorithm. Bring the list in the right order by dragging each step to the corresponding box on the left. If the new proposal has a higher posterior value than the most recent sample, then accept the new proposal. Generate a new value (proposal). Set an initial value. Compare the posterior value of the new proposal and the height of the posterior at the previous step. Choose to accept or reject the new proposal concerning the computed proportion. If the new proposal has a lower posterior value than the most recent sample, compute the proportion of the posterior value of the new proposal and the height of the posterior at the previous step. Step 1: Step 2: Step 3: Step 4: Step 5: Step 6: Solution Step 1: Set an initial value. Step 2: Generate a new value (proposal). Step 3: Compare the posterior value of the new proposal and the height of the posterior at the previous step. Step 4: If the new proposal has a higher posterior value than the most recent sample, then accept the new proposal. Step 5: If the new proposal has a lower posterior value than the most recent sample, compute the proportion of the posterior value of the new proposal and the height of the posterior at the previous step. Step 6: Choose to accept or reject the new proposal concerning the computed proportion. 9.3.2 Excursion: Probabilistic modeling with Stan There are a number of software solutions for Bayesian posterior approximation, all of which implement a form of MCMC sampling, and most of which also realize at least one other form of parameter estimation. Many of these use a special language to define the model and rely on a different programming language (like R, Python, Julia, etc.) to communicate with the program that does the sampling. Some options are: WinBUGS: a classic which has grown out of use a bit JAGS: another classic Stan: strongly developed current workhorse WebPPL: light-weight, browser-based full probabilistic programming language pyro: for probabilistic (deep) machine learning, based on PyTorch greta: R-only probabilistic modeling package, based on Python and TensorFlow This section will showcase an example using Stan. Later parts of this book will focus on regression models, for which we will use an R package called brms. This package uses Stan in the background. We do not have to write or read Stan code to work with brms. Still, a short peek at how Stan works is interesting if only to get a rough feeling for what is happening under the hood. 9.3.2.1 Basics of Stan In order to approximate a posterior distribution over parameters for a model, given some data, using an MCMC algorithm, we need to specify the model for the sampler. In particular, we must tell it about (i) the parameters, (ii) their priors, and (iii) the likelihood function. The latter requires that the sampler knows about the data. To communicate with Stan we will use the R package rstan (there are similar packages also for Python, Julia and other languages). More information about Stan can be found in the documentation section of the Stan homepage. The usual workflow with Stan and rstan consists of the following steps. First, we use R to massage the data into the right format for passing to Stan (a named list, see below). Second, we write the model in the Stan programming language. We do this in a stand-alone file.52 Then, we run the Stan code with the R command rstan::stan supplied by the package rstan. Finally, we collect the output of this operation (basically: a set of samples from the posterior distribution) and do with it as we please (plotting, further analysis, diagnosing the quality of the samples, …). This is best conveyed by a simple example. 9.3.2.2 Binomial Model Figure 9.8 shows the Binomial model for coin flips, as discussed before. We are going to implement it in Stan. Figure 9.8: The Binomial Model (repeated from before). We use the data from the King of France example, where we are interested in the number \\(k = 109\\) of “true” responses to sentences with a false presupposition over all \\(N = 311\\) relevant observations. We collect this information in a named list, which we will pass to Stan. KoF_data_4_Stan &lt;- list( k = 109, N = 311 ) Next, we need to write the actual model. Notice that Stan code is strictly regimented to be divided into different blocks, so that Stan knows what is data, what are parameters and what constitutes the actual model (prior and likelihood). Stan also wants to know the type of its variables (and the ranges of values these can take on). data { int&lt;lower=0&gt; N ; int&lt;lower=0,upper=N&gt; k ; } parameters { real&lt;lower=0,upper=1&gt; theta ; } model { # prior theta ~ beta(1,1) ; # likelihood k ~ binomial(N, theta) ; } We save this Stan code in a file binomial_model.stan (which you can download here) in a folder models_stan and then use the function rstan::stan to run the Stan code from within R. stan_fit_binomial &lt;- rstan::stan( # where is the Stan code file = &#39;models_stan/binomial_model.stan&#39;, # data to supply to the Stan program data = KoF_data_4_Stan, # how many iterations of MCMC iter = 3000, # how many warmup steps warmup = 500 ) ## ## SAMPLING FOR MODEL &#39;binomial_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 9e-06 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 3000 [ 0%] (Warmup) ## Chain 1: Iteration: 300 / 3000 [ 10%] (Warmup) ## Chain 1: Iteration: 501 / 3000 [ 16%] (Sampling) ## Chain 1: Iteration: 800 / 3000 [ 26%] (Sampling) ## Chain 1: Iteration: 1100 / 3000 [ 36%] (Sampling) ## Chain 1: Iteration: 1400 / 3000 [ 46%] (Sampling) ## Chain 1: Iteration: 1700 / 3000 [ 56%] (Sampling) ## Chain 1: Iteration: 2000 / 3000 [ 66%] (Sampling) ## Chain 1: Iteration: 2300 / 3000 [ 76%] (Sampling) ## Chain 1: Iteration: 2600 / 3000 [ 86%] (Sampling) ## Chain 1: Iteration: 2900 / 3000 [ 96%] (Sampling) ## Chain 1: Iteration: 3000 / 3000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.00382 seconds (Warm-up) ## Chain 1: 0.020671 seconds (Sampling) ## Chain 1: 0.024491 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;binomial_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 5e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 3000 [ 0%] (Warmup) ## Chain 2: Iteration: 300 / 3000 [ 10%] (Warmup) ## Chain 2: Iteration: 501 / 3000 [ 16%] (Sampling) ## Chain 2: Iteration: 800 / 3000 [ 26%] (Sampling) ## Chain 2: Iteration: 1100 / 3000 [ 36%] (Sampling) ## Chain 2: Iteration: 1400 / 3000 [ 46%] (Sampling) ## Chain 2: Iteration: 1700 / 3000 [ 56%] (Sampling) ## Chain 2: Iteration: 2000 / 3000 [ 66%] (Sampling) ## Chain 2: Iteration: 2300 / 3000 [ 76%] (Sampling) ## Chain 2: Iteration: 2600 / 3000 [ 86%] (Sampling) ## Chain 2: Iteration: 2900 / 3000 [ 96%] (Sampling) ## Chain 2: Iteration: 3000 / 3000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.003846 seconds (Warm-up) ## Chain 2: 0.020041 seconds (Sampling) ## Chain 2: 0.023887 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;binomial_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 7e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 3000 [ 0%] (Warmup) ## Chain 3: Iteration: 300 / 3000 [ 10%] (Warmup) ## Chain 3: Iteration: 501 / 3000 [ 16%] (Sampling) ## Chain 3: Iteration: 800 / 3000 [ 26%] (Sampling) ## Chain 3: Iteration: 1100 / 3000 [ 36%] (Sampling) ## Chain 3: Iteration: 1400 / 3000 [ 46%] (Sampling) ## Chain 3: Iteration: 1700 / 3000 [ 56%] (Sampling) ## Chain 3: Iteration: 2000 / 3000 [ 66%] (Sampling) ## Chain 3: Iteration: 2300 / 3000 [ 76%] (Sampling) ## Chain 3: Iteration: 2600 / 3000 [ 86%] (Sampling) ## Chain 3: Iteration: 2900 / 3000 [ 96%] (Sampling) ## Chain 3: Iteration: 3000 / 3000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.003833 seconds (Warm-up) ## Chain 3: 0.01732 seconds (Sampling) ## Chain 3: 0.021153 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;binomial_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 7e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 3000 [ 0%] (Warmup) ## Chain 4: Iteration: 300 / 3000 [ 10%] (Warmup) ## Chain 4: Iteration: 501 / 3000 [ 16%] (Sampling) ## Chain 4: Iteration: 800 / 3000 [ 26%] (Sampling) ## Chain 4: Iteration: 1100 / 3000 [ 36%] (Sampling) ## Chain 4: Iteration: 1400 / 3000 [ 46%] (Sampling) ## Chain 4: Iteration: 1700 / 3000 [ 56%] (Sampling) ## Chain 4: Iteration: 2000 / 3000 [ 66%] (Sampling) ## Chain 4: Iteration: 2300 / 3000 [ 76%] (Sampling) ## Chain 4: Iteration: 2600 / 3000 [ 86%] (Sampling) ## Chain 4: Iteration: 2900 / 3000 [ 96%] (Sampling) ## Chain 4: Iteration: 3000 / 3000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.004103 seconds (Warm-up) ## Chain 4: 0.020183 seconds (Sampling) ## Chain 4: 0.024286 seconds (Total) ## Chain 4: The object returned from this call to Stan is a special model fit object. If we just print it, we get interesting information about the estimated parameters: print(stan_fit_binomial) ## Inference for Stan model: binomial_model. ## 4 chains, each with iter=3000; warmup=500; thin=1; ## post-warmup draws per chain=2500, total post-warmup draws=10000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## theta 0.35 0.00 0.03 0.30 0.33 0.35 0.37 0.41 3674 1 ## lp__ -203.44 0.01 0.73 -205.54 -203.61 -203.16 -202.98 -202.93 3555 1 ## ## Samples were drawn using NUTS(diag_e) at Mon Oct 11 14:53:34 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). To get the posterior samples in a tidy format we use a function from the tidybayes package: tidy_samples &lt;- tidybayes::tidy_draws(stan_fit_binomial) %&gt;% select(theta) tidy_samples ## # A tibble: 10,000 × 1 ## theta ## &lt;dbl&gt; ## 1 0.332 ## 2 0.358 ## 3 0.349 ## 4 0.353 ## 5 0.353 ## 6 0.385 ## 7 0.360 ## 8 0.362 ## 9 0.361 ## 10 0.365 ## # … with 9,990 more rows We can then pull out the column theta as a vector and feed it into the summary function from the aida package to get our key Bayesian estimates: Bayes_estimates &lt;- tidy_samples %&gt;% pull(theta) %&gt;% aida::summarize_sample_vector(&quot;theta&quot;) Bayes_estimates ## # A tibble: 1 × 4 ## Parameter `|95%` mean `95%|` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 theta 0.300 0.351 0.407 Figure 9.9 moreover shows a density plot derived from the MCMC samples, together with the estimated 95% HDI and the true posterior distribution (in back), as derived by conjugacy. Figure 9.9: Posterior over bias \\(\\theta\\) given \\(k=109\\) and \\(N=311\\) approximated by samples from Stan, with estimated 95% credible interval (red area). The black curve shows the true posterior, derived through conjugacy. RStudio provides syntax highlighting for Stan code. Use the file ending *.stan.↩︎ "],["ch-03-04-parameter-estimation-normal.html", "9.4 Estimating the parameters of a Normal distribution", " 9.4 Estimating the parameters of a Normal distribution To keep matters simple and the sample size low (so as to better see effects of different priors; more on this below), we look at a (boring) fictitious data set, which we imagine being measurements of height of two species of flowers, unflowerly named species ‘A’ and ‘B’. # fictitious data from height measurements (25 flowers of two species each in cm) heights_A &lt;- c(6.94, 11.77, 8.97, 12.2, 8.48, 9.29, 13.03, 13.58, 7.63, 11.47, 10.24, 8.99, 8.29, 10.01, 9.47, 9.92, 6.83, 11.6, 10.29, 10.7, 11, 8.68, 11.71, 10.09, 9.7) heights_B &lt;- c(11.45, 11.89, 13.35, 11.56, 13.78, 12.12, 10.41, 11.99, 12.27, 13.43, 10.91, 9.13, 9.25, 9.94, 13.5, 11.26, 10.38, 13.78, 9.35, 11.67, 11.32, 11.98, 12.92, 12.03, 12.02) On the assumption that the metric measurements for flower ‘A’ come from a normal distribution, the goal is to estimate credible values for that normal distribution’s parameters \\(\\mu_{A}\\) and \\(\\sigma_{A}\\); and similarly for flower ‘B’. The “research question” of interest is whether it is credible that the mean of heights for flower ‘A’ is smaller than that of ‘B’ - or, in other words, whether the difference in means \\(\\delta = \\mu_{B} - \\mu_{A}\\) is credibly positive. Here are relevant summary statistics for this case, and a plot, both of which seem to support the conjecture that flower ‘A’ is smaller, on average, than flower ‘B’. # bring data into a more practical format ffm_data &lt;- tibble( A = heights_A, B = heights_B ) %&gt;% pivot_longer( cols = everything(), names_to = &#39;species&#39;, values_to = &#39;height&#39; ) # some summary statistics ffm_data %&gt;% group_by(species) %&gt;% summarise( mean = mean(height), std_dev = sd(height) ) ## # A tibble: 2 × 3 ## species mean std_dev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 10.0 1.76 ## 2 B 11.7 1.38 ffm_data %&gt;% ggplot(aes(x = height)) + geom_density(aes(color = species), size = 2) + geom_rug() + facet_grid(~species, scales = &quot;free&quot;) + theme(legend.position = &#39;none&#39;) The remainder of this chapter will introduce two models for inferring the parameters of a (single) normal distribution, both of which are set-up in such a way that it is possible to compute a closed-form solution for the posterior distributions over \\(\\mu\\) and \\(\\sigma\\): (i) a model with uninformative priors, and (ii) a model with conjugate priors. These two models are also explained in the video below. 9.4.1 Uninformative priors The model with uninformative priors is shown in Figure 9.10. Figure 9.10: A model to infer the parameter of a normal distribution with non-informative priors. The posterior for variance \\(\\sigma^{2}\\) and mean \\(\\mu\\) for this model with uninformative priors is as follows: \\[ \\begin{align*} P(\\mu, \\sigma^2 \\mid \\mathbf{y}) &amp; = {\\color{7F2615}{P(\\sigma^2 | \\mathbf{y})}} \\ \\ \\ {\\color{353585}{P(\\mu \\mid \\sigma^2, \\mathbf{y})}} &amp; \\text{with:} \\\\ \\sigma^2 \\mid \\mathbf{y} &amp; \\sim \\mathrm{Inv}\\text{-}\\chi^2 \\left(n-1,\\ s^2 \\right) \\\\ \\mu \\mid \\sigma^2, \\mathbf{y} &amp; \\sim \\mathrm{Normal} \\left (\\bar{y} \\mid \\frac{\\sigma}{\\sqrt{n}} \\right) \\end{align*} \\] The aida package provides the convenience function aida::get_samples_single_noninformative, which we use below but also show explicitly first. It takes a vector data_vector (like height_A) of metric observations as input and returns n_samples samples from the posterior. get_samples_single_noninformative &lt;- function(data_vector, n_samples = 1000) { # determine sample variance s_squared &lt;- var(data_vector) # posterior samples of the variance var_samples &lt;- extraDistr::rinvchisq( n = n_samples, nu = length(data_vector) - 1, tau = s_squared ) # posterior samples of the mean given the sampled variance mu_samples &lt;- map_dbl( var_samples, function(var) rnorm( n = 1, mean = mean(data_vector), sd = sqrt(var / length(data_vector)) ) ) # return pairs of values tibble( mu = mu_samples, sigma = sqrt(var_samples) ) } If we apply this function for the data of flower ‘A’, we get samples of likely pairs consisting of means and standard deviations (each row is one pair of associated samples): aida::get_samples_single_noninformative(heights_A, n_samples = 5) ## # A tibble: 5 × 2 ## mu sigma ## &lt;dbl&gt; &lt;dbl&gt; ## 1 9.94 1.44 ## 2 10.2 2.07 ## 3 10.3 1.76 ## 4 9.78 1.58 ## 5 10.3 1.71 By taking more samples from this 2-dimensional (joint) posterior distribution a scatter point reveals its approximate shape. # take 10,000 samples from the posterior post_samples_A_noninfo &lt;- aida::get_samples_single_noninformative(data_vector = heights_A, n_samples = 10000) # look at a scatter plot post_samples_A_noninfo %&gt;% ggplot(aes(x = sigma, y = mu)) + geom_point(alpha = 0.4, color = &quot;lightblue&quot;) The plot below shows the marginal distributions of each variable, \\(\\mu\\) and \\(\\sigma\\), separately: post_samples_A_noninfo %&gt;% pivot_longer(cols = everything(), names_to = &quot;parameter&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = value)) + geom_density() + facet_grid(~ parameter, scales = &quot;free&quot;) As usual, we can also produce the relevant Bayesian summary statistics for our samples, like so: rbind( aida::summarize_sample_vector(post_samples_A_noninfo$mu, &quot;mu&quot;), aida::summarize_sample_vector(post_samples_A_noninfo$sigma, &quot;sigma&quot;) ) ## # A tibble: 2 × 4 ## Parameter `|95%` mean `95%|` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mu 9.30 10.0 10.7 ## 2 sigma 1.32 1.82 2.36 9.4.2 Conjugate priors The model with uninformative priors is useful when modelers have no or wish to not include any a priori assumptions about \\(\\mu\\) and \\(\\sigma\\). When prior assumptions are relevant, we can use a slightly more complex model with conjugate priors. The model is shown in Figure 9.11. Figure 9.11: Model with conjugate priors. With this prior structure, the posterior is of the form: \\[ \\begin{align*} P(\\mu, \\sigma^2 \\mid \\mathbf{y}) &amp; = {\\color{7F2615}{P(\\sigma^2 | \\mathbf{y})}} \\ \\ \\ {\\color{353585}{P(\\mu \\mid \\sigma^2, \\mathbf{y})}} &amp; \\text{with:} \\\\ \\sigma^2 \\mid \\mathbf{y} &amp; \\sim {\\color{7F2615}{\\mathrm{Inv}\\text{-}\\chi^2 \\left({\\color{3F9786}{\\nu_1}},\\ {\\color{3F9786}{\\sigma^2_1}} \\right)}} \\\\ \\mu \\mid \\sigma^2, \\mathbf{y} &amp; \\sim {\\color{353585}{\\mathrm{Normal} \\left ({\\color{3F9786}{\\mu_1}}, \\frac{\\sigma}{\\sqrt{{\\color{3F9786}{\\kappa_1}}}} \\right)}} &amp; \\text{where:} \\\\ {\\color{3F9786}{\\nu_1}} &amp; = \\nu_0 + n \\\\ \\nu_n{\\color{3F9786}{\\sigma_1^2}} &amp; = \\nu_0 \\sigma_0^2 + (n-1) s^2 + \\frac{\\kappa_0 \\ n}{\\kappa_0 + n} (\\bar{y} - \\mu_0)^2 \\\\ {\\color{3F9786}{\\mu_1}} &amp; = \\frac{\\kappa_0}{\\kappa_0 + n} \\mu_0 + \\frac{n}{\\kappa_0 + n} \\bar{y} \\\\ {\\color{3F9786}{\\kappa_1}} &amp; = \\kappa_0 + n \\end{align*} \\] Exercise 9.7 The aida package provides the convenience function aida::sample_Norm_inv_chisq for sampling from the ‘normal inverse-\\(\\chi^2\\)’ prior. Here is the source code of this function: sample_Norm_inv_chisq &lt;- function( n_samples = 10000, nu = 1, var = 1, mu = 0, kappa = 1 ) { var_samples &lt;- extraDistr::rinvchisq( n = n_samples, nu = nu, tau = var ) mu_samples &lt;- map_dbl( var_samples, function(s) rnorm( n = 1, mean = mu, sd = sqrt(s / kappa) ) ) tibble( sigma = sqrt(var_samples), mu = mu_samples ) } In the code below, we use this function to plot 10,000 samples from the prior with a particular set of parameter values. Notice the line filter(abs(value) &lt;= 10) which is useful for an informative plot (try commenting it out: what does that tell you about the range of values reasonably likely to get sampled?). # samples from the prior samples_prior_1 &lt;- aida::sample_Norm_inv_chisq( nu = 1, var = 1, # a priori &quot;variance of the variance&quot; mu = 0, kappa = 1 ) samples_prior_1 %&gt;% pivot_longer(cols = everything(), names_to = &quot;parameter&quot;, values_to = &quot;value&quot;) %&gt;% filter(abs(value) &lt;= 10) %&gt;% ggplot(aes(x = value)) + geom_density() + facet_grid(~parameter, scales = &quot;free&quot;) To get comfortable with this ‘normal inverse-\\(\\chi^2\\)’ distribution, fill in the XXX in the following code box (possibly removing or altering parts of the plotting code if you need to) to find parameter values that encode a prior belief according to which credible values of \\(\\sigma\\) are not much bigger than (very roughly) 7.5, and credible values of \\(\\mu\\) lie (very roughly) in the range of 15 to 25. (Hint: intuit what the meaning of each parameter value is by a trial-error-think method.) The plot you generate could look roughly like the one below. (Motivation for the exercise: you should get familiar with this distribution, and also realize that it is clunky and that you might want to use a different prior structure in order to encode specific beliefs … which is exactly why we might want to be more flexible and go beyond conjugate priors in some cases.) # samples from the prior samples_prior_2 &lt;- aida::sample_Norm_inv_chisq( nu = XXX, var = XXX, mu = XXX, kappa = XXX ) samples_prior_2 %&gt;% pivot_longer(cols = everything(), names_to = &quot;parameter&quot;, values_to = &quot;value&quot;) %&gt;% filter(!(parameter == &quot;mu&quot; &amp; (value &gt;= 40 | value &lt;= 0))) %&gt;% filter(!(parameter == &quot;sigma&quot; &amp; value &gt;= 10)) %&gt;% ggplot(aes(x = value)) + geom_density() + facet_grid(~parameter, scales = &quot;free&quot;) Solution # samples from the prior samples_prior_2 &lt;- aida::sample_Norm_inv_chisq( nu = 1, var = 1, # a priori &quot;variance of the variance&quot; mu = 20, kappa = 1 ) samples_prior_2 %&gt;% pivot_longer(cols = everything(), names_to = &quot;parameter&quot;, values_to = &quot;value&quot;) %&gt;% filter(!(parameter == &quot;mu&quot; &amp; (value &gt;= 40 | value &lt;= 0))) %&gt;% filter(!(parameter == &quot;sigma&quot; &amp; value &gt;= 10)) %&gt;% ggplot(aes(x = value)) + geom_density() + facet_grid(~parameter, scales = &quot;free&quot;) Here is another convenience function from the aida package for obtaining posterior samples for the conjugate prior model, taking as input a specification of the prior beliefs. Again, we first show the function explicitly before applying it to the flower data set. get_samples_single_normal_conjugate &lt;- function( data_vector, nu = 1, var = 1, mu = 0, kappa = 1, n_samples = 1000 ) { n &lt;- length(data_vector) aida::sample_Norm_inv_chisq( n_samples = n_samples, nu = nu + n, var = (nu * var + (n - 1) * var(data_vector) + (kappa * n) / (kappa + n)) / (nu + n), mu = kappa / (kappa + n) * mu + n / (kappa + n) * mean(data_vector), kappa = kappa + n ) } The code below calls this function to obtain samples from the posterior for two different models. This will help illustrate the effect of priors on the posterior once more, especially for a case like the one at hand where we have only rather few data observations. # posterior samples for prior 1 post_samples_A_conjugate_1 &lt;- aida::get_samples_single_normal_conjugate( heights_A, nu = 1, var = 1, mu = 0, kappa = 1, n_samples = 10000 ) # posterior samples for prior 2 post_samples_A_conjugate_2 &lt;- aida::get_samples_single_normal_conjugate( heights_A, nu = 1, var = 1/1000, mu = 40, kappa = 10, n_samples = 10000 ) rbind( aida::summarize_sample_vector(post_samples_A_conjugate_1$mu, &quot;mu&quot;) %&gt;% mutate(model = 1), aida::summarize_sample_vector(post_samples_A_conjugate_1$sigma, &quot;sigma&quot;) %&gt;% mutate(model = 1), aida::summarize_sample_vector(post_samples_A_conjugate_2$mu, &quot;mu&quot;) %&gt;% mutate(model = 2), aida::summarize_sample_vector(post_samples_A_conjugate_2$sigma, &quot;sigma&quot;) %&gt;% mutate(model = 2) ) ## # A tibble: 4 × 5 ## Parameter `|95%` mean `95%|` model ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mu 8.96 9.65 10.3 1 ## 2 sigma 1.31 1.76 2.27 1 ## 3 mu 18.0 18.6 19.2 2 ## 4 sigma 1.37 1.82 2.37 2 The posterior is a mixture of prior and likelihood. The prior for model 1 is rather weak (high variance, low \\(\\kappa\\) leading to a large range of plausible values for \\(\\mu\\)). The prior for model 2 is rather biased. The credible values of \\(\\mu\\) are rather high. 9.4.3 Estimating the difference between group means The ulterior “research question” to address is: should we believe that flowers of type B are higher, on average, than flowers of type A? To address this question, it suffices to take samples for \\(\\mu_{A}\\) and \\(\\mu_{B}\\), obtained by one of the methods introduced in the previous sections (using the same model for both flower types, unless we have a good reason not to), and then to inspect the vector of differences between samples \\(\\delta = \\mu_{B} - \\mu_{A}\\). If the derived samples of \\(\\delta\\) are credibly bigger than zero, there is reason to believe that there is a difference between flower types such that ‘B’ is bigger than ‘A’. So, let’s use the (conjugate) prior of model 1 from above to also take 10,000 samples from the posterior when conditioning with the data in heights_B. Store the results in a vector called post_samples_B_conjugate_1. post_samples_B_conjugate_1 &lt;- aida::get_samples_single_normal_conjugate( heights_B, nu = 1, var = 1, mu = 0, kappa = 1, n_samples = 10000 ) The summary of the difference vector gives us information about credible values of \\(\\delta = \\mu_{B} - \\mu_{A}\\). delta_flower_heights &lt;- post_samples_B_conjugate_1$mu - post_samples_A_conjugate_1$mu aida::summarize_sample_vector(delta_flower_heights, name = &#39;delta&#39;) ## # A tibble: 1 × 4 ## Parameter `|95%` mean `95%|` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 delta 0.703 1.57 2.47 We might conclude from this that a positive difference in height is credible. More on Bayesian testing of such hypotheses about parameter values in Chapter 11. "],["Chap-03-06-model-comparison.html", "10 Model Comparison", " 10 Model Comparison Parameter estimation (the topic of the last chapter) asks: given a single model and the data, what are good (e.g., credible) values of the model’s parameters? Model comparison (the topic of this chapter) asks: based on the data at hand, which of several models is better? Or even: how much better is this model compared to another, given the data? The pivotal criterion by which to compare models is how well a model explains the observed data. A good explanation of observed data \\(D\\) is one that makes \\(D\\) unsurprising. Intuitively, we long for an explanation for things that puzzle us. A good explanation is a way of looking at the world in which puzzles disappear, in which all observations make sense, in which what we have seen would have been quite expectable after all. Consequently, the pivotal quantity for comparing models is how likely \\(D\\) is given a model \\(M_i\\): \\(P(D \\mid M_i)\\). But there is more to a good explanation, also intuitively. All else equal, a good explanation is simple. If theories \\(A\\) and \\(B\\) both explain the facts equally well, but \\(A\\) does so with less “mental machinery”, most people would choose the more economical explanation \\(A\\). In this chapter, we will look at two common methods of comparing models: the Akaike information criterion (AIC) and Bayes factors. AICs are a non-Bayesian method in the sense that it does not require (or ignores) a model’s priors over parameter values. Bayes factors are the flagship Bayesian method for model comparison. There are many other approaches to model comparison (e.g., other kinds of information criteria, or methods based on cross-validation). Our goal is not to be exhaustive, but to introduce the main ideas of model comparison and showcase a reasonable selection of representative approaches. The learning goals for this chapter are: understand the differences between estimation and model comparison understand and apply the two covered methods: Akaike information criterion Bayes factor become familiar with the pros and cons of each of these methods [optional] get acquainted with some methods for computing Bayes factors "],["Chap-03-06-model-comparison-case-study.html", "10.1 Case study: recall models", " 10.1 Case study: recall models As a running example for this chapter, we borrow from Myung (2003) and consider a fictitious data set of recall rates and two models to explain this data. As for the data, for each time point (in seconds) \\(t \\in \\{1, 3, 6, 9, 12, 18\\}\\), we have 100 (binary) observations of whether a previously memorized item was recalled correctly. # time after memorization (in seconds) t &lt;- c(1, 3, 6, 9, 12, 18) # proportion (out of 100) of correct recall y &lt;- c(.94, .77, .40, .26, .24, .16) # number of observed correct recalls (out of 100) obs &lt;- y * 100 A visual representation of this data set is here: We are interested in comparing two theoretically different models for this data. Models differ in their assumption about the functional relationship between recall probability and time. The exponential model assumes that the recall probability \\(\\theta_t\\) at time \\(t\\) is an exponential decay function with parameters \\(a\\) and \\(b\\): \\[\\theta_t(a, b) = a \\exp (-bt), \\ \\ \\ \\ \\text{where } a,b&gt;0 \\] Taking the binary nature of the data (recalled / not recalled) into account, this results in the following likelihood function for the exponential model: \\[ \\begin{aligned} P(k \\mid a, b, N , M_{\\text{exp}}) &amp; = \\text{Binom}(k,N, a \\exp (-bt)), \\ \\ \\ \\ \\text{where } a,b&gt;0 \\end{aligned} \\] In contrast, the power model assumes that the relationship is that of a power function: \\[\\theta_t(c, d) = ct^{-d}, \\ \\ \\ \\ \\text{where } c,d&gt;0 \\] The resulting likelihood function for the power model is: \\[ \\begin{aligned} P(k \\mid c, d, N , M_{\\text{pow}}) &amp; = \\text{Binom}(k,N, c\\ t^{-d}), \\ \\ \\ \\ \\text{where } c,d&gt;0 \\end{aligned} \\] These models therefore make different (parameterized) predictions about the time course of forgetting/recall. Figure 10.1 shows the predictions of each model for \\(\\theta_t\\) for different parameter values: Figure 10.1: Examples of predictions of the exponential and the power model of forgetting for different values of each model’s parameters. The research question of relevance is: which of these two models is a better model for the observed data? We are going to look at the Akaike information criterion (AIC) first, which only considers the models’ likelihood functions and is therefore a non-Bayesian method. We will see that AIC scores are easy to compute, but give numbers that are hard to interpret or only approximation of quantities that have a clear interpretation. Then we look at a Bayesian method, using Bayes factors, which does take priors over model parameters into account. We will see that Bayes factors are much harder to compute, but do directly calculate quantities that are intuitively interpretable. We will also see that AIC scores only very indirectly take a model’s complexity into account. References "],["Chap-03-06-model-comparison-AIC.html", "10.2 Akaike Information Criterion", " 10.2 Akaike Information Criterion A wide-spread non-Bayesian approach to model comparison is to use the Akaike information criterion (AIC). The AIC is the most common instance of a class of measures for model comparison known as information criteria, which all draw on information-theoretic notions to compare how good each model is. If \\(M_i\\) is a model, specified here only by its likelihood function \\(P(D \\mid \\theta_i, M_i)\\), with \\(k_i\\) model parameters in parameter vector \\(\\theta_i\\), and if \\(D_\\text{obs}\\) is the observed data, then the AIC score of model \\(M_i\\) given \\(D_\\text{obs}\\) is defined as: \\[ \\begin{aligned} \\text{AIC}(M_i, D_\\text{obs}) &amp; = 2k_i - 2\\log P(D_\\text{obs} \\mid \\hat{\\theta_i}, M_i) \\end{aligned} \\] Here, \\(\\hat{\\theta}_i = \\arg \\max_{\\theta_i} P(D_\\text{obs} \\mid \\theta_i, M_i)\\) is the best-fitting parameter vector, i.e., the maximum likelihood estimate (MLE), and \\(k\\) is the number of free parameters in model \\(M_i\\). The lower an AIC score, the better the model (in comparison to other models for the same data \\(D_\\text{obs}\\)). All else equal, the higher the number of free parameters \\(k_i\\), the worse the model’s AIC score. The first summand in the definition above can, therefore, be conceived of as a measure of model complexity. As for the second summand, think of \\(- \\log P(D_\\text{obs} \\mid \\hat{\\theta}_i, M_i)\\) as a measure of (information-theoretic) surprisal: how surprising is the observed data \\(D_\\text{obs}\\) from the point of view of model \\(M\\) under the most favorable circumstances (that is, the MLE of \\(\\theta_i\\)). The higher the probability \\(P(D_\\text{obs} \\mid \\hat{\\theta}_i, M_i)\\), the better the model \\(M_i\\)’s AIC score, all else equal. To apply AIC-based model comparison to the recall models, we first need to compute the MLE of each model (see Chapter 9.1.3). Here are functions that return the negative log-likelihood of each model, for any (suitable) pair of parameter values: # generic neg-log-LH function (covers both models) nLL_generic &lt;- function(par, model_name) { w1 &lt;- par[1] w2 &lt;- par[2] # make sure paramters are in acceptable range if (w1 &lt; 0 | w2 &lt; 0 | w1 &gt; 20 | w2 &gt; 20) { return(NA) } # calculate predicted recall rates for given parameters if (model_name == &quot;exponential&quot;) { theta &lt;- w1 * exp(-w2 * t) # exponential model } else { theta &lt;- w1 * t^(-w2) # power model } # avoid edge cases of infinite log-likelihood theta[theta &lt;= 0.0] &lt;- 1.0e-4 theta[theta &gt;= 1.0] &lt;- 1 - 1.0e-4 # return negative log-likelihood of data - sum(dbinom(x = obs, prob = theta, size = 100, log = T)) } # negative log likelihood of exponential model nLL_exp &lt;- function(par) {nLL_generic(par, &quot;exponential&quot;)} # negative log likelihood of power model nLL_pow &lt;- function(par) {nLL_generic(par, &quot;power&quot;)} These functions are then optimized with R’s built-in function optim. The results are shown in the table below. # getting the best fitting values bestExpo &lt;- optim(nLL_exp, par = c(1, 0.5)) bestPow &lt;- optim(nLL_pow, par = c(0.5, 0.2)) MLEstimates &lt;- data.frame(model = rep(c(&quot;exponential&quot;, &quot;power&quot;), each = 2), parameter = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), value = c(bestExpo$par, bestPow$par)) MLEstimates ## model parameter value ## 1 exponential a 1.0701722 ## 2 exponential b 0.1308151 ## 3 power c 0.9531330 ## 4 power d 0.4979154 The MLE-predictions of each model are shown in Figure 10.2 below, alongside the observed data. Figure 10.2: Predictions of the exponential and the power model under best-fitting parameter values. By visual inspection of Figure 10.2 alone, it is impossible to say with confidence which model is better. Numbers might help see more fine-grained differences. So, let’s look at the log-likelihood and the corresponding probability of the data for each model under each model’s best fitting parameter values. predExp &lt;- expo(t, a, b) predPow &lt;- power(t, c, d) modelStats &lt;- tibble( model = c(&quot;expo&quot;, &quot;power&quot;), `log likelihood` = round(c(-bestExpo$value, -bestPow$value), 3), probability = signif(exp(c(-bestExpo$value, -bestPow$value)), 3), # sum of squared errors SS = round(c(sum((predExp - y)^2), sum((predPow - y)^2)), 3) ) modelStats ## # A tibble: 2 × 4 ## model `log likelihood` probability SS ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 expo -18.7 7.82e- 9 0.019 ## 2 power -26.7 2.47e-12 0.057 The exponential model has a higher log-likelihood, a higher probability, and a lower sum of squares. This suggests that the exponential model is better. The AIC-score of these models is a direct function of the negative log-likelihood. Since both models have the same number of parameters, we arrive at the same verdict as before: based on a comparison of AIC-scores, the exponential model is the better model. get_AIC &lt;- function(optim_fit) { 2 * length(optim_fit$par) + 2 * optim_fit$value } AIC_scores &lt;- tibble( AIC_exponential = get_AIC(bestExpo), AIC_power = get_AIC(bestPow) ) AIC_scores ## # A tibble: 1 × 2 ## AIC_exponential AIC_power ## &lt;dbl&gt; &lt;dbl&gt; ## 1 41.3 57.5 How should we interpret the difference in AIC-scores? Some suggest that differences in AIC-scores larger than 10 should be treated as implying that the weaker model has practically no empirical support (Burnham and Anderson 2002). Adopting such a criterion, we would therefore favor the exponential model based on the data observed. But we could also try to walk a more nuanced, more quantitative road. We would ideally want to know the absolute probability of \\(M_i\\) given the data: \\(P(M_i \\mid D)\\). Unfortunately, to calculate this (by Bayes rule), we would need to normalize by quantifying over all models. Alternatively, we look at the relative probability of a small selection of models. Indeed, we can look at relative AIC-scores in terms of so-called Akaike weights (Wagenmakers and Farrell 2004; Burnham and Anderson 2002) to derive an approximation of \\(P(M_i \\mid D)\\), at least for the case where we only consider a small set of candidate models. So, if we want to compare models \\(M_1, \\dots, M_n\\) and \\(\\text{AIC}(M_i, D)\\) is the AIC-score of model \\(M_i\\) for observed data \\(D\\), then the Akaike weight of model \\(M_i\\) is defined as: \\[ \\begin{aligned} w_{\\text{AIC}}(M_i, D) &amp; = \\frac{\\exp (- 0.5 * \\Delta_{\\text{AIC}}(M_i,D) )} {\\sum_{j=1}^k\\exp (- 0.5 * \\Delta_{\\text{AIC}}(M_j,D) )}\\, \\ \\ \\ \\ \\text{where} \\\\ \\Delta_{\\text{AIC}}(M_i,D) &amp; = \\text{AIC}(M_i, D) - \\min_j \\text{AIC}(M_j, D) \\end{aligned} \\] Akaike weights are relative and normalized measures, and may serve as an approximate measure of a model’s posterior probability given the data: \\[ P(M_i \\mid D) \\approx w_{\\text{AIC}}(M_i, D) \\] For the running example at hand, this would mean that we could conclude that the posterior probability of the exponential model is approximately: delta_AIC_power &lt;- AIC_scores$AIC_power - AIC_scores$AIC_exponential delta_AIC_exponential &lt;- 0 Akaike_weight_exponential &lt;- exp(-0.5 * delta_AIC_exponential) / (exp(-0.5 * delta_AIC_exponential) + exp(-0.5 * delta_AIC_power)) Akaike_weight_exponential ## [1] 0.9996841 We can interpret this numerical result as indicating that, given a universe in which only the exponential and the power model exist, the posterior probability of the exponential model is almost 1 (assuming, implicitly, that both models are equally likely a priori). We would conclude from this approximate quantitative assessment that the empirical evidence supplied by the given data in favor of the exponential model is very strong. Our approximation is better the more data we have. We will see a method below, the Bayesian method using Bayes factors, which computes \\(P(M_i \\mid D)\\) in a non-approximate way. Exercise 11.1 Describe what the following variables represent in the AIC formula: \\[ \\begin{aligned} \\text{AIC}(M_i, D_\\text{obs}) &amp; = 2k_i - 2\\log P(D_\\text{obs} \\mid \\hat{\\theta_i}, M_i) \\end{aligned} \\] \\(k_i\\) stands for: \\(\\hat{\\theta_i}\\) stands for: \\(P(D_\\text{obs} \\mid \\hat{\\theta_i}, M_i)\\) stands for: Solution the number of free parameters in model \\(M_{i}\\); the parameter vector obtained by maximum likelihood estimation for model \\(M_{i}\\) and data \\(D_{\\text{obs}}\\); the likelihood of the data \\(D_{\\text{obs}}\\) under the best fitting parameters of a model \\(M_{i}\\). Do you see that there is something “circular” in the definition of AICs? (Hint: What do we use the data \\(D_{\\text{obs}}\\) for?) Solution We use the same data twice! We use \\(D_{\\text{obs}}\\) to find the best fitting parameter values, and then we ask how likely \\(D_{\\text{obs}}\\) is given the best fitting parameter values. If model comparison is about how well a model explains the data, then this is a rather circular measure: we quantify how well a model explains or predicts a data set after having “trained / optimized” the model for exactly this data set. References "],["Chap-03-06-model-comparison-BF.html", "10.3 Bayes factors", " 10.3 Bayes factors At the end of the previous section, we saw that we can use the AIC-approach to calculate an approximate value of the posterior probability \\(P(M_{i} \\mid D)\\) for model \\(M_{i}\\) given data \\(D\\). The Bayes factor approach is similar to this, but avoids taking priors over models into the equation by focusing on the extent to which data \\(D\\) changes our beliefs about which model is more likely. Take two Bayesian models: \\(M_1\\) has prior \\(P(\\theta_1 \\mid M_1)\\) and likelihood \\(P(D \\mid \\theta_1, M_1)\\) \\(M_2\\) has prior \\(P(\\theta_2 \\mid M_2)\\) and likelihood \\(P(D \\mid \\theta_2, M_2)\\) Using Bayes rule, we compute the posterior odds of models (given the data) as the product of the likelihood ratio and the prior odds. \\[\\underbrace{\\frac{P(M_1 \\mid D)}{P(M_2 \\mid D)}}_{\\text{posterior odds}} = \\underbrace{\\frac{P(D \\mid M_1)}{P(D \\mid M_2)}}_{\\text{Bayes factor}} \\ \\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{prior odds}}\\] The likelihood ratio is also called the Bayes factor. Formally, the Bayes factor is the factor by which a rational agent changes her prior odds in the light of observed data to arrive at the posterior odds. More intuitively, the Bayes factor quantifies the strength of evidence given by the data about the models of interest. It expresses this evidence in terms of the models’ relative prior predictive accuracy. To see the latter, let’s expand the Bayes factor as what it actually is: the ratio of marginal likelihoods. \\[ \\frac{P(D \\mid M_1)}{P(D \\mid M_2)} = \\frac{\\int P(\\theta_1 \\mid M_1) \\ P(D \\mid \\theta_1, M_1) \\text{ d}\\theta_1}{\\int P(\\theta_2 \\mid M_2) \\ P(D \\mid \\theta_2, M_2) \\text{ d}\\theta_2} \\] Three insights are to be gained from this expansion. Firstly, the Bayes factor is a measure of how well each model would have predicted the data ex ante, i.e., before having seen any data. In this way, it is diametrically opposed to a concept like AIC, which relies on models’ maximum likelihood fits (therefore using the data, so being ex post). Secondly, the marginal likelihood of a model is exactly the quantity that we identified (in the context of parameter estimation) as being very hard to compute, especially for complex models. The fact that marginal likelihoods are hard to compute was the reason that methods like MCMC sampling are useful, since they give posterior samples without requiring the calculation of marginal likelihoods. It follows that Bayes factors can be very difficult to compute in general. However, for many prominent models, it is possible to calculate Bayes factors analytically if the right kinds of priors are specified (Rouder et al. 2009; Rouder and Morey 2012; Gronau, Ly, and Wagenmakers 2019). We will see an example of this in Chapter 11. Also, as we will see in the following there are very clever approaches to computing Bayes factors in special cases and good algorithms for approximating marginal likelihoods also for complex models. Thirdly, Bayes factor model comparison implicitly (and quite vigorously) punishes model complexity, but in a more sophisticated manner than just counting free parameters. To appreciate this intuitively, imagine a model with a large parameter set and a very diffuse, uninformative prior that spreads its probability over a wide range of parameter values. Since Bayes factors are computed based on ex ante predictions, a diffuse model is punished for its imprecision of prior predictions because we integrate over all parameters (weighted by priors) and their associated likelihood. As for notation, we write: \\[\\text{BF}_{12} = \\frac{P(D \\mid M_1)}{P(D \\mid M_2)}\\] for the Bayes factor in favor of model \\(M_1\\) over model \\(M_2\\). This quantity can take on positive values, which are often translated into natural language as follows: \\(BF_{12}\\) interpretation 1 irrelevant data 1 - 3 hardly worth ink or breath 3 - 6 anecdotal 6 - 10 now we’re talking: substantial 10 - 30 strong 30 - 100 very strong 100 + decisive (bye, bye \\(M_2\\)!) As \\(\\text{BF}_{12} = \\text{BF}_{21}^{-1}\\), it suffices to give this translation into natural language only for values \\(\\ge 1\\). There are at least two general approaches to calculating or approximating Bayes factors, paired here with a (non-exhaustive) list of example methods: get each model’s marginal likelihood grid approximation (see Section 10.3.1) by Monte Carlo sampling (see Section 10.3.2) bridge sampling (see Section 10.3.3) get Bayes factor directly Savage-Dickey method (see Section 11.4.1) using encompassing models (see Section 11.4.2) 10.3.1 Grid approximation We can use grid approximation to approximate a model’s marginal likelihood if the model is small enough, say, no more than 4-5 free parameters. Grid approximation considers discrete values for each parameter evenly spaced over the whole range of plausible parameter values, thereby approximating the integral in the definition of marginal likelihoods. Let’s calculate an example for the comparison of the exponential and the power model of forgetting. To begin with, we need to define a prior over parameters to obtain Bayesian versions of the exponential and power model. Here, we assume flat priors over a reasonable range of parameter values for simplicity. For the exponential model, we choose: \\[ \\begin{aligned} P(k \\mid a, b, N, M_{\\text{exp}}) &amp; = \\text{Binom}(k,N, a \\exp (-bt_i)) \\\\ P(a \\mid M_{\\text{exp}}) &amp; = \\text{Uniform}(a, 0, 1.5) \\\\ P(b \\mid M_{\\text{exp}}) &amp; = \\text{Uniform}(b, 0, 1.5) \\end{aligned} \\] The (Bayesian) power model is given by: \\[ \\begin{aligned} P(k \\mid c, d, N, M_{\\text{pow}}) &amp; = \\text{Binom}(k,N, c\\ t_i^{-d}) \\\\ P(c \\mid M_{\\text{pow}}) &amp; = \\text{Uniform}(c, 0, 1.5) \\\\ P(d \\mid M_{\\text{pow}}) &amp; = \\text{Uniform}(d, 0, 1.5) \\end{aligned} \\] We can also express these models in code, like so: # prior exponential model priorExp &lt;- function(a, b){ dunif(a, 0, 1.5) * dunif(b, 0, 1.5) } # likelihood function exponential model lhExp &lt;- function(a, b){ theta &lt;- a * exp(-b * t) theta[theta &lt;= 0.0] &lt;- 1.0e-5 theta[theta &gt;= 1.0] &lt;- 1 - 1.0e-5 prod(dbinom(x = obs, prob = theta, size = 100)) } # prior power model priorPow &lt;- function(c, d){ dunif(c, 0, 1.5) * dunif(d, 0, 1.5) } # likelihood function power model lhPow &lt;- function(c, d){ theta &lt;- c * t^(-d) theta[theta &lt;= 0.0] &lt;- 1.0e-5 theta[theta &gt;= 1.0] &lt;- 1 - 1.0e-5 prod(dbinom(x = obs, prob = theta, size = 100)) } To approximate each model’s marginal likelihood via grid approximation, we consider equally spaced values for both parameters (a tighly knit grid), assess the prior and likelihood for each parameter pair and finally take the sum over all of the visited values: # make sure the functions accept vector input lhExp &lt;- Vectorize(lhExp) lhPow &lt;- Vectorize(lhPow) # define the step size of the grid stepsize &lt;- 0.01 # calculate the marginal likelihood marg_lh &lt;- expand.grid( x = seq(0.005, 1.495, by = stepsize), y = seq(0.005, 1.495, by = stepsize) ) %&gt;% mutate( lhExp = lhExp(x, y), priExp = 1 / length(x), # uniform priors! lhPow = lhPow(x, y), priPow = 1 / length(x) ) # output result str_c( &quot;BF in favor of exponential model: &quot;, with(marg_lh, sum(priExp * lhExp) / sum(priPow * lhPow)) %&gt;% round(2) ) ## [1] &quot;BF in favor of exponential model: 1221.39&quot; Based on this computation, we would be entitled to conclude that the data provide overwhelming evidence in favor of the exponential model. The result tells us that a rational agent should adjust her prior odds by a factor of more than 1000 in favor of the exponential model when updating her beliefs with the data. In other words, the data tilt our beliefs very strongly towards the exponential model, no matter what we believed initially. In this sense, the data provide strong evidence for the exponential model. 10.3.2 Naive Monte Carlo For simple models (with maybe 4-5 free parameters), we can also use naive Monte Carlo sampling to approximate Bayes factors. In particular, we can approximate the marginal likelihood by taking samples from the prior, calculating the likelihood of the data for each sampled parameter tuple, and then averaging over all calculated likelihoods: \\[P(D, M_i) = \\int P(D \\mid \\theta, M_i) \\ P(\\theta \\mid M_i) \\ \\text{d}\\theta \\approx \\frac{1}{n} \\sum^{n}_{\\theta_j \\sim P(\\theta \\mid M_i)} P(D \\mid \\theta_j, M_i)\\] Here is a calculation using one million samples from the prior of each model: nSamples &lt;- 1000000 # sample from the prior a &lt;- runif(nSamples, 0, 1.5) b &lt;- runif(nSamples, 0, 1.5) # calculate likelihood of data for each sample lhExpVec &lt;- lhExp(a, b) lhPowVec &lt;- lhPow(a, b) # compute marginal likelihoods str_c( &quot;BF in favor of exponential model: &quot;, round(mean(lhExpVec) / mean(lhPowVec), 2) ) ## [1] &quot;BF in favor of exponential model: 1251.66&quot; We can also check the time course of our MC-estimate by a plot like that in Figure 10.3. The plot shows the current estimate of the Bayes factor on the \\(y\\)-axis after having taken the number of samples given on the \\(x\\)-axis. We see that the initial calculations (after only 10,000 samples) are far off, but that the approximation finally gets reasonably close to the value calculated by grid approximation, which is shown as the red line. Figure 10.3: Temporal development (as more samples come in) of the Monte Carlo estimate of the Bayes factor in favor of the exponential model over the power model of forgetting. The red horizontal line indicates the Bayes factor estimate obtained previously via grid approximation. Exercise 11.3 Which statements concerning Bayes Factors (BF) are correct? The Bayes Factor shows the absolute probability of a particular model to be a good explanation of the observed data. If \\(BF_{12} = 11\\), one should conclude that there is strong evidence in favor of \\(M_1\\). Grid approximation allows us to compare no more than five models simultaneously. With the Naive Monte Carlo method, we can only approximate the BF for models with continuous parameters. BF computation penalizes more complex models. Solution Statements b. and e. are correct. 10.3.3 Excursion: Bridge sampling For more complex models (e.g., high-dimensional/hierarchical parameter spaces), naive Monte Carlo methods can be highly inefficient. If random sampling of parameter values from the priors is unlikely to deliver values for which the likelihood of the data is reasonably high, most naive MC samples will contribute very little information to the overall estimate of the marginal likelihood. For this reason, there are better sampling-based procedures which preferentially sample a posteriori credible parameter values (given the data) and use clever math to compensate for using the wrong distribution to sample from. This is the main idea behind approaches like importance sampling. A very promising approach is in particular bridge sampling, which also has its own R package (Gronau et al. 2017). We will not go into the formal details of this method, but just showcase here an application of the bridgesampling package. This approach requires samples from the posterior, which we can obtain using Stan (see Section 9.3.2). Towards this end, we first assemble the data for input to the Stan program in a list: forgetting_data &lt;- list( N = 100, k = obs, t = t ) The models are implemented in Stan. We here only show the exponential model. data { int&lt;lower=1&gt; N ; int&lt;lower=0,upper=N&gt; k[6] ; int&lt;lower=0&gt; t[6]; } parameters { real&lt;lower=0,upper=1.5&gt; a ; real&lt;lower=0,upper=1.5&gt; b ; } model { // likelihood for (i in 1:6) { target += binomial_lpmf(k[i] | N, a * exp(-b * t[i])) ; } } We then use Stan to obtain samples from the posterior in the usual way. To get reliable estimates of Bayes factors via bridge sampling, we should take a much larger number of samples than we usually would for a reliable estimation of, say, the posterior means and credible intervals. stan_fit_expon &lt;- rstan::stan( # where is the Stan code file = &#39;models_stan/model_comp_exponential_forgetting.stan&#39;, # data to supply to the Stan program data = forgetting_data, # how many iterations of MCMC iter = 20000, # how many warmup steps warmup = 2000 ) ## ## SAMPLING FOR MODEL &#39;model_comp_exponential_forgetting&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.4e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 1: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 1: Iteration: 2001 / 20000 [ 10%] (Sampling) ## Chain 1: Iteration: 4000 / 20000 [ 20%] (Sampling) ## Chain 1: Iteration: 6000 / 20000 [ 30%] (Sampling) ## Chain 1: Iteration: 8000 / 20000 [ 40%] (Sampling) ## Chain 1: Iteration: 10000 / 20000 [ 50%] (Sampling) ## Chain 1: Iteration: 12000 / 20000 [ 60%] (Sampling) ## Chain 1: Iteration: 14000 / 20000 [ 70%] (Sampling) ## Chain 1: Iteration: 16000 / 20000 [ 80%] (Sampling) ## Chain 1: Iteration: 18000 / 20000 [ 90%] (Sampling) ## Chain 1: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.055513 seconds (Warm-up) ## Chain 1: 0.444982 seconds (Sampling) ## Chain 1: 0.500495 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;model_comp_exponential_forgetting&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1.1e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 2: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 2: Iteration: 2001 / 20000 [ 10%] (Sampling) ## Chain 2: Iteration: 4000 / 20000 [ 20%] (Sampling) ## Chain 2: Iteration: 6000 / 20000 [ 30%] (Sampling) ## Chain 2: Iteration: 8000 / 20000 [ 40%] (Sampling) ## Chain 2: Iteration: 10000 / 20000 [ 50%] (Sampling) ## Chain 2: Iteration: 12000 / 20000 [ 60%] (Sampling) ## Chain 2: Iteration: 14000 / 20000 [ 70%] (Sampling) ## Chain 2: Iteration: 16000 / 20000 [ 80%] (Sampling) ## Chain 2: Iteration: 18000 / 20000 [ 90%] (Sampling) ## Chain 2: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.054433 seconds (Warm-up) ## Chain 2: 0.461635 seconds (Sampling) ## Chain 2: 0.516068 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;model_comp_exponential_forgetting&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 9e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 3: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 3: Iteration: 2001 / 20000 [ 10%] (Sampling) ## Chain 3: Iteration: 4000 / 20000 [ 20%] (Sampling) ## Chain 3: Iteration: 6000 / 20000 [ 30%] (Sampling) ## Chain 3: Iteration: 8000 / 20000 [ 40%] (Sampling) ## Chain 3: Iteration: 10000 / 20000 [ 50%] (Sampling) ## Chain 3: Iteration: 12000 / 20000 [ 60%] (Sampling) ## Chain 3: Iteration: 14000 / 20000 [ 70%] (Sampling) ## Chain 3: Iteration: 16000 / 20000 [ 80%] (Sampling) ## Chain 3: Iteration: 18000 / 20000 [ 90%] (Sampling) ## Chain 3: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.05527 seconds (Warm-up) ## Chain 3: 0.487729 seconds (Sampling) ## Chain 3: 0.542999 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;model_comp_exponential_forgetting&#39; NOW (CHAIN 4). ## Chain 4: Rejecting initial value: ## Chain 4: Error evaluating the log probability at the initial value. ## Chain 4: Exception: binomial_lpmf: Probability parameter is 1.02476, but must be in the interval [0, 1] (in &#39;model3e961577733f_model_comp_exponential_forgetting&#39; at line 13) ## ## Chain 4: ## Chain 4: Gradient evaluation took 1e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 4: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 4: Iteration: 2001 / 20000 [ 10%] (Sampling) ## Chain 4: Iteration: 4000 / 20000 [ 20%] (Sampling) ## Chain 4: Iteration: 6000 / 20000 [ 30%] (Sampling) ## Chain 4: Iteration: 8000 / 20000 [ 40%] (Sampling) ## Chain 4: Iteration: 10000 / 20000 [ 50%] (Sampling) ## Chain 4: Iteration: 12000 / 20000 [ 60%] (Sampling) ## Chain 4: Iteration: 14000 / 20000 [ 70%] (Sampling) ## Chain 4: Iteration: 16000 / 20000 [ 80%] (Sampling) ## Chain 4: Iteration: 18000 / 20000 [ 90%] (Sampling) ## Chain 4: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.055248 seconds (Warm-up) ## Chain 4: 0.424547 seconds (Sampling) ## Chain 4: 0.479795 seconds (Total) ## Chain 4: stan_fit_power &lt;- rstan::stan( # where is the Stan code file = &#39;models_stan/model_comp_power_forgetting.stan&#39;, # data to supply to the Stan program data = forgetting_data, # how many iterations of MCMC iter = 20000, # how many warmup steps warmup = 2000 ) ## ## SAMPLING FOR MODEL &#39;model_comp_power_forgetting&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 1: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 1: Iteration: 2001 / 20000 [ 10%] (Sampling) ## Chain 1: Iteration: 4000 / 20000 [ 20%] (Sampling) ## Chain 1: Iteration: 6000 / 20000 [ 30%] (Sampling) ## Chain 1: Iteration: 8000 / 20000 [ 40%] (Sampling) ## Chain 1: Iteration: 10000 / 20000 [ 50%] (Sampling) ## Chain 1: Iteration: 12000 / 20000 [ 60%] (Sampling) ## Chain 1: Iteration: 14000 / 20000 [ 70%] (Sampling) ## Chain 1: Iteration: 16000 / 20000 [ 80%] (Sampling) ## Chain 1: Iteration: 18000 / 20000 [ 90%] (Sampling) ## Chain 1: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.045508 seconds (Warm-up) ## Chain 1: 0.37634 seconds (Sampling) ## Chain 1: 0.421848 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;model_comp_power_forgetting&#39; NOW (CHAIN 2). ## Chain 2: Rejecting initial value: ## Chain 2: Error evaluating the log probability at the initial value. ## Chain 2: Exception: binomial_lpmf: Probability parameter is 1.31725, but must be in the interval [0, 1] (in &#39;model3e9655fa1358_model_comp_power_forgetting&#39; at line 13) ## ## Chain 2: ## Chain 2: Gradient evaluation took 1e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 2: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 2: Iteration: 2001 / 20000 [ 10%] (Sampling) ## Chain 2: Iteration: 4000 / 20000 [ 20%] (Sampling) ## Chain 2: Iteration: 6000 / 20000 [ 30%] (Sampling) ## Chain 2: Iteration: 8000 / 20000 [ 40%] (Sampling) ## Chain 2: Iteration: 10000 / 20000 [ 50%] (Sampling) ## Chain 2: Iteration: 12000 / 20000 [ 60%] (Sampling) ## Chain 2: Iteration: 14000 / 20000 [ 70%] (Sampling) ## Chain 2: Iteration: 16000 / 20000 [ 80%] (Sampling) ## Chain 2: Iteration: 18000 / 20000 [ 90%] (Sampling) ## Chain 2: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.044843 seconds (Warm-up) ## Chain 2: 0.333266 seconds (Sampling) ## Chain 2: 0.378109 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;model_comp_power_forgetting&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 3: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 3: Iteration: 2001 / 20000 [ 10%] (Sampling) ## Chain 3: Iteration: 4000 / 20000 [ 20%] (Sampling) ## Chain 3: Iteration: 6000 / 20000 [ 30%] (Sampling) ## Chain 3: Iteration: 8000 / 20000 [ 40%] (Sampling) ## Chain 3: Iteration: 10000 / 20000 [ 50%] (Sampling) ## Chain 3: Iteration: 12000 / 20000 [ 60%] (Sampling) ## Chain 3: Iteration: 14000 / 20000 [ 70%] (Sampling) ## Chain 3: Iteration: 16000 / 20000 [ 80%] (Sampling) ## Chain 3: Iteration: 18000 / 20000 [ 90%] (Sampling) ## Chain 3: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.046901 seconds (Warm-up) ## Chain 3: 0.365732 seconds (Sampling) ## Chain 3: 0.412633 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;model_comp_power_forgetting&#39; NOW (CHAIN 4). ## Chain 4: Rejecting initial value: ## Chain 4: Error evaluating the log probability at the initial value. ## Chain 4: Exception: binomial_lpmf: Probability parameter is 1.16612, but must be in the interval [0, 1] (in &#39;model3e9655fa1358_model_comp_power_forgetting&#39; at line 13) ## ## Chain 4: ## Chain 4: Gradient evaluation took 1e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 4: Iteration: 2000 / 20000 [ 10%] (Warmup) ## Chain 4: Iteration: 2001 / 20000 [ 10%] (Sampling) ## Chain 4: Iteration: 4000 / 20000 [ 20%] (Sampling) ## Chain 4: Iteration: 6000 / 20000 [ 30%] (Sampling) ## Chain 4: Iteration: 8000 / 20000 [ 40%] (Sampling) ## Chain 4: Iteration: 10000 / 20000 [ 50%] (Sampling) ## Chain 4: Iteration: 12000 / 20000 [ 60%] (Sampling) ## Chain 4: Iteration: 14000 / 20000 [ 70%] (Sampling) ## Chain 4: Iteration: 16000 / 20000 [ 80%] (Sampling) ## Chain 4: Iteration: 18000 / 20000 [ 90%] (Sampling) ## Chain 4: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.046359 seconds (Warm-up) ## Chain 4: 0.398662 seconds (Sampling) ## Chain 4: 0.445021 seconds (Total) ## Chain 4: The bridgesampling package can then be used to calculate each model’s marginal likelihood. expon_bridge &lt;- bridgesampling::bridge_sampler(stan_fit_expon, silent = T) power_bridge &lt;- bridgesampling::bridge_sampler(stan_fit_power, silent = T) We then obtain an estimate of the Bayes factor in favor of the exponential model with this function: bridgesampling::bf(expon_bridge, power_bridge) ## Estimated Bayes factor in favor of expon_bridge over power_bridge: 1220.52849 References "],["ch-03-07-hypothesis-testing-Bayes.html", "11 Bayesian hypothesis testing", " 11 Bayesian hypothesis testing This chapter introduces common Bayesian methods of testing what we could call statistical hypotheses. A statistical hypothesis is a hypothesis about a particular model parameter or a set of model parameters. Most often, such a hypothesis concerns one parameter, and the assumption in question is that this parameter takes on a specific value, or some value from a specific interval. Henceforth, we speak just of a “hypothesis” even though we mean a specific hypothesis about particular model parameters. For example, we might be interested in what we will call a point-valued hypothesis, stating that the value of parameter \\(\\theta\\) is fixed to a specific value \\(\\theta = \\theta^*\\). Section 11.1 introduces different kinds of statistical hypotheses in more detail. Given a statistical hypothesis about parameter values, we are interested in “testing” it. Strictly speaking, the term “testing” should probably be reserved for statistical decision procedures which give clear categorical judgements, such as whether to reject a hypothesis, accept it as true or to withhold judgement because no decision can be made (yet/currently). While we will encounter such categorical decision routines in this chapter, Bayesian approaches to hypotheses “testing” are first and foremost concerned, not with categorical decisions, but with quantifying evidence in favor or against the hypothesis in question. (In a second step, using Bayesian decision theory which also weighs in the utility of different policy choices, we can use Bayesian inference also for informed decision making, of course.) But instead of speaking of “Bayesian inference to weigh evidence for/against a hypothesis” we will just speak of “Bayesian hypothesis testing” for ease of parlor. We consider two conceptually distinct approaches within Bayesian hypothesis testing. Estimation-based testing considers just one model. It uses the observed data \\(D_\\text{obs}\\) to retrieve posterior beliefs \\(P(\\theta \\mid D_{\\text{obs}})\\) and checks whether, a posteriori, our hypothesis is credible. Comparison-based testing uses Bayesian model comparison, in the form of Bayes factors, to compare two models, namely one model that assumes that the hypothesis in question is true, and one model that assumes that the complement of the hypothesis is true. The main difference between these two approaches is that estimation-based hypothesis testing is simpler (conceptually and computationally), but less informative than comparison-based hypothesis testing. In fact, comparison-based methods give a clearer picture of the quantitative evidence for/against a hypothesis because they explicitly take into account a second alternative to the hypothesis which is to be tested. As we will see in this chapter, the technical obstacles for comparison-based approaches can be overcome. For special but common use cases, like testing directional hypotheses, there are efficient methods of performing comparison-based hypothesis testing. The learning goals for this chapter are: understand the notion of a statistical hypothesis point-valued, ROPE-d and directional hypotheses complement / alternative hypothesis be able to apply Bayesian hypothesis testing to (simple) case studies understand and be able to apply the Savage-Dickey method (and its extension to interval-based hypotheses in terms of encompassing models) become familiar with a Bayesian \\(t\\)-test model for comparing the means of two groups of metric measurements "],["ch-03-07-hypothesis-testing-Bayes-hypotheses.html", "11.1 Statistical hypotheses", " 11.1 Statistical hypotheses Given a model \\(M\\) with parameter space \\(\\Theta\\), a statistical hypothesis, in the sense entertained here, is an assumption (made for purposes of investigation) that certain parameters \\(\\Theta_i\\) take on only a restricted range of values. For example, we might be interested in the question of whether a particular coin is fair. We consider the Binomial model, which contains the coin bias parameter \\(\\theta_c\\). Informal assumptions about the coin’s bias can then be translated into a concrete question about values of \\(\\theta_{c}\\). This chapter considers three types of statistical hypotheses, which are also represented schematically in Figure 11.1. While all of the below also applies to discrete parameters and vectors of parameters, the implicit assumption in what follows is that we are dealing with a single continuous parameter. Point-valued hypotheses ask whether it is plausible that the parameter of relevance is identical to exactly one specific value. For example, in a Binomial model with the coin’s bias parameter \\(\\theta_{c}\\), a point-valued hypothesis could be that \\(\\theta_c = 0.5\\). More generally, we write \\(\\theta = \\theta^*\\) for a point-valued hypothesis about some (singular) parameter \\(\\theta\\). ROPE-d hypotheses, where “ROPE” is short for region of practical equivalence, define a small \\(\\epsilon\\)-region around a point-value of interest, and address the question of whether it is plausible that the parameter value lies inside this interval. For example, suppose that instead of addressing the point-valued hypothesis \\(\\theta_{c} = 0.5\\) about a coin’s latent bias, we are able (e.g., through prior research or a priori conceptual considerations) to specify a reasonable region of practical equivalence (= ROPE) around the parameter value of interest. For instance, we might know that a difference of 0.1 in a coin’s bias really counts as normal slack and negligible for practical purposes. We then address the ROPE-d hypothesis that \\(\\theta_{c} \\in [0.49, 0.51]\\). More generally, we write \\(\\theta \\in [\\theta^* - \\epsilon\\ ;\\ \\theta^* + \\epsilon]\\), or \\(\\theta = \\theta^* \\pm \\epsilon\\) for ROPE-d hypothesis around the pivotal values \\(\\theta^*\\). Directional hypotheses fix a specific parameter value, as a lower or upper bound and ask whether it is plausible that the parameter’s value is bigger or smaller than that fixed value. For example, \\(\\theta_{c} &gt; 0.5\\) could be the directional hypothesis that a coin is biased towards heads. Figure 11.1: Three common types of hypotheses anchored to a point-value of interest of a parameter. Ignoring trivial edge cases, both ROPE-d and directional hypotheses are instances of interval-based hypotheses in the sense that they assume that the true value lies in an interval. The complement of a point-valued hypothesis \\(\\theta = \\theta^*\\) is the hypothesis that the true value is not equal to the critical value: \\(\\theta \\neq \\theta^*\\). The complement of an interval-based hypothesis is the hypothesis that the true parameter value does not lie in the relevant interval. For example, the complement of the ROPE-d hypothesis \\(\\theta \\in [\\theta^* - \\epsilon\\ ;\\ \\theta^* + \\epsilon]\\) is that \\(\\theta \\not \\in [\\theta^* - \\epsilon\\ ;\\ \\theta^* + \\epsilon]\\). In the context of hypothesis testing, in particular frequentist testing (see Chapter ??), we often address the hypothesis to be tested as the null hypothesis. The complement of the null hypothesis is called alternative hypothesis. "],["data-and-models-for-this-chapter.html", "11.2 Data and models for this chapter", " 11.2 Data and models for this chapter This chapter uses two case studies as running examples: the (fictitious) 24/7 coin-flip example analyzed with the Binomial model, and data from the Simon task analyzed with a so-called Bayesian \\(t\\)-test model. 11.2.1 24/7 We will use the same (old) example of binomial data: \\(k = 7\\) heads out of \\(N = 24\\) coin flips. Just as before, we will use the standard binomial model with a flat Beta prior, shown below in graphical notation: Figure 11.2: The Binomial Model (repeated from before). We are interested in the following hypotheses: Point-valued: \\(\\theta_c = 0.5\\) ROPE-d \\(\\theta_c = \\in [0.5 - \\epsilon; 0.5 + \\epsilon]\\) with \\(\\epsilon = 0.01\\) Directional \\(\\theta_c &lt; 0.5\\) 11.2.2 Simon task The Simon task is a classic experimental design to investigate interference of, intuitively put, task-relevant properties and task-irrelevant properties. Chapter ?? introduces the experiment and the (cleaned) data we analyze here. data_simon_cleaned &lt;- aida::data_ST The most important columns in this data set for our current purposes are: RT: The reaction time for each trial. condition: Whether the trial was a congruent or an incongruent trial. Concretely, we are interested in comparing the mean reaction times across conditions: Figure 11.3: Distribution of reaction times of correct answers in the congruent and incongruent condition of the Simon task. Vertical lines indicate the mean of each condition. In order to compare the means of continuous measurements between two groups we will use a so-called \\(t\\)-test model. (The reason why this is called a “\\(t\\)-test model” is historical and will become clear in Chapter ??.) There are different variations of Bayesian \\(t\\)-test models. Here, we use the one proposed by Gönen et al. (2005), which enables us to compute Bayes factor model comparison for point-valued hypotheses analytically. The model is shown in Figure 11.4. Figure 11.4: Bayesian \\(t\\)-test model following Gönen et al. (2005) for inferences about the difference in means in the Simon task data. The model in Figure 11.4 assumes that there are two vectors \\(y_1\\) and \\(y_2\\) of continuous measurements. In our case, these are the continuous measurements of reaction times in the incongruent (\\(y_1\\)) and congruent (\\(y_2\\)) group. The model further assumes that all measurements in \\(y_1\\) and \\(y_2\\) are samples from two normal distributions, one for each group, with shared variance but possibly different means. The means of the two normal distributions are represented in terms of the midpoint \\(\\mu\\) between the means of either group. The model is set-up in such a way that there is a difference parameter \\(\\delta\\) which specifies the standardized difference between group means. Standardization here means that the difference between the means is represented in relation to the variance of the measurements in each group (which is assumed to be the same in both groups). The free variables in this model are therefore: the average of the group means \\(\\mu\\), the standardized difference \\(\\delta\\) of the group means from each other, and the common variance \\(\\sigma\\) of measurements in each group. The priors for these parameters are chosen in such a way as to enable direct calculation of Bayes factors for point-valued hypotheses. Notice that, by explicitly representing the difference parameter \\(\\delta\\) in the model, it is possible to put different kinds of a priori assumptions about the likely differences between groups directly into the model, namely in the form of \\(\\mu_g\\) and \\(g\\), which are not free model parameters, but will be set by us modelers, here as \\(\\mu_g = 0\\) and \\(g = 1\\). We focus on the first hypothesis spelled out in Chapter ??, namely that the correct choices are faster in the congruent condition than in the incongruent condition. So, based on this data and model, we are interested in the following statistical hypotheses: Point-valued: \\(\\delta = 0\\) ROPE-d \\(\\delta = \\in [0 - \\epsilon; 0 + \\epsilon]\\) with \\(\\epsilon = 0.1\\) Directional \\(\\delta &gt; 0\\) Exercise 11.1 Paraphrase the three hypotheses given for the 24/7 data and the three hypotheses given for the Simon task in your own words. Solution 24/7: Point-valued: \\(\\theta_c = 0.5\\) - the coin is fair, with a bias of exactly 0.5 ROPE-d \\(\\theta_c = \\in [0.5 - \\epsilon; 0.5 + \\epsilon]\\) with \\(\\epsilon = 0.01\\) - the coins bias lies between 0.49 and 0.51 Directional \\(\\theta_c &lt; 0.5\\) - the coin is biased towards tails Simon task: Point-valued: \\(\\delta = 0\\) - the difference between the means of reaction times in both groups is 0 ROPE-d \\(\\delta = \\in [0 - \\epsilon; 0 + \\epsilon]\\) with \\(\\epsilon = 0.1\\) - the absolute difference between the means of reaction times in both groups is no bigger than 10% of the variance in both groups Directional \\(\\delta &gt; 0\\) - the mean reaction time in group 1 is bigger than the mean reaction time in group 2 References "],["ch-03-05-Bayes-testing-estimation.html", "11.3 Testing via posterior estimation", " 11.3 Testing via posterior estimation The general logic of Bayesian hypothesis testing via parameter estimation is this. Let \\(M\\) be the assumed model for observed data \\(D_{\\text{obs}}\\). We use Bayesian posterior inference to calculate or approximate the posterior \\(P_M(\\theta \\mid M)\\). We then look at an interval-based estimate, most usually a Bayesian credible interval, and compare the hypothesis in question to the region of a posteriori most probable values for the parameter(s) targeted by the hypothesis. Concretely, for point-valued hypotheses we can use the following approach. Let \\(\\Theta\\) be the parameter space of a model \\(M\\). We are interested in some component \\(\\Theta_i\\) and our hypothesis is \\(\\Theta_i = \\theta^*_i\\) for some specific value \\(\\theta^*_i\\). A simple (but crude and controversial) way of addressing this point-valued hypothesis based on observed data \\(D\\) is to look at whether \\(\\theta^*_i\\) lies inside some credible interval for parameter \\(\\Theta_i\\) in the posterior derived by updating with data \\(D\\). A customary choice here are 95% credible intervals, but also other choices, e.g., 80% credible intervals, are used. If a categorical decision rule is needed, we can: accept the point-valued hypothesis if \\(\\theta^*\\) is inside of the credible interval; and reject the point-valued hypothesis if \\(\\theta^*\\) is outside of the credible interval. Kruschke (2015) extends this approach to also address ROPE-d hypotheses. He argues that we should not be concerned with point-valued hypotheses, but rather with intervals constructed around the point-value of interest. Kruschke, therefore, suggests looking at a region of practical equivalence (ROPE), usually defined by some \\(\\epsilon\\)-region around \\(\\theta^*_i\\): \\[\\text{ROPE}(\\theta^*_i) = [\\theta^*_i- \\epsilon, \\theta^*_i+ \\epsilon]\\] The choice of \\(\\epsilon\\) is context-dependent and requires an understanding of the scale at which parameter values \\(\\Theta_i\\) differ. If the parameter of interest is, for example, a difference \\(\\delta\\) in the means of reaction times, like in the Simon task, this parameter is intuitively interpretable. We can say, for instance, that an \\(\\epsilon\\)-region of \\(\\pm 15\\text{ms}\\) is really so short that any value in \\([-15\\text{ms}; 15\\text{ms}]\\) would be regarded as identical to \\(0\\) for all practical purposes because of what we know about reaction times and their potential differences. However, with parameters that are less clearly anchored to a concrete physical measurement about which we have solid distributional knowledge and/or reliable intuitions, fixing the size of the ROPE can be more difficult. For the bias of a coin flip, for instance, which we want to test at the point value \\(\\theta^* = 0.5\\) (testing the coin for fairness), we might want to consider a ROPE like \\([0.49; 0.51]\\), although this choice may be less objectively defensible without previous experimental evidence from similar situations. In Kruschke’s ROPE-based approach where \\(\\epsilon &gt; 0\\), the decision about a point-valued hypothesis becomes ternary. If \\([l;u]\\) is an interval-based estimate of parameter \\(\\Theta_i\\) and \\([\\theta^*_i - \\epsilon; \\theta^*_i + \\epsilon]\\) is the ROPE around the point-value of interest, we would: accept the point-valued hypothesis iff \\([l;u]\\) is contained entirely in \\([\\theta^*_i - \\epsilon; \\theta^*_i + \\epsilon]\\); reject the point-valued hypothesis iff \\([l;u]\\) and \\([\\theta^*_i - \\epsilon; \\theta^*_i + \\epsilon]\\) have no overlap; and withhold judgement otherwise. Going beyond Kruschke’s approach to ROPE-d hypotheses, it is possible to extend this ternary decision logic also to cover directional hypotheses. 11.3.1 Example: 24/7 For the Binomial model and the 24/7 data, we know that the posterior is of the form \\(\\text{Beta}(8,18)\\). Here is a plot of the posterior (repeated from before) which also includes the 95% credible interval for the coin bias \\(\\theta_c\\). To address our point-valued hypothesis of \\(\\theta_{c} = 0.5\\) that the coin is fair, we just have to check if the critical value of 0.5 is inside or outside the 95% credible interval. In the case at hand, it is not. We would therefore, by the binary decision logic of this approach, reject the hypothesis \\(\\theta_{c} = 0.5\\) that the coin is fair. (Notice that while, strictly speaking, this approach does not pay attention to how closely the credible interval includes or excludes the critical value, we should normally take into account that the boundaries of the credible intervals are uncertain estimates based on posterior samples.) Using the ROPE-approach of Kruschke, we notice that our ROPE of \\(\\theta = 0.5 \\pm 0.01\\) is also fully outside of the 95% HDI. Here too, we conclude that the idea of an “approximately fair coin” is sufficiently unlikely to act as if it was false. In other words, by the ternary decision logic of this approach, we would reject the ROPE-d hypothesis \\(\\theta = 0.5 \\pm 0.01\\). (In practice, especially when we are uncertain about how exactly to pin down \\(\\epsilon\\), we might also sometimes want to give the range of \\(\\epsilon\\) values for which the ROPE-d hypothesis would be accepted or rejected. So, here we could also say that for any \\(\\epsilon &lt; 0.016\\) we would reject the ROPE-d hypothesis.) The directional hypothesis that the coin is biased towards tails \\(\\theta_c &lt; 0.5\\) contains the 95% credible interval in its entirety. We would therefore, following the ternary decision logic, accept this hypothesis based on the model and data. 11.3.2 Example: Simon Task We use Stan to draw samples from the posterior distribution. We start with assembling the data: simon_data_4_Stan &lt;- list( y1 = data_simon_cleaned %&gt;% filter(condition == &quot;incongruent&quot;) %&gt;% pull(RT), N1 = nrow(data_simon_cleaned %&gt;% filter(condition == &quot;incongruent&quot;)), y2 = data_simon_cleaned %&gt;% filter(condition == &quot;congruent&quot;) %&gt;% pull(RT), N2 = nrow(data_simon_cleaned %&gt;% filter(condition == &quot;congruent&quot;)) ) Here is the model from Figure 11.4 implemented in Stan. data { int&lt;lower=1&gt; N1 ; int&lt;lower=1&gt; N2 ; vector[N1] y1 ; vector[N2] y2 ; } parameters { real mu ; real&lt;lower=0&gt; sigma ; real delta ; } model { # priors target += log(1/sigma) ; delta ~ normal(0, 1) ; # likelihood y1 ~ normal(mu + sigma*delta/2, sigma^2) ; y2 ~ normal(mu - sigma*delta/2, sigma^2) ; } # sampling stan_fit_ttest &lt;- rstan::stan( # where is the Stan code file = &#39;models_stan/ttest_model.stan&#39;, # data to supply to the Stan program data = simon_data_4_Stan, # how many iterations of MCMC # more samples b/c of following approximations iter = 20000, # how many warmup steps warmup = 1000 ) ## ## SAMPLING FOR MODEL &#39;ttest_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000116 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.16 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 1: Iteration: 1001 / 20000 [ 5%] (Sampling) ## Chain 1: Iteration: 3000 / 20000 [ 15%] (Sampling) ## Chain 1: Iteration: 5000 / 20000 [ 25%] (Sampling) ## Chain 1: Iteration: 7000 / 20000 [ 35%] (Sampling) ## Chain 1: Iteration: 9000 / 20000 [ 45%] (Sampling) ## Chain 1: Iteration: 11000 / 20000 [ 55%] (Sampling) ## Chain 1: Iteration: 13000 / 20000 [ 65%] (Sampling) ## Chain 1: Iteration: 15000 / 20000 [ 75%] (Sampling) ## Chain 1: Iteration: 17000 / 20000 [ 85%] (Sampling) ## Chain 1: Iteration: 19000 / 20000 [ 95%] (Sampling) ## Chain 1: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 2.26135 seconds (Warm-up) ## Chain 1: 12.4931 seconds (Sampling) ## Chain 1: 14.7545 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;ttest_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0.000113 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.13 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 2: Iteration: 1001 / 20000 [ 5%] (Sampling) ## Chain 2: Iteration: 3000 / 20000 [ 15%] (Sampling) ## Chain 2: Iteration: 5000 / 20000 [ 25%] (Sampling) ## Chain 2: Iteration: 7000 / 20000 [ 35%] (Sampling) ## Chain 2: Iteration: 9000 / 20000 [ 45%] (Sampling) ## Chain 2: Iteration: 11000 / 20000 [ 55%] (Sampling) ## Chain 2: Iteration: 13000 / 20000 [ 65%] (Sampling) ## Chain 2: Iteration: 15000 / 20000 [ 75%] (Sampling) ## Chain 2: Iteration: 17000 / 20000 [ 85%] (Sampling) ## Chain 2: Iteration: 19000 / 20000 [ 95%] (Sampling) ## Chain 2: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 1.88242 seconds (Warm-up) ## Chain 2: 14.2257 seconds (Sampling) ## Chain 2: 16.1081 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;ttest_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0.000114 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.14 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 3: Iteration: 1001 / 20000 [ 5%] (Sampling) ## Chain 3: Iteration: 3000 / 20000 [ 15%] (Sampling) ## Chain 3: Iteration: 5000 / 20000 [ 25%] (Sampling) ## Chain 3: Iteration: 7000 / 20000 [ 35%] (Sampling) ## Chain 3: Iteration: 9000 / 20000 [ 45%] (Sampling) ## Chain 3: Iteration: 11000 / 20000 [ 55%] (Sampling) ## Chain 3: Iteration: 13000 / 20000 [ 65%] (Sampling) ## Chain 3: Iteration: 15000 / 20000 [ 75%] (Sampling) ## Chain 3: Iteration: 17000 / 20000 [ 85%] (Sampling) ## Chain 3: Iteration: 19000 / 20000 [ 95%] (Sampling) ## Chain 3: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 3.13506 seconds (Warm-up) ## Chain 3: 13.2721 seconds (Sampling) ## Chain 3: 16.4072 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;ttest_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 0.000114 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.14 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 20000 [ 0%] (Warmup) ## Chain 4: Iteration: 1001 / 20000 [ 5%] (Sampling) ## Chain 4: Iteration: 3000 / 20000 [ 15%] (Sampling) ## Chain 4: Iteration: 5000 / 20000 [ 25%] (Sampling) ## Chain 4: Iteration: 7000 / 20000 [ 35%] (Sampling) ## Chain 4: Iteration: 9000 / 20000 [ 45%] (Sampling) ## Chain 4: Iteration: 11000 / 20000 [ 55%] (Sampling) ## Chain 4: Iteration: 13000 / 20000 [ 65%] (Sampling) ## Chain 4: Iteration: 15000 / 20000 [ 75%] (Sampling) ## Chain 4: Iteration: 17000 / 20000 [ 85%] (Sampling) ## Chain 4: Iteration: 19000 / 20000 [ 95%] (Sampling) ## Chain 4: Iteration: 20000 / 20000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 2.38547 seconds (Warm-up) ## Chain 4: 13.1485 seconds (Sampling) ## Chain 4: 15.5339 seconds (Total) ## Chain 4: Here is a concise summary of the relevant parameters: Bayes_estimates_ST &lt;- rstan::As.mcmc.list( stan_fit_ttest, pars = c(&#39;delta&#39;, &#39;mu&#39;, &#39;sigma&#39;) ) %&gt;% aida::summarize_mcmc_list() Bayes_estimates_ST ## # A tibble: 3 × 4 ## Parameter `|95%` mean `95|%` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 delta 1.44 1.71 1.97 ## 2 mu 460. 462. 463. ## 3 sigma 9.63 9.68 9.72 Figure 11.5 shows the posterior distribution over \\(\\delta\\) and the 95% HDI (in red). Figure 11.5: Posterior density of the \\(\\delta\\) parameter in the Bayesian \\(t\\)-test model for Simon task data with the 95% HDI (in red). For the point-valued estimate of \\(\\delta = 0\\), which is clearly outside of the 95% credible interval, the binary decision criterion would have us reject the hypothesis that the difference between group means is precisely zero. For a ROPE-d hypothesis \\(\\delta = 0 \\pm 0.1\\), we reach the same conclusion by the ternary decision rule of Kruschke, since the entire ROPE is outside of the credible interval. The directional hypothesis that \\(\\delta &gt; 0\\) is accepted by the ternary decision approach. Exercise 11.2 In this exercise, we will recap the decision rules of the two approaches introduced in this chapter. Using the binary approach for point-valued hypotheses, there are two possible outcomes, namely rejecting \\(H_0\\) and failing to reject \\(H_0\\). Following Kruschke’s ROPE approach, we can also withhold judgment. Use pen and paper to draw examples of the situations a-e given below. For each case, draw any distribution representing the posterior (e.g., a bell-shaped curve), the approximate 95% HDI and an arbitrary point value of interest \\(\\theta^*\\). For tasks c-e, also draw an arbitrary ROPE around the point value. Concretely, we’d like you to sketch… …one instance where we would not reject a point-valued hypothesis \\(H_0: \\theta = \\theta^*\\). …one instance where we would reject a point-valued hypothesis \\(H_0: \\theta = \\theta^*\\). …two instances where we would not reject a ROPE-d hypothesis \\(H_0: \\theta = \\theta^* \\pm \\epsilon\\). …two instances where we would reject a ROPE-d hypothesis \\(H_0: \\theta = \\theta^* \\pm \\epsilon\\). …two instances where we would withhold judgement regarding a ROPE-d hypothesis \\(H_0: \\theta = \\theta^* \\pm \\epsilon\\). Solution One solution to this exercise might look as follows. The red shaded area under the curves shows the 95% credible interval. The black dots represent (arbitrary) point values of interest, and the horizontal bars in panels (c)-(e) depict the ROPE around a given point value. References "],["ch-03-05-Bayesian-testing-comparison.html", "11.4 Testing via model comparison", " 11.4 Testing via model comparison Testing hypotheses based on parameter estimation, and in particular the categorical decision rules for accepting or rejecting hypotheses outlined in the previous section, only give a very coarse-grained picture. Bayesian analysis is about providing quantitative information about uncertainty and evidence, which are intuitive and easily interpretable. So, we would also like to have a quantitative assessment of the evidence for or against a hypothesis provided by some data against the background of a given model. This is what the comparison-based approaches to Bayesian hypothesis testing give us. Here is some further motivation why model comparison might be a good replacement for “testing via estimation”. A statistical hypothesis \\(H\\) is basically an event: a subset of parameter values are picked out of the whole parameter space. After observing data \\(D_\\text{obs}\\) and based on model \\(M\\), the ideal measure to have is \\(P_M(H \\mid D_\\text{obs})\\): given data and model, how likely is the hypothesis in question? The problem with this posterior formulation \\(P_M(H \\mid D_\\text{obs})\\) is that, for it to be meaningful, it must quantify over the set of all alternative hypotheses. If \\(H\\) is a point-valued hypothesis over a single parameter, the set of all alternative hypotheses could comprise all other logically possible point-valued hypotheses for the same parameter. But then, if that parameter is a continuous parameter, the posterior density at \\(P_M(H \\mid D_\\text{obs})\\) is not meaningfully interpretable as a probability (mass). If \\(H\\) is an interval-based hypothesis, the posterior \\(P_M(H \\mid D_\\text{obs})\\) would be meaningfully interpretable as a probability (mass), but still the question of what exactly the space of alternatives is is left implicit. Moreover, the posterior \\(P_M(H \\mid D_\\text{obs})\\) is influenced by the model’s prior over \\(H\\). So, a nominally high value of \\(P_M(H \\mid D_\\text{obs})\\) is as such uninteresting because we would need to take the prior \\(P_M(H)\\) into account as well. This is why a comparison-based approach to Bayesian hypothesis testing explicitly compares two models: The null model \\(M_0\\) is the model that incorporates the assumption of the hypothesis \\(H\\) to be tested. For example, the null model would put prior probability zero on those parameter values which are ruled out by \\(H\\). The alternative model \\(M_1\\) is an explicitly formulated model which incorporates some contextually or technically useful alternative to \\(M_0\\). The comparison-based approach to hypothesis testing then quantifies, using Bayes factors, the evidence that \\(D_\\text{obs}\\) provides for or against \\(M_0\\) (the model representing the “null hypothesis”) over the alternative model \\(M_1\\) (the model representing the alternative hypothesis). In this way, by looking at the ratio: \\[ BF_{01} = \\frac{P(D_\\text{obs} \\mid M_0)}{P(D_\\text{obs} \\mid M_1)} \\] this approach is independent of the prior probability assigned to models \\(P(M_0)\\) and \\(P(M_1)\\). Notice, however, that it is not independent of the priors over \\(\\theta\\) used in \\(M_1\\)! When the null hypothesis is point-valued, the alternative model is not based on the complement \\(\\theta \\neq \\theta^*\\), but on the technically much more practical and also conceptually more plausible alternative model that assumes that \\(\\theta\\) is free to range over a larger interval including, but not limited to \\(\\theta^*\\). We can then use the so-called Savage-Dickey method, described in Section 11.4.1, to compare the null and the alternative models as so-called nested models. When the null hypothesis is interval-valued, the alternative model can be conceived as based on the complement of the null hypothesis. We can then use an extension of the Savage-Dickey method based on a so-called encompassing model, described in Section 11.4.2, where we construe both the null model and the alternative model as nested under a third, well, encompassing model. This chapter shows how Bayes factors can be approximated based on samples from the posterior following both of these approaches. 11.4.1 The Savage-Dickey method The Savage-Dickey method is a very convenient way of computing Bayes factors for nested models, especially when models only differ with respect to one parameter. Suppose that there are \\(n\\) continuous parameters of interest \\(\\theta = \\langle \\theta_1, \\dots, \\theta_n \\rangle\\). \\(M_1\\) is a (Bayesian) model defined by \\(P(\\theta \\mid M_1)\\) and \\(P(D \\mid \\theta, M_1)\\). \\(M_0\\) is properly nested under \\(M_1\\) if: \\(M_0\\) assigns fixed values to parameters \\(\\theta_i = x_i, \\dots, \\theta_n = x_n\\) \\(P(D \\mid \\theta_1, \\dots, \\theta_{i-1}, M_0) = P(D \\mid \\theta_1, \\dots, \\theta_{i-1}, \\theta_i = x_i, \\dots, \\theta_n = x_n, M_1)\\) \\(\\lim_{\\theta_i \\rightarrow x_i, \\dots, \\theta_n \\rightarrow x_n} P(\\theta_1, \\dots, \\theta_{i-1} \\mid \\theta_i, \\dots, \\theta_n, M_1) = P(\\theta_1, \\dots, \\theta_{i-1} \\mid M_0)\\) Intuitively put, \\(M_0\\) is properly nested under \\(M_1\\), if \\(M_0\\) is a special case of \\(M_1\\) which fixes certain parameters to specific point-values. Notice that the last condition is satisfied in particular when \\(M_1\\)’s prior over \\(\\theta_1, \\dots, \\theta_{i-1}\\) is independent of the values for the remaining parameters. We can express a point-valued hypothesis in terms of a model \\(M_0\\) which is nested under the alternative model \\(M_1\\), the latter of which assumes that the parameters in question can take more than one value. For such properly nested models, we can compute a Bayes factor efficiently using the following result. Theorem 11.1 (Savage-Dickey Bayes factors for nested models) Let \\(M_0\\) be properly nested under \\(M_1\\) s.t. \\(M_0\\) fixes \\(\\theta_i = x_i, \\dots, \\theta_n = x_n\\). The Bayes factor \\(\\text{BF}_{01}\\) in favor of \\(M_0\\) over \\(M_1\\) is then given by the ratio of posterior probability to prior probability of the parameters \\(\\theta_i = x_i, \\dots, \\theta_n = x_n\\) from the point of view of the nesting model \\(M_1\\): \\[ \\begin{aligned} \\text{BF}_{01} &amp; = \\frac{P(\\theta_i = x_i, \\dots, \\theta_n = x_n \\mid D, M_1)}{P(\\theta_i = x_i, \\dots, \\theta_n = x_n \\mid M_1)} \\end{aligned} \\] Show proof. Proof. Let’s assume that \\(M_0\\) has parameters \\(\\theta = \\langle\\phi, \\psi\\rangle\\) with \\(\\phi = \\phi_0\\), and that \\(M_1\\) has parameters \\(\\theta = \\langle\\phi, \\psi \\rangle\\) with \\(\\phi\\) free to vary. If \\(M_0\\) is properly nested under \\(M_1\\), we know that \\(\\lim_{\\phi \\rightarrow \\phi_0} P(\\psi \\mid \\phi, M_1) = P(\\psi \\mid M_0)\\). We can then rewrite the marginal likelihood under \\(M_0\\) as follows: \\[ \\begin{aligned} P(D \\mid M_0) &amp; = \\int P(D \\mid \\psi, M_0) P(\\psi \\mid M_0) \\ \\text{d}\\psi &amp; \\text{[marginalization]} \\\\ &amp; = \\int P(D \\mid \\psi, \\phi = \\phi_0, M_1) P(\\psi \\mid \\phi = \\phi_0, M_1) \\ \\text{d}\\psi &amp; \\text{[assumption of nesting]} \\\\ &amp; = P(D \\mid \\phi = \\phi_0, M_1) &amp; \\text{[marginalization]} \\\\ &amp; = \\frac{P(\\phi = \\phi_0 \\mid D, M_1) P(D \\mid M_1)}{P(\\phi = \\phi_0 \\mid M_1)} &amp; \\text{[Bayes rule]} \\end{aligned} \\] The result follows if we divide by \\(P(D \\mid M_1)\\) on both sides of the equation.   11.4.1.1 Example: 24/7 Here is an example based on the 24/7 data. For a nesting model with a flat prior (\\(\\theta \\sim^{M_1} \\text{Beta}(1,1)\\)), and a point hypothesis \\(\\theta_c = 0.5\\), we just have to calculate the prior and posterior probability of the critical value \\(\\theta_c = 0.5\\): # point-value of interest theta_star &lt;- 0.5 # posterior probability in nesting model posterior_theta_star &lt;- dbeta(theta_star, 8, 18) # prior probability in nesting model prior_theta_star &lt;- dbeta(theta_star, 1, 1) # Bayes factor (using Savage-Dickey) BF_01 &lt;- posterior_theta_star / prior_theta_star BF_01 ## [1] 0.5157351 This is very minor evidence in favor of the alternative model (Bayes factor \\(\\text{BF}_{10} \\approx 1.94\\)). We would not like to draw any (strong) categorical conclusions from this result regarding the question of whether the coin might be fair. Figure 11.6 also shows the relation between prior and posterior at the point-value of interest. Figure 11.6: Illustration of the Savage-Dickey method of Bayes factor computation for the 24/7 case. 11.4.1.2 Example: Simon task In the previous 24/7 example, using the Savage-Dickey method was particularly easy because we know a closed-form solution of the precise posterior, so that we could easily calculate the posterior for the critical value without further ado. When this is not the case, like in the application to the Simon task data, we have to obtain an estimate for the posterior density at the critical value, here: \\(\\delta = 0\\), from the posterior samples which we obtain from sampling, as we did earlier in this chapter (using Stan). An approximate method for obtaining this value is implemented in the polspline package (using polynomial splines to approximate the posterior curve). # extract the samples for the delta parameter # from the earlier Stan fit delta_samples &lt;- tidy_draws_tt2 %&gt;% filter(Parameter == &quot;delta&quot;) %&gt;% pull(value) # estimating the posterior density at delta = 0 with polynomial splines fit.posterior &lt;- polspline::logspline(delta_samples) posterior_delta_null &lt;- polspline::dlogspline(0, fit.posterior) # computing the prior density of the point-value of interest # [NB: the prior on delta was a standard normal] prior_delta_null &lt;- dnorm(0, 0, 1) # compute BF via Savage-Dickey BF_delta_null = posterior_delta_null / prior_delta_null BF_delta_null ## [1] 9.901836e-16 We conclude from this result that the data provide extremely strong evidence against the null model, which assumes that \\(\\delta = 0\\), when compared to an alternative model \\(M_1\\), which assumes that \\(\\delta \\sim \\mathcal{N}(0,1)\\) in the prior. Exercise 11.3: Bayes factors with the Savage-Dickey method Look at the plot below. You see the prior distribution and the posterior distribution over the \\(\\delta\\) parameter in a Bayesian \\(t\\)-test model. We are going to use this plot to determine (roughly) the Bayes factor of two models: the full Bayesian \\(t\\)-test model, and a model nested under this full model which assumes that \\(\\delta = 0\\). Describe in intuitive terms what it means for a Bayesian model to be nested under another model. It is sufficient to neglect the conditions on the priors. Solution A model nested under another model fixes certain parameters to specific values which may take on more than one value in the nesting model. Write down the formula for the Bayes factor in favor of the null model (where \\(\\delta = 0\\)) over the full model using the Savage-Dickey theorem. Solution \\(BF_{01}=\\frac{P(\\delta = 0 \\mid D, M_1)}{P(\\delta = 0 \\mid M_1)}\\). Give a natural language paraphrase of the formula you wrote down above. Solution The Bayes factor in favor of the embedded null model over the embedding model is given by the posterior density at \\(\\delta = 0\\) under the nesting model divided by the prior in the nesting model at \\(\\delta = 0\\). Now look at the plot above. Give your approximate guess of the Bayes factor in favor of the null model in terms of a fraction of whole integers (something like: \\(\\frac{4}{3}\\) or \\(\\frac{27}{120}\\), …). Solution \\(BF_{01} \\approx \\frac{5}{2}\\) (see plot above). Formulate a conclusion to be drawn from this numerical result about the research hypothesis that the mean of the two groups compared here is identical. Write one concise sentence like you would in a research paper. Solution A BF of \\(\\frac{5}{2}\\) is mild evidence in favor of the null model, but conventionally not considered strong enough to be particularly noteworthy. 11.4.1.3 [Excursion:] Calculating the Bayes factor precisely under construction 11.4.2 Encompassing models The Savage-Dickey method can be generalized to also cover interval-valued hypotheses. The previous literature has focused on inequality-based intervals/hypotheses (like \\(\\theta \\ge 0.5\\)) (Klugkist, Kato, and Hoijtink 2005; Wetzels, Grasman, and Wagenmakers 2010; Oh 2014), but the method also applies to ROPE-d hypotheses. The advantage of this method is that we can use samples from the posterior distribution to approximate integrals, which is more robust than having to estimate point-values of posterior density. Following previous work (Klugkist, Kato, and Hoijtink 2005; Wetzels, Grasman, and Wagenmakers 2010; Oh 2014), the main idea is to use so-called encompassing priors. Let \\(\\theta\\) be a single parameter of interest (for simplicity), which can in principle take on any real value. We are interested in the interval-based hypotheses: \\(H_0 \\colon \\theta \\in I_0\\), and \\(H_a \\colon \\theta \\not \\in I_{0}\\) where \\(I_{0}\\) is some possibly half-open interval. An encompassing model \\(M_e\\) has a suitable likelihood function \\(P_{M_e}(D \\mid \\theta, \\omega)\\) (where \\(\\omega\\) is a vector of other parameters besides the parameter \\(\\theta\\) of interest). It also defines a prior \\(P_{M_e}(\\theta, \\omega)\\), which does not already rule out \\(H_{0}\\) or \\(H_{a}\\). Generalizing over the Savage-Dickey approach, we construct two models, one for each hypothesis, both of which are nested under the encompassing model: \\(M_0\\) has prior \\(P_{M_0}(\\theta, \\omega) = P_{M_e}(\\theta, \\omega \\mid \\theta \\in [a;b])\\) \\(M_a\\) has prior \\(P_{M_a}(\\theta, \\omega) = P_{M_e}(\\theta, \\omega \\mid \\theta \\not \\in [a;b])\\) Both \\(M_0\\) and \\(M_a\\) have the same likelihood function as \\(M_e\\), which is why we drop the model index for better readability in the following. Figure 11.7 shows an example of the priors of an encompassing model for two nested models based on a ROPE-d hypothesis testing approach. Figure 11.7: Example of the prior of an encompassing model and the priors of two models nested under it. Theorem 11.2 Fix a Bayesian model \\(M\\) (the encompassing model) with prior \\(P_M(\\theta, \\omega)\\) and likelihood function \\(P_M(D \\mid \\theta, \\omega)\\), where \\(\\theta\\) is the parameter of interest and \\(\\omega\\) is a vector of other (nuisance) parameters. Assume that the priors over \\(\\theta\\) are independent of the nuisance parameters \\(\\omega\\). For an interval-valued hypothesis \\(H_0 \\colon \\theta \\in I_0\\), the Bayes factor in favor of this hypothesis over its negation \\(H_a \\colon \\theta \\not \\in I_0\\) can be expressed as: \\[ \\begin{aligned} \\text {BF}_{01} &amp; = \\frac{\\text{posterior-odds of } H_0}{\\text{prior-odds of } H_0} \\\\ &amp; = \\frac{P_M(\\theta \\in I_0 \\mid D)}{P_M(\\theta \\not \\in I_0 \\mid D)} \\frac{P_M(\\theta \\not \\in I_0)}{P_M(\\theta \\in I_0)} \\end{aligned} \\] Show proof. Proof. TBD   11.4.2.1 Example: 24/7 The Bayes factor using the ROPE-d method to compute the interval-valued hypothesis \\(\\theta = 0.5 \\pm \\epsilon\\) is: # set the scene theta_null &lt;- 0.5 epsilon &lt;- 0.01 # epsilon margin for ROPE upper &lt;- theta_null + epsilon # upper bound of ROPE lower &lt;- theta_null - epsilon # lower bound of ROPE # calculate prior odds of the ROPE-d hypothesis prior_of_hypothesis &lt;- pbeta(upper, 1, 1) - pbeta(lower, 1, 1) prior_odds &lt;- prior_of_hypothesis / (1 - prior_of_hypothesis) # calculate posterior odds of the ROPE-d hypothesis posterior_of_hypothesis &lt;- pbeta(upper, 8, 18) - pbeta(lower, 8, 18) posterior_odds &lt;- posterior_of_hypothesis / (1 - posterior_of_hypothesis) # calculate Bayes factor bf_ROPEd_hypothesis &lt;- posterior_odds / prior_odds bf_ROPEd_hypothesis ## [1] 0.5133012 This is unnoteworthy evidence in favor of the alternative hypothesis (Bayes factor \\(\\text{BF}_{10} \\approx 1.95\\)). Notice that the reason why the alternative hypothesis does not fare better in this analysis is because it also includes a lot of parameter values (\\(\\theta &gt; 0.5\\)) which explain the observed data even more poorly than the values included in the null hypothesis. We can also use this approach to test the directional hypothesis that \\(\\theta &lt; 0.5\\). # calculate prior odds of the ROPE-d hypothesis # [trivial in the case at hand, but just to be explicit] prior_of_hypothesis &lt;- pbeta(0.5, 1, 1) prior_odds &lt;- prior_of_hypothesis / (1 - prior_of_hypothesis) # calculate posterior odds of the ROPE-d hypothesis posterior_of_hypothesis &lt;- pbeta(0.5, 8, 18) posterior_odds &lt;- posterior_of_hypothesis / (1 - posterior_of_hypothesis) # calculate Bayes factor bf_directional_hypothesis &lt;- posterior_odds / prior_odds bf_directional_hypothesis ## [1] 45.20512 Here we should conclude that the data provide substantial evidence in favor of the assumption that the coin is biased towards tails, when compared against the alternative assumption that it is biased towards heads. If the dichotomy is “heads bias vs tails bias” the data clearly tilts our beliefs towards the “tails bias” possibility. 11.4.2.2 Example: Simon task Using posterior samples, we can also do similar calculations for the Simon task. Let’s first approximate the Bayes factor in favor of the ROPE-d hypothesis \\(\\delta = 0 \\pm 0.1\\) when compared against the alternative hypothesis \\(\\delta \\not \\in 0 \\pm 0.1\\). # estimating the BF for ROPE-d hypothesis with encompassing priors delta_null &lt;- 0 epsilon &lt;- 0.1 # epsilon margin for ROPE upper &lt;- delta_null + epsilon # upper bound of ROPE lower &lt;- delta_null - epsilon # lower bound of ROPE # calculate prior odds of the ROPE-d hypothesis prior_of_hypothesis &lt;- pnorm(upper, 0, 1) - pnorm(lower, 0, 1) prior_odds &lt;- prior_of_hypothesis / (1 - prior_of_hypothesis) # calculate posterior odds of the ROPE-d hypothesis posterior_of_hypothesis &lt;- mean( lower &lt;= delta_samples &amp; delta_samples &lt;= upper ) posterior_odds &lt;- posterior_of_hypothesis / (1 - posterior_of_hypothesis) # calculate Bayes factor bf_ROPEd_hypothesis &lt;- posterior_odds / prior_odds bf_ROPEd_hypothesis ## [1] 0 This is overwhelming evidence against the ROPE-d hypothesis that \\(\\delta = 0 \\pm 0.1\\). We can also use this approach to test the directional hypothesis that \\(\\delta &gt; 0.5\\). # calculate prior odds of the ROPE-d hypothesis # [trivial in the case at hand, but just to be explicit] prior_of_hypothesis &lt;- 1 - pnorm(0, 0, 1) prior_odds &lt;- prior_of_hypothesis / (1 - prior_of_hypothesis) # calculate posterior odds of the ROPE-d hypothesis posterior_of_hypothesis &lt;- mean( delta_samples &gt;= 0.5 ) posterior_odds &lt;- posterior_of_hypothesis / (1 - posterior_of_hypothesis) # calculate Bayes factor bf_directional_hypothesis &lt;- posterior_odds / prior_odds bf_directional_hypothesis ## [1] Inf Modulo imprecision induced by sampling, we see that the evidence in favor of the directional hypothesis \\(\\delta &gt; 0.5\\) is immense. Exercise 11.4: True or False? Decide for the following statements whether they are true or false. An encompassing model for addressing ROPE-d hypotheses needs two competing models nested under it. A Bayes factor of \\(BF_{01} = 20\\) constitutes strong evidence in favor of the alternative hypothesis. A Bayes factor of \\(BF_{10} = 20\\) constitutes minor evidence in favor of the alternative hypothesis. We can compute the BF in favor of the alternative hypothesis with \\(BF_{10} = \\frac{1}{BF_{01}}\\). Solution Statements a. and d. are correct. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
