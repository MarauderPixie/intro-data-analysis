# (PART) Applied (generalized) linear modeling {-}

# Linear regression {#Chap-04-01-simple-linear-regression}

<hr>

This chapter introduces the basics of linear regression modeling. 
It covers ordinary least-squares (OLS) regression, a maximum-likelihood approach, and finally a Bayesian approach.
Impatient readers may wish to skip to the Bayesian analyses directly, but understanding the OLS and MLE approaches helps to see the bigger (historical) picture and also helps appreciating some of the formal results pertaining to the Bayesian analysis. 

All concrete calculations in this chapter are based on the same running example using the [murder data set](app-93-data-sets-murder-data). 

Based on a different example, the following video gives an overview of the main ideas behind a Bayesian approach to simple linear regression.

<iframe src="https://vimeo.com/414228236" width="640" display="block" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>


```{block, type='infobox'}
The learning goals for this chapter are:

- understand what a linear regression model is
- see the conceptual differences between OLS, MLE and Bayesian approaches to linear regression
- be able to find best fitting values for OLS and MLE regression using R's built-in functions
- be able to sample *a posteriori* credible values for a regression model using the non-informative standard model 
```

## Ordinary least squares regression

This section introduces ordinary least squares (OLS) linear regression. The main idea is that we look for the best-fitting line in a (multi-dimensional) cloud of points, where "best-fitting" is defined in terms of a geometrical measure of distance (squared prediction error).

### Prediction without any further information

We are interested in explaining or predicting the murder rates in a city using the [murder data set](app-93-data-sets-murder-data).
Concretely, we are interested in whether knowing a city's unemployment rate (stored in variable `unemployment`) helps make better predictions for that city's murder rate (stored in variable `murder_rate`).

Let's first plot the murder rate for every city (just numbered consecutively in order of their appearance in the data set):

```{r, echo = F}
murder_data <- read_csv('data_sets/murder_rates.csv')

murder_data %>%
  ggplot(aes(x = 1:20, y = murder_rate)) +
  geom_point() +
  labs(
    x = "n-th city",
    y = "murder rate"
  ) 
```

Suppose we know the vector $y$ of all observed murder rates but we don't know which murder rate belongs to which city.
We are given a city to guess its murder rate.
But we cannot tell cities apart.
So we must guess one number as a prediction for any of the cities. 
What's a good guess?

Actually, how good a guess is depends on what we want to do with this guess (the utility function of a decision problem).
For now, let's just assume that we have a measure of **prediction error** which we would like to minimize with our guesses.
A common measure of **prediction error** uses intuitions about geometric distance and is defined in terms of the **total sum of squares**, where $y$ is the $n$-dimensional vector of observed murder rates and $\xi
in \mathbb{R}$ is a single numeric prediction:

$$
\text{TSS}(\xi) = \sum_{i=1}^n (y_i - \xi)^2
$$

This measure of prediction error is what underlies the ordinary least squares approach to  regression.

It turns out that the **best prediction** we can make, i.e., the number $\hat{\xi} = \arg \min_{\xi} \text{TSS}(\xi)$ for which TSS is minimized, is the mean $\bar{y}$ of the original predictions.
So, given the goal of minimizing TSS, our best guess is the mean of the observed murder rates.

<div class = "exercises">

```{proposition, "TSS-solution-mean", name = "Mean minimizes total sum of squares."}

$$
\arg \min_{\xi} \sum_{i=1}^n (y_i - \xi)^2 = \frac{1}{n} \sum_{i=1}^n y_i = \bar{y}
$$ 

```

<div class="collapsibleSolution">
<button class="trigger">Show proof.</button>
<div class="content">

```{proof}

To find a minimum, consider the first derivative of TSS(\xi) and find its zero points:

$$
\begin{align*}
& f(\xi)   = \sum_{i=1}^n (y_i - \xi)^2 = \sum_{i=1}^n (y_i^2 - 2 y_i \xi + \xi^2) \\
& f'(\xi)  = \sum_{i=1}^n (-2y_i + 2\xi) = 0 \\
\Leftrightarrow &  \sum_{i=1}^n -2y_i  = -2 n \xi \\
\Leftrightarrow &  \xi = \frac{1}{n} \sum_{i=1}^n y_i  = \bar{y} \\
\end{align*}
$$

Indeed, the zero point $\xi = \bar{y}$ is a minimum because its second derivative is positive:

$$
f''(\bar{y}) = 2
$$

```

&nbsp;

</div>
</div>
</div>


The plot below visualizes the prediction we make based on the naive predictor $\hat{y}$.
The black dots show the data points, the red line shows the prediction we make (the mean murder rate), the small hollow dots show the specific predictions for each observed value and the gray lines show the distance between our prediction and the actual data observation.

```{r echo = F}
mean_y <- murder_data %>% pull(murder_rate) %>% mean()
murder_data %>% 
  ggplot(aes(x = 1:20, y = murder_rate)) +
  geom_segment(
    aes(
      x = 1:20, 
      y = murder_rate,
      xend = 1:20, 
      yend = mean_y
    ),
    color = "lightgray"
  ) +
  geom_abline(slope = 0, intercept = mean_y, color = "firebrick") +
  geom_point(aes(y = mean_y), shape = "O", alpha = 0.5) +
  geom_point() +
  labs(
    x = "n-th city",
    y = "murder rate"
  ) 
```

To obtain the TSS for the prediction shown in the plot above, we would need to take each gray line, measure it's distance, square this number and sum over all lines (cities).
In the case at hand, the prediction error we make by assuming just the mean as predictor is:

```{r}
y <- murder_data %>% pull(murder_rate)
n <- length(y)
tss_simple <- sum((y - mean(y))^2)
tss_simple
```

At this stage, a question might arise:
Why square the distances to obtain the total sum of, well, *squares*?
One intuitive motivation is that we want small deviations from our prediction to have less overall impact than huge deviations.
A technical motivation is that the best solution to OLS estimation corresponds to the best solution under a maximum likelihood approach, if we use a normal distribution as likelihood function.
This is what we will cover in Section \@ref(Chap-04-01-linear-regression-MLE) after having introduced the regression model in full.

### Prediction with knowledge of unemployment rate

We might not be very content with this prediction error. Suppose we could use some piece of information about the random city whose murder rate we are trying to predict. E.g., we might happen to know the value of the variable `unemployment`. How could that help us make a better prediction?

There does seem to be some useful information in the unemployment rate, which may lead to better predictions of the murder rate. We see this in a scatter plot:

```{r, echo = F}
murder_data %>% 
  ggplot(aes(x = unemployment, y = murder_rate)) +
  geom_point() +
  labs(y = "murder rate")
```


Let us assume, for the sake of current illustration, that we expect a very particular functional relationship between the variables `murder_rate` and `unemployment`. For some reason or other, we hypothesize that even with 0% unemployment, the murder rate would be positive, namely at 4 murders per million inhabitants. We further hypothesize that with each increase of 1% in the unemployment percentage, the murder rate per million increases by 2. The functional relationship between dependent variable $y$ (= murder rate) and predictor variable $x$ (= unemployment) can then be expressed as a linear function of the following form, where $\xi \in \mathbb{R}^n$ is now a vector of $n$ predictions (one prediction $\xi_i$ for each data observation $y_i$):^[The predictor vector for  linear regression is often written as $\hat{y}$. The notation in terms of a linear predictor $\xi$ is useful for later extensions to generalized linear regression models.]

$$
\xi_i = 2x_i + 4
$$

Here is a graphical representation of this particular functional relationship assumed in the equation above. Again, the black dots show the data points, the red line the linear function $f(x) = 2x +4$, the small hollow dots show the specific predictions for each observed value $x_i$ and the gray lines show the distance between our prediction $\xi_i$ and the actual data observation $y_i$. (Notice that there are data points for which the unemployment rate is the same, but we observed different murder rates.)

```{r, echo = F}
murder_data %>% 
  ggplot(aes(x = unemployment, y = murder_rate)) +
    geom_segment(
    aes(
      x = unemployment, 
      y = murder_rate,
      xend = unemployment, 
      yend = 2*unemployment + 4
    ),
    color = "lightgray"
  ) +
  geom_point() +
  geom_abline(slope = 2, intercept = 4, color = "firebrick") +
  geom_point(aes(y = 2*unemployment + 4), shape = "O", alpha = 0.5) +
  labs(y = "murder rate")
```

We can again quantify our prediction error in terms of a sum of squares like we did before. For the case of a prediction vector $\xi$, the quantity in question is called the **residual sum of squares**.

$$
\text{RSS} = \sum_{i=1}^n (y_i - \xi_i)^2
$$

Here is how we can calculate RSS in R for the particular vector $\xi \ in \mathbb{R}^n$ for which $\xi_{i} = 2x_i + 4$:

```{r}
y <- murder_data %>% pull(murder_rate)
x <- murder_data %>% pull(unemployment)
predicted_y <- 2 * x + 4
n <- length(y)
rss_guesswork <- sum((y - predicted_y)^2)
rss_guesswork
```

Compared to the previous prediction, which was based on the mean $\bar{y}$ only, this linear function reduces the prediction error (measured here geometrically in terms of a sum of squares).
This alone could be taken as *prima facie* evidence that knowledge of `unemployment` helps make better predictions about `murder_rate`.


<div class = "exercises">
**Exercise 13.1 [optional]**

a. Compare RSS and TSS. How / where exactly do these notions differ from each other? Think about which information the difference between the two measures conveys.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

TSS computes the distance between a data point and the overall mean of all data points, whereas RSS computes the distance between a data point and a predictor value specific to this data point. 
The difference between RSS and TSS tells us how good our prediction is in comparison to a naive prediction (using just the mean). 

</div>
</div>

b. Is it possible for TSS to be smaller than RSS? 
That is, could the error based on a single numeric prediction for all data points be smaller than an error obtained for a linear predictor that has a single prediction for each data point?  

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

Yes, that's possible.
The definition of RSS and TSS does not imply that we look at the *optimal* point-valued or linear predictor.
It is conceivable to choose a good single number and a very bad linear predictor, so that RSS is smaller than TSS.

</div>
</div>

</div>

### Linear regression: general problem formulation

Suppose we have $k$ predictor variables $x_1, \dots , x_k$ and a dependent variable $y$. 
We consider the linear relation: 

$$ \xi_i({\beta}_0, {\beta}_1, \dots, {\beta}_k) = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} $$
Often we do not explicitly write $\xi$ as a function of the parameters $\beta_0, \dots \beta_k$, and write instead:

$$ \xi_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_k x_{ki} $$
The parameters $\beta_0, \beta_1, \dots, \beta_k$ are called **(regression) coefficients**. 
In particular, $\beta_0$ is called the **(regression) intercept** and $\beta_1, \dots, \beta_k$ are **(regression) slope coefficients**. 

The term **simple linear regression** is often used to cover the special case of $k=1$.
If there is more than one predictor, i.e., $k \ge 1$, the term **multiple linear regression** is common.

Based on the predictions of a parameter vector $\langle {\beta}_0, {\beta}_1, \dots, {\beta}_k\rangle$, we consider the residual sum of squares as a measure of prediction error:

$$\text{RSS}_{\langle {\beta}_0, {\beta}_1, \dots, {\beta}_k\rangle} = \sum_{i = 1}^k [y_i - \xi_i ({\beta}_0, {\beta}_1, \dots, {\beta}_k) ]^2 $$

We would like to find the *best parameter values* (denoted traditionally by a hat on the parameter's variable: $\hat{\beta}_i$) in the sense of minimizing the residual sum of squares:

$$
\langle \hat{\beta}_0, \hat{\beta}_1, \dots  , \hat{\beta}_k\rangle = \arg \min_{\langle \beta_0, \beta_1, \dots, \beta_k\rangle} \text{RSS}_{\langle {\beta}_0, {\beta}_1, \dots, {\beta}_k\rangle}
$$

The prediction corresponding to the best parameter values is denoted by $\hat{\xi} \in \mathbb{R}^n$ and called the *best linear predictor*:

$$ \hat{\xi}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1i} + \dots + \hat{\beta}_k x_{ki}$$

It is also possible, and often convenient, to state the linear regression model in terms of matrix operations.
Traditionally, we consider a so-called **predictor matrix** $X$ of size $n \times (k+1)$), where $n$ is the number of observations in the data set and $k$ is the number of predictor variables.
The predictor matrix includes the values for all predictor variables and it also includes an "intercept column" $(X^{T})_0$ for which $X_{i0}=1$ for all $1 \le i \le n$ so that the intercept $\beta_0$ can be treated on a par with the other regression coefficients.^[You can conveniently think of the predictor matrix as just the data you want to analyze in a tidy tibble format, with the columns giving the variables of interest, only that you have removed the column with the dependent variable $y$ and that you have instead added the "intercept column" containing the entry 1 in each row.]

Using the predictor matrix $X$, the linear predictor vector $\xi$ is:

$$\xi = X \beta$$

<div class = "exercises">
**Exercise 13.2**

How can we interpret the parameters $a$ and $b$ of the linear model $\xi_i = a x_i + b$?
How are these parameters usually called in regression jargon?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

Parameter $a$ is the slope, $b$ the intercept of a simple linear regression.
Parameter $a$ gives the amount of change of $y$ for each unit change in $x$. 
Parameter $b$ gives the the prediction xi for  $x=0$.

</div>
</div>
</div>


### Finding the OLS-solution

In the above example, where we regressed `murder_rate` against `unemployment`, the model has two regression coefficients: an intercept term and a slope for `unemployment`.
The optimal solution for these delivers the regression line in the graph below.

```{r, echo = F}
lm_fit_murder <- lm(murder_rate ~ unemployment, data = murder_data)
intercept_simple_murder <- lm_fit_murder$coef[1]
slope_simple_murder <- lm_fit_murder$coef[2]
murder_data %>% 
  ggplot(aes(x = unemployment, y = murder_rate)) +
    geom_segment(
    aes(
      x = unemployment, 
      y = murder_rate,
      xend = unemployment, 
      yend = slope_simple_murder * unemployment + intercept_simple_murder
    ),
    color = "lightgray"
  ) +
  geom_point() +
  geom_abline(slope = slope_simple_murder, intercept = intercept_simple_murder, color = "firebrick") +
  geom_point(aes(y = slope_simple_murder * unemployment + intercept_simple_murder), shape = "O", alpha = 0.5) +
  labs(y = "murder rate")
```

The total sum of squares for the best fitting parameters is:

```{r, echo = F}
y <- murder_data %>% pull(murder_rate)
x <- murder_data %>% pull(unemployment)
predicted_y <- slope_simple_murder * x + intercept_simple_murder
n <- length(y)
tss_best <- sum((y - predicted_y)^2)
tss_best
```

This is the best prediction we can make based on a linear predictor.
In the following, we discuss several methods of finding the best-fitting values for regression coefficients that minimize the residual sum of squares.

#### Finding optimal parameters with `optim`

We can use the `optim` function to find the best-fitting parameter values for our simple linear regression example.

```{r, message = T}
# data to be explained / predicted
y <- murder_data %>% pull(murder_rate)
# data to use for prediction / explanation
x <- murder_data %>% pull(unemployment)
# function to calculate residual sum of squares
get_rss = function(y, x, beta_0, beta_1) {
  yPred = beta_0 + x * beta_1
  sum((y-yPred)^2) 
}
# finding best-fitting values for RSS
fit_rss = optim(par = c(0, 1),  # initial parameter values
  fn = function(par) {  # function to minimize
    get_rss(y, x, par[1], par[2])
  }
)
# output the results
message(
  "Best fitting parameter values:",
  "\n\tIntercept: ", fit_rss$par[1] %>% round(2),
  "\n\tSlope: ", fit_rss$par[2] %>%  round(2),
  "\nRSS for best fit: ", fit_rss$value %>% round(2)
)
```

#### Fitting OLS regression lines with `lm`

R also has a built-in function `lm` which fits linear regression models via RSS minimization. Here is how you call this function for the running example:

```{r}
# fit an OLS regression
fit_lm <- lm(
  # the formula argument specifies dependent and independent variables
  formula = murder_rate ~ unemployment,
  # we also need to say where the data (columns) should come from
  data = murder_data
)
# output the fitted object
fit_lm
```

The output of the fitted object shows the best-fitting values (compare them to what we obtained before).^[The fitted object `fit_lm` also contains additional information, to be inspected with `summary(fit_lm)`. We skip these details here because the current focus is on applied *Bayesian* analyses.]

#### Finding optimal parameter values with math

It is also possible to determine the OLS-fits by a mathematical derivation. We start with the case of a simple linear regression with just one predictor variable.

<div class = "exercises">

```{theorem, "OLS-Solution", name = "OLS solution for simple linear regression"}
For a simple linear regression model with just one predictor for a data set with $n$ observations, the solution for:

$$\arg \min_{\langle \beta_0, \beta_1\rangle} \sum_{i = 1}^n (y_i - (\beta_0 + \beta_1 x_{i}))^2$$
  
is given by:
  
$$
\begin{aligned}
\hat{\beta_1} &= \frac{Cov(x,y)}{Var(x)} & 
\hat{\beta_0} &= \bar{y} - \hat{\beta}_1 \bar{x} 
\end{aligned}
$$

```


<div class="collapsibleSolution">
<button class="trigger">Show proof.</button>
<div class="content">

```{proof}
*[See e.g., @olive2017, pp. 57-59]*

Given a set of $n$ observations $(x_i, y_i)$, we want to find:

$$\langle \hat{\beta}_0, \hat{\beta}_1 \rangle = \arg \min_{\langle \beta_0, \beta_1 \rangle} \sum_{i = 1}^n (y_i - (\beta_0 + \beta_1 x_{i}))^2$$

Let $Q$ denote the RSS function. We want to find the minima of $Q$.
So, we want to find the values $\hat\beta_0$ and $\hat\beta_1$ for which $\frac{\partial Q}{\partial \hat\beta_0}=0$ and $\frac{\partial Q}{\partial \hat\beta_1}=0$, since all partial derivatives equal to 0 at the global minimum.

The first condition is:

$$ \begin{align} \frac{\partial Q}{\partial \hat\beta_0}=\sum_{i=1}^{n}-2(y_i-\hat\beta_0-\hat\beta_1x_i)&= 0\\
&=-\sum_{i=1}^ny_i+\sum_{i=1}^n\hat \beta_0+\sum_{i=1}^n\hat\beta_1x_i\\
&=-\sum_{i=1}^ny_i+n\hat\beta_0+\sum_{i=1}^n\hat\beta_1x_i
\end{align}$$

Ff we solve for $\hat\beta_0$, this becomes:

$$\begin{align}
\hat\beta_0&=\frac{1}{n}\sum_{i=1}^{n}y_i-\frac{1}{n}\hat\beta_1\sum_{i=1}^{n}x_i\\
&=\bar y - \hat\beta_1\bar x
\end{align}$$

This solution is indeed a minimum as the second partial derivative is positive:

$\frac{\partial^2 Q}{\partial\hat\beta_0^2}=n>0$

The second condition is:

$$ \begin{align}
\frac{\partial Q}{\partial \hat\beta_1}& =\sum_{i=1}^{n}-2x_i(y_i-\hat\beta_0-\hat\beta_1x_i)=0\\
&=\sum_{i=1}^{n}(-x_iy_i+\hat\beta_0x_i+\hat\beta_1x_i^2)\\
&=-\sum_{i=1}^{n}x_iy_i+\hat\beta_0\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2
\end{align}$$

Substitution of $\hat\beta_0$ by (1.1.5) yields:

$$ \begin{align}
0&=-\sum_{i=1}^{n}x_iy_i+(\bar y - \hat\beta_1\bar x)\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2\\
&=-\sum_{i=1}^{n}x_iy_i+\bar y\sum_{i=1}^{n}x_i-\hat\beta_1\bar x\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2
\end{align}$$

Separating into two sums:

$$ \sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)-\hat\beta_1\sum_{i=1}^{n}\left(x_i^2-x_i\bar x\right)=0 \tag{1.1.9}$$

So that:

$$ \hat\beta_1 = \frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)} = \frac{\sum_{i=1}^{n}\left( x_iy_i\right)-n\bar x\bar y}{\sum_{i=1}^{n}\left( x_i^2\right)-n \bar x^2} \tag{1.1.10}$$

Thus:

$$ \sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)=0$$

And:

$$ \sum_{i=1}^{n}\left(\bar x \bar y - y_i \bar x\right)=0$$

This can be used in order to expand the previous term and finally to rewrite $\hat\beta_1$ as the ratio of $Cov(x,y)$ to $Var(x)$:

$$
\begin{align}
\hat\beta_1&=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+\sum_{i=1}^{n}\left(\bar x\bar y - y_i \bar x\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+\sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)}=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+0}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+0}\\
\\
&=\frac{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right) \left(y_i- \bar y \right)}{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right)^2}\\
\\
&=\frac{Cov(x,y)}{Var(x)}
\end{align}$$

The solution is indeed a minimum as the second partial derivative is positive:

$$\frac{\partial^2Q}{\partial \hat\beta_1^2}= 2 \sum_{i=1}^{n}x_i^2 >0$$
```

&nbsp;


</div>
</div>
</div>


Let's use these formulas to calculate regression coefficients for the running example as well:

```{r}
tibble(
  beta_1 = cov(x,y) / var(x),
  beta_0 = mean(y) - beta_1 * mean(x)
)
```

A similar result also exists for regression with more than one predictor variable, so-called **multiple linear regression**.

<div class = "exercises">

```{theorem, "OLS-Solution-general", name = "OLS general"}
Let $X$ be the $n \times (k+1)$ regression matrix for a linear regression model with $k$ predictor variables for a data set $y$ with $n$ observations. The solution for OLS regression

$$
\hat{\beta} = \langle \hat{\beta}_0, \hat{\beta}_1, \dots  , \hat{\beta}_k\rangle = \arg \min_{\beta} \sum_{i = 1}^k (y_i - (X \beta)_i)^2
$$
  
is given by:
  
$$
\hat{\beta} = (X^T \ X)^{-1}\ X^Ty
$$

```

<div class="collapsibleSolution">
<button class="trigger">Show proof.</button>
<div class="content">

```{proof}

With $n$ observations, the vector $\xi$ of predicted values for given coefficient vector $\beta$ is:

$$
\xi=X \beta
$$

More explicitly, this means that:

$$
\begin{align*}
\xi_1&=\beta_0 + \beta_{1} X_{11}+\beta_2 X_{12} + \ldots + \beta_k X_{1k}\\
\xi_2&=\beta_0 + \beta_{1} X_{21}+\beta_2 X_{22} + \ldots + \beta_k X_{2k}\\
\ldots\\
\xi_n&=\beta_0 + \beta_{1} X_{n1}+\beta_2 X_{n2}+ \ldots + \beta_k X_{nk}
\end{align*}
$$

The OLS estimator is obtained (like in the special case of simple linear regression) by minimizing the residual sum of squares (RSS).
The RSS for the multiple linear regression model is

$$
Q=\sum_{i=1}^n \left(y_i-\beta_0 - \beta_1 X_{i1}- \beta_2 X_{i2}-...-\beta_k X_{ik}\right)^2
$$

To find the minimum of $Q$ we calculate the first partial derivative of $Q$ for each $\beta_j$:

$$\begin{align}
\frac{\partial Q}{\partial\beta_0}&=2\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 X_{i1}-\beta_2 X_{i2}- \ldots -\beta_k X_{ik}\right)(-1)\\
\\
\frac{\partial Q}{\partial\beta_1}&=2\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 X_{i1}-\beta_2 X_{i2}- \ldots -\beta_k X_{ik}\right)(- X_{i1})\\
\\
\frac{\partial Q}{\partial\beta_2}&=2\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 X_{i1}-\beta_2 X_{i2}- \ldots -\beta_k X_{ik}\right)(- X_{i2})\\
 \ldots \\
\frac{\partial Q}{\partial\beta_k}&=2\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 X_{i1}-\beta_2 X_{i2}- \ldots -\beta_k X_{ik}\right)(- X_{ik})
\end{align}$$

For the minimium $\hat{\beta}$ the derivative of each equation must be zero:

$$\begin{align}
&\sum_{i=1}^n\left(y_i-\hat\beta_0-\hat\beta_1 X_{i1}-\hat\beta_2 X_{i2}- \ldots -\hat\beta_k X_{ik}\right) = 0\\
&\sum_{i=1}^n\left(y_i-\hat\beta_0-\hat\beta_1 X_{i1}-\hat\beta_2 X_{i2}- \ldots -\hat\beta_k X_{ik}\right) X_{i1} = 0\\
&\sum_{i=1}^n\left(y_i-\hat\beta_0-\hat\beta_1 X_{i1}-\hat\beta_2 X_{i2}- \ldots -\hat\beta_k X_{ik}\right) X_{i2} = 0\\
& \ldots \\
&\sum_{i=1}^n\left(y_i-\hat\beta_0-\hat\beta_1 X_{i1}-\hat\beta_2 X_{i2}- \ldots -\hat\beta_k X_{ik}\right) X_{ik} = 0
\end{align}$$

Alternatively, we can use matrix notation and combine the above equations into the following form:

$$X^Ty-X^TX\hat\beta=0$$

Rearranging this, the following expression is known as **normal equations**:

$$X^TX\hat\beta=X^Ty$$

Just for illustration, the system of normal equations in expanded matrix notation is:

$$
\begin{bmatrix} 
n & \sum_{i=1}^n X_{i1} & ... & \sum_{i=1}^n X_{ik}\\
\sum_{i=1}^n X_{i1} & \sum_{i=1}^n X_{i1}^2 & ... & \sum_{i=1}^n X_{i1} X_{ik}\\... & ... & ... & ...\\
\sum_{i=1}^n X_{ik} & \sum_{i=1}^n X_{ik} X_{i1} & ... & \sum_{i=1}^n X_{ik}^2
\end{bmatrix}
\begin{bmatrix}
\hat\beta_0 \\
\hat\beta_1 \\
\ldots \\
\hat\beta_k
\end{bmatrix}
=
\begin{bmatrix}
\sum_{i=1}^ny_i\\\sum_{i=1}^n X_{i1}y_i \\
\ldots \\
\sum_{i=1}^nX_{ik}y_i
\end{bmatrix}
$$

The estimator $\hat\beta$ can be obtained by rearranging again:

$$
\hat{\beta} = (X^T \ X)^{-1}\ X^Ty
$$

Finally, to see that $\hat\beta$ is indeed a global minimizer of the OLS criterion, we check that the second order condition is always a semidefinite positive matrix (details omitted here):

$$\frac{\partial^2 Q}{\partial \mathbf{\hat\beta}^2}=2X'X >0.$$

```

&nbsp;

</div>
</div>
</div>



The availability of these elegant mathematical solutions for OLS-regression explains why the computation of best-fitting regression coefficients with a built-in function like `lm` is lightning fast: it does not rely on optimization with `optim`, sampling methods or other similar computational approaches. Instead, it instantaneously calculates the analytical solution.



## A maximum-likelihood approach {#Chap-04-01-linear-regression-MLE}

In order to be able to extend regression modeling to predictor variables other than metric variables (so-called generalized linear regression models, see Chapter \@ref(Chap-04-04-GLM)), the geometric approach needs to be abandoned in favor of a likelihood-based approach. The likelihood-based approach tries to find coefficients that explain the observed data most plausibly. 

<!-- TODO -->
<!-- https://medium.com/quick-code/maximum-likelihood-estimation-for-regression-65f9c99f815d -->
<!-- Include the second figure if allowed by the author. Caption: In MLE approach, each predicted $y$ value decides mean of the normal distribution. Since the likelihood $f(y_i|\theta)$ is inversely proportional to the residual $\epsilon_i$, $\theta$ should be chosen such that sum of $\epsilon_i$ is minimised. -->

### A likelihood-based model

There are two equivalent formulations of a (simple) linear regression model using a likelihood-based approach. The first is more explicit, showing clearly that the model assumes that for each observation $y_i$ there is an error term $\epsilon_i$, which is an iid sample from a Normal distribution. (Notice that the likelihood-based model assumes an additional parameter $\sigma$, the standard deviation of the error terms.)

$$ 
\text{likelihood-based regression }
\text{[explicit version]}
$$

$$
\begin{aligned}
\xi & = X \beta  \\
y_i & = \xi + \epsilon_i \\
\epsilon_i & \sim \text{Normal}(0, \sigma)  \\
\end{aligned}
$$

The second, equivalent version of this writes this more compactly, suppressing the explicit mentioning of iid error terms:

$$ 
\text{likelihood-based regression }
\text{[compact version]}
$$

$$
\begin{aligned}
y_i & \sim \text{Normal}((X \beta)_i, \sigma)
\end{aligned}
$$

### Finding the MLE-solution with `optim`

We can use `optim` to find maximum likelihood estimates for the simple linear regression of `murder_rates` predicted in terms of `unemployment` like so:

```{r, message = T}
# data to be explained / predicted
y <- murder_data %>% pull(murder_rate)
# data to use for prediction / explanation
x <- murder_data %>% pull(unemployment)
# function to calculate negative log-likelihood
get_nll = function(y, x, beta_0, beta_1, sd) {
  if (sd <= 0) {return( Inf )}
  yPred = beta_0 + x * beta_1
  nll = -dnorm(y, mean=yPred, sd=sd, log = T)
  sum(nll)
}
# finding MLE
fit_lh = optim(par = c(0, 1, 1), 
  fn = function(par) {
    get_nll(y, x, par[1], par[2], par[3])
  }
)
# output the results
message(
  "Best fitting parameter values:",
  "\n\tIntercept: ", fit_lh$par[1] %>% round(2),
  "\n\tSlope: ", fit_lh$par[2] %>%  round(2),
  "\nNegative log-likelihood for best fit: ", fit_lh$value %>% round(2)
)

```
### Finding the MLE-solution with `glm`

R also has a built-in way of approaching simple linear regression with a maximum-likelihood approach, namely by using the function `glm` (generalized linear model). 
Notice that the output looks slightly different from that of `lm`.

```{r}
fit_glm <- glm(murder_rate ~ unemployment, data = murder_data)
fit_glm
```

### Finding the MLE-solution with math

It is no coincidence that these fitted values are (modulo number imprecision) the same as for the geometric OLS approach.

<div class = "exercises">

```{theorem, "MLE-Solution", name = "MLE solution"}
The vector $\hat{\beta} \in \mathbb{R}^k$ maximizing the likelihood of a linear regression model with $k$ predictors is the same as the vector that minimizes the residua sum of squares, namely:

$$
\arg \max_{\beta} \prod_{i = 1}^n \text{Normal}(y_i \mid \mu = (X \beta)_i, \sigma) = (X^T X)^{-1} X^Ty
$$
```

<div class="collapsibleSolution">
<button class="trigger">Show proof.</button>
<div class="content">

```{proof}

Using the more explicit formulation of likelihood-based regression, we can rewrite the likelihood function in terms of the probability of "sampling" error terms $\epsilon_i$ for each $y_i$ in such a way that $\epsilon_i = y_i - \xi_i = y_i - (X \beta)_i$:

$$
\begin{align*}
LH(\beta) & = \prod_{i = 1}^n \text{Normal}(\epsilon_i \mid \mu = 0, \sigma) \\
& = \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon_i^2}{\sigma^2}\right)}\right] & \text{[by def. of normal distr.]}
\end{align*}
$$

Since we are only interested in the maximum of this function, we can also look for the maximum of $\log LH(\beta)$ because the logarithm is a strictly monotone increasing function. This is is useful because the logarithm can then be rewritten as a sum.

$$
\begin{align}
LLH(\beta)&=\log \left(LH(\beta)\right)\\
&=-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2
\tag{2.7}
\end{align}
$$

Since only the last summand depends on $\beta$, and since we can drop the factor $frac{1}{2}\sigma^2$ for finding a maximum, we obtain:

$$
\arg \max_\beta LLH(\beta) = - \sum_{i=1}^n(\epsilon_i)^2
$$

If we substitute $\epsilon_i$ and multiply with $-1$ to find the minimum, we see that we are back at the original problem of finding the OLS solution:

$$
\arg \min_\beta -LLH(\beta) = \sum_{i=1}^n(y_i - (X \beta)_i)^2
$$

Notice that this result holds independently of $\sigma$, which just cancelled out in this derivation.

&nbsp;

```

</div>
</div>
</div>

<div class = "exercises">
**Exercise 13.3**

Let's assume that following the MLE approach, we obtained $\beta_0 = 1$, $\beta_1 = 2$ and $\sigma = 0.5$. For $x_i = 0$, which $\xi_i$ value will maximize the likelihood?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
Since $y_i \sim \text{Normal} (\beta_0 + \beta_1 x_i , \sigma)$, the likelihood is maximized at the mean of the normal distribution, i.e., $y_i = \beta_0 + \beta_1 x_i = 1$.
</div>
</div>
</div>

## A Bayesian approach

The Bayesian approach to linear regression just builds on the likelihood-based approach of the last section, to which it adds priors for the model parameters $\beta$ (a vector of regression coefficients) and $\sigma$ (the standard deviation of the normal distribution).
The next Chapter \@ref(Chap-04-02-Bayes-regression-practice) introduces ways of conveniently sampling from Bayesian regression models with variable specifications of these model priors using the R package `brms`.
This section introduces a Bayesian regression model with non-informative priors which extends the non-informative priors model for inferring the parameters of a normal distribution, which was introduced in section \@ref(ch-03-04-parameter-estimation-normal).

The Bayesian non-informative priors regression model uses the same likelihood function as the likelihood-based model from before and assumes essentially the same non-informative priors as the model from section \@ref(ch-03-04-parameter-estimation-normal).
Concretely, it assumes an (improper) flat distribution over regression coefficients, and it assumes that the variance $\sigma^2$ is log-uniformly distributed, which is equivalent to saying that $\sigma^2$ follows an inverse distribution.
So, for a regression problem with $k$ predictors, predictor matrix $X$ of size $x \times (k+1)$ and dependent data $y$ of size $n$, we have:

$$
\begin{aligned}
\beta_j & \sim \mathrm{Uniform}(-\infty, \infty) \ \ \ \text{for all } 1 \le j \le k \\
\log(\sigma^2)  & \sim \mathrm{Uniform}(-\infty, \infty) \\
y_i & \sim \text{Normal}((X \beta)_i, \sigma)
\end{aligned}
$$

Using this prior, we can calculate a closed-form of the posterior to sample from (for details see @gelman2014 Chap. 14).
The posterior has the general form: 

$$
P(\beta, \sigma^2 \mid y) \propto P(\sigma^2 \mid y) \ P(\beta \mid \sigma^2, y)
$$

Without going into details here, the posterior distribution of the variance $\sigma^2$ is an inverse-$\chi^2$:

$$
\sigma^2 \mid y \sim \text{Inv-}\chi^2(n-k, \color{gray}{\text{ some-complicated-term}})
$$

The posterior of the regression coefficients is a (multivariate) normal distribution:

$$
\beta \mid \sigma^2, y \sim \text{MV-Normal}(\hat{\beta}, \color{gray}{\text{ some-complicated-term}})
$$
What is interesting to note is that the mean of the posterior distribution of regression coefficients is exactly the optimal solution for ordinary least-squares regression and the maximum likelihood estimate:

$$
\hat{\beta} = (X^T \ X)^{-1}\ X^Ty
$$

This is not surprising given that the mean of a normal distribution is also its mode and that the MAP for a non-informative prior coincides with the MLE.

The `aida` package provides a convenience function for sampling from the posterior of the Bayesian non-informative priors model.


```{r}

# variables for regression
y <- murder_data$murder_rate
x <- murder_data$unemployment

# the predictor 'intercept' is just a 
# column vector of ones of the same length as y
int <- rep(1, length(y))

# create predictor matrix with values of all explanatory variables 
#    (here only intercept and slope for MAD)
X <- matrix(c(int, x),  ncol = 2)
colnames(X) <- c("intercept", "slope")


get_samples_regression_noninormative <- function(
  X, # design matrix
  y, # dependent variable
  n_samples = 1000
) {
  
  n <- length(y)
  k <- ncol(X)
  
  # calculating the formula from Gelman et al
  # NB 'solve' computes the inverse of a matrix
  beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y
  V_beta   <- solve(t(X) %*% X)
  
  # 'sample co-variance matrix'
  s_squared <- 1 / (n-k) * t(y - (X %*% beta_hat)) %*% (y - (X %*% beta_hat))
  
  # sample from posterior of variance
  samples_sigma_squared <- extraDistr::rinvchisq(
    n = n_samples, 
    nu = n - k, 
    tau = s_squared
  )
  
  # sample full joint posterior triples
  samples_posterior <- map_df(
    seq(n_samples), 
    function(i) {
      s <-  mvtnorm::rmvnorm(1, beta_hat, V_beta * samples_sigma_squared[i]) 
      tibble(
        intercept = s[1],
        slope = s[2],
        sigma = samples_sigma_squared[i] %>% sqrt()
      )
    }
  )  
  return(samples_posterior)
}

samples_Bayes_regression <- get_samples_regression_noninormative(X,y, 10000)

```

The tibble `samples_Bayes_regression` contains 10,000 samples from the posterior. Let's have a look at the first couple of samples:

```{r}
head(samples_Bayes_regression)
```

Remember that each sample is a triple, one value for each of the model's parameters. 

We can also look at some summary statistics, using the convenience function from the `aida` package:

```{r}
rbind(
  aida::summarize_sample_vector(samples_Bayes_regression$intercept, "intercept"),
  aida::summarize_sample_vector(samples_Bayes_regression$slope, "slope"),
  aida::summarize_sample_vector(samples_Bayes_regression$sigma, "sigma")
)
```


Here's a density plot of the (marginal) posteriors for each parameter value:

```{r}
samples_Bayes_regression %>% 
  pivot_longer(cols = everything(), values_to = "sample", names_to = "parameter") %>% 
  mutate(parameter = factor(parameter, levels = c('intercept', 'slope', 'sigma'))) %>% 
  ggplot(aes(x = sample)) +
    geom_density(fill="lightgray", alpha = 0.5) +
    facet_wrap(~parameter, scales = "free")
```


While these results are approximate, because they are affected by random sampling, they also convey a sense of uncertainty: given the data and the model, how certain should we be that, say, the slope is really at 7.08?
We could, for instance, now test the hypothesis that the factor `unemployment` does not contribute at all to predicting the value of the variable `murder_rate`.
This could be addressed as the point-valued hypothesis that the slope coefficient is exactly equal to zero. 
Since the 95% credible interval clearly excludes this value, we might (based on this binary decision logic) now say that based on the model and the data there is reason to believe that `unemployment` *does* contribute to predicting `murder_rate`, in fact that the higher a city's unemployment rate, the higher we should expect its murder rate to be.
(Remember: this is not a claim about any kind of causal relation!)

## Comparison of approaches

We saw three conceptually different approaches to linear regression: (i) based on ordinary least squares, (ii) based on likelihood alone and (iii) based on Bayesian inference (with non-informative priors).
At the core of linear regression lies the linear predictor $\xi = X \beta$, which all three approaches used.
The three approaches differ in how they determine the regression coefficients $\beta$ that feed this linear predictor.
Another crucial difference is in how these different approaches make predictions about new data observations $y_\text{new}$ given some (hypothetical or actually observed) vector of predictor variables $x_\text{new}$.
Let's go through these differences with some more eye for detail.

The OLS-based approach determined coefficients based on a geometric notion of distance in terms of *squared loss*. 
The prediction of an OLS regression model for a new data set's dependent variables would just be:^[Here, $X_\text{new}$ is the predictor matrix for the new predictor vector $x_\text{new}$.]

$$y_\text{new} = \hat\xi = X_\text{new} \hat \beta$$

In words, The OLS-model predicts that $y_\text{new}$ is given as a point on the best predictor linear regression line.
This is a deterministic, very clear-cut point-valued prediction and almost certainly always false.
The OLS approach, insofar as we have seen it, does not contain a measure of spread around this best predictor.

The MLE-based approach uses a normal distribution to also quantify the likely spread of observations around the best predictor line.
The (posterior) predictions of a trained MLE-based regression model are probabilistic.
They are samples from a normal distribution whose central tendency is the best linear predictor:

$$
\begin{align*}
\hat \xi  & = X_\text{new} \hat \beta \\
y_\text{new} & \sim \text{Normal}(\hat \xi, \hat \sigma)
\end{align*}
$$

Finally, the Bayesian is even more stochastic, so to speak, than the MLE-based approach.
The Bayesian approach does not assume a single best linear predictor vector $\hat{\xi}$ for its (posterior) predictions, but rather gives us a probability distribution over linear predictors.
In vague terms, we could say that Bayesian regression gives us, not a single regression line, but a weighted cloud of (usually: infinitely many) regression lines.
A schematic representation of the posterior predictive for the new data point $y_\text{new}$ given $x_\text{new}$ in Bayesian regression is:

$$
\begin{align*}
\beta_\text{sample}, \sigma_\text{sample} & \sim \text{Bayesian posterior given data} \\
\xi_\text{sample}  & = X_\text{new} \beta_\text{sample} \\ 
y_\text{new} & \sim \text{Normal}(\xi_\text{sample}, \sigma_\text{sample})
\end{align*}
$$






<!-- ### Implementation in `Stan` -->

<!-- Here is an implementation of a Bayesian regression model for the running example murder data using `Stan`: -->

<!-- ```{r, echo = F, eval = F} -->
<!-- # data to be explained / predicted -->
<!-- y <- murder_data %>% pull(murder_rate) -->
<!-- # data to use for prediction / explanation -->
<!-- x <- murder_data %>% pull(unemployment) -->
<!-- y_greta     <- as_data(y) -->
<!-- x_greta     <- as_data(x) -->
<!-- # latent variables and priors -->
<!-- intercept <- student(df= 1, mu = 0, sigma = 10) -->
<!-- slope     <- student(df= 1, mu = 0, sigma = 10) -->
<!-- sigma     <- normal(0, 5, truncation = c(0, Inf)) -->
<!-- # derived latent variable (linear model) -->
<!-- y_pred <- intercept + slope * x_greta -->
<!-- # likelihood  -->
<!-- distribution(y) <- normal(y_pred, sigma) -->
<!-- # finalize model, register which parameters to monitor -->
<!-- murder_model <- model(intercept, slope, sigma) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- murder_data_4_Stan <- list( -->
<!--   N = murder_data %>% nrow(), -->
<!--   x = murder_data %>% pull(unemployment), -->
<!--   y = murder_data %>% pull(murder_rate) -->
<!-- ) -->
<!-- ``` -->

<!-- ```{mystan, eval = F} -->
<!-- data { -->
<!-- int<lower=1> N ; -->
<!-- vector[N] x ; -->
<!-- vector[N] y ; -->
<!-- } -->
<!-- parameters { -->
<!-- real intercept ; -->
<!-- real slope ; -->
<!-- real<lower=0> sigma ; -->
<!-- }  -->
<!-- model { -->
<!-- # priors -->
<!-- intercept ~ student_t(1, 0, 10) ; -->
<!-- slope ~ student_t(1, 0, 10) ; -->
<!-- sigma ~ normal(0, 5) ; -->

<!-- # likelihood -->
<!-- y ~ normal(intercept + slope * x, sigma) ; -->
<!-- } -->
<!-- ``` -->

<!-- <link rel="stylesheet" href="hljs.css"> -->
<!-- <script src="stan.js"></script> -->
<!-- <script>$('pre.mystan code').each(function(i, block) {hljs.highlightBlock(block);});</script> -->

<!-- We can draw samples from the posterior distribution as usual: -->

<!-- ```{r, echo = F, eval = F} -->
<!-- # draw samples -->
<!-- draws_murder_data <- greta::mcmc( -->
<!--   murder_model,  -->
<!--   n_samples = 2000,  -->
<!--   chains = 4,  -->
<!--   warmup = 1000 -->
<!-- ) -->
<!-- # cast results (type 'mcmc.list') into tidy tibble -->
<!-- tidy_draws_murder_data <- ggmcmc::ggs(draws_murder_data) -->
<!-- ``` -->

<!-- ```{r echo = F, eval = F} -->
<!-- draws_murder_data <- readRDS('models_greta/linear_regression_simple_murder_draws.rds') -->
<!-- tidy_draws_murder_data <- ggmcmc::ggs(draws_murder_data) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- stan_fit_linear_regression <- rstan::stan( -->
<!--   # where is the Stan code -->
<!--   file = 'models_stan/simple_linear_regression_model.stan', -->
<!--   # data to supply to the Stan program -->
<!--   data = murder_data_4_Stan, -->
<!--   # how many iterations of MCMC -->
<!--   iter = 3000, -->
<!--   # how many warmup steps -->
<!--   warmup = 500 -->
<!-- ) -->

<!-- # cast results (type 'mcmc.list') into tidy tibble -->
<!-- tidy_draws_murder_data = ggmcmc::ggs(stan_fit_linear_regression) -->
<!-- ``` -->


<!-- Here is a plot of the posterior: -->

<!-- ```{r} -->
<!-- # plot posterior -->
<!-- tidy_draws_murder_data %>%  -->
<!--   ggplot(aes(x = value)) + -->
<!--   geom_density(fill = "lightgray", alpha = 0.5) + -->
<!--   facet_wrap(~ Parameter, scales = "free") -->
<!-- ``` -->




<!-- ## Snippets -->

<!-- ### Preliminaries -->

<!-- <div class = "grey"> -->
<!-- - *data* for simple linear regression: -->
<!--     - single dependent variable $y$ with observed data points $i: y_1,..., y_n$ -->
<!--     - $k$ predictor variables $x_1, ..., x_k$ and observations $x_{j1}, ..., x_{jn}$ for each $x_j$ -->
<!-- - we consider a *regression model*: -->
<!--     - $y_i \sim Normal(\beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}, \sigma)$ -->
<!--     - $\beta_j$ and $\sigma$ are free parameters, as usual -->
<!-- - further notation: -->
<!--     - $\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i$    -   the mean of data observations  -->
<!--     - $\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_1i + ... + \hat{\beta_k} x_{ki}$ - prediction for i-th data point for best fitting parameter values -->
<!-- </div> -->

<!-- --- -->

<!-- ### Definitions -->

<!-- <div class = "grey"> -->
<!-- **Total sum of squares:**  $$TSS = \sum_{i=1}^n (y_i - \bar{y})^2 \tag{0.1}$$ -->
<!-- **Explained sum of squares:**  $$ESS = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2\tag{0.2}$$ -->
<!-- **Residual sum of squares:**  $$RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2\tag{0.3}$$ -->
<!-- **Likelihood:**  $$LH = \prod_{i=1}^n Normal(\beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki}, \sigma)\tag{0.4}$$ -->
<!-- </div> -->

<!-- --- -->

<!-- ### Theorem 1 -->

<!-- #### Theorem 1a - special case: one predictor ($k=1$) -->

<!-- <div class = "blue"> -->
<!-- **Closed-form solution for parameter fit under ordinary least squares loss function (special case where $k=1$).** -->

<!-- *If there is only one predictor variable $(k=1)$, the closed-form solution of predictors in the model that minimize RSS (Residual Sum of Squares) is:* -->


<!-- $$\begin{align} -->
<!-- \hat{\beta_0} &= \bar{y} - \hat{\beta}_1 \bar{x}\textrm{, and}\\ -->
<!-- \\ -->
<!-- \hat{\beta_1} &= \frac{Cov(x,y)}{Var(x)}. -->
<!-- \end{align}$$ -->
<!-- </div> -->

<!-- ##### Proof -->

<!-- *[See e.g., @kirchner2003, pp. 1-3; @olive2017, pp. 57-59]* -->

<!-- Given a set of $n$ observations $(X_i,Y_i)$ (or points on a scatter plot), we want to find the best-fit line,  -->
<!-- $$\hat y_i=\hat\beta_0+\hat\beta_1x_i,\tag{1.1.1}$$ -->
<!-- such that the sum of squared errors (RSS) in $Y$ is minimized: -->

<!-- $$RSS=\sum_{i=1}^n (y_i - \hat{y}_i)^2 \rightarrow min.\tag{1.1.2}$$ -->

<!-- Let the *Residual Sum of Squares (RSS)*  be denoted as $Q$ with, -->

<!-- $$\begin{align} -->
<!-- Q=RSS&=\sum_{i=1}^{n}(y_i-\hat y_i)^2\\ &=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2. -->
<!-- \tag{1.1.3} -->
<!-- \end{align}$$ -->

<!-- We want to minimize $Q$ (that is minimizing *RSS*) at the values of $\hat\beta_0$ and $\hat\beta_1$ for which $\frac{\partial Q}{\partial \hat\beta_0}=0$ (1) and $\frac{\partial Q}{\partial \hat\beta_1}=0$ (2). -->

<!-- The first condition (1) is, -->

<!-- $$ \begin{align} \frac{\partial Q}{\partial \hat\beta_0}=\sum_{i=1}^{n}-2(y_i-\hat\beta_0-\hat\beta_1x_i)&= 0\\ -->
<!-- &=-\sum_{i=1}^ny_i+\sum_{i=1}^n\hat \beta_0+\sum_{i=1}^n\hat\beta_1x_i\\ -->
<!-- &=-\sum_{i=1}^ny_i+n\hat\beta_0+\sum_{i=1}^n\hat\beta_1x_i -->
<!-- \tag{1.1.4} -->
<!-- \end{align}$$ -->


<!-- which, if we solve for $\hat\beta_0$, becomes -->

<!-- $$\begin{align} -->
<!-- \hat\beta_0&=\frac{1}{n}\sum_{i=1}^{n}y_i-\frac{1}{n}\hat\beta_1\sum_{i=1}^{n}x_i\\ -->
<!-- &=\bar y - \hat\beta_1\bar x, -->
<!-- \tag{1.1.5} -->
<!-- \end{align}$$ -->

<!-- which says that the constant $\hat\beta_0$ (the y-intercept) is set such that the line must go through the mean of $x$ and $y$. This makes sense because this point is the "center" of the data cloud. -->

<!-- The solution is indeed a minimum as the second partial derivative is positive: -->

<!-- $\frac{\partial^2 Q}{\partial\hat\beta_0^2}=2n>0. \tag{1.1.6}$ -->

<!-- The second condition (2) is, -->

<!-- $$ \begin{align} -->
<!-- \frac{\partial Q}{\partial \hat\beta_1}=\sum_{i=1}^{n}-2x_i(y_i-\hat\beta_0-\hat\beta_1x_i)&=0\\ -->
<!-- &=\sum_{i=1}^{n}(-x_iy_i+\hat\beta_0x_i+\hat\beta_1x_i^2)\\ -->
<!-- &=-\sum_{i=1}^{n}x_iy_i+\hat\beta_0\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2 -->
<!-- \tag{1.1.7} -->
<!-- \end{align}$$ -->

<!-- If we substitute the expression by (1.1.5), we get, -->

<!-- $$ \begin{align} -->
<!-- 0&=-\sum_{i=1}^{n}x_iy_i+(\bar y - \hat\beta_1\bar x)\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2\\ -->
<!-- &=-\sum_{i=1}^{n}x_iy_i+\bar y\sum_{i=1}^{n}x_i-\hat\beta_1\bar x\sum_{i=1}^{n}x_i+\hat\beta_1\sum_{i=1}^{n}x_i^2 -->
<!-- \tag{1.1.8} -->
<!-- \end{align}$$ -->

<!-- separating this into two sums, -->

<!-- $$ \sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)-\hat\beta_1\sum_{i=1}^{n}\left(x_i^2-x_i\bar x\right)=0 \tag{1.1.9}$$ -->

<!-- becomes, -->

<!-- $$ \hat\beta_1 = \frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)} = \frac{\sum_{i=1}^{n}\left( x_iy_i\right)-n\bar x\bar y}{\sum_{i=1}^{n}\left( x_i^2\right)-n \bar x^2} \tag{1.1.10}$$ -->

<!-- The model assumes that the deviation from the values from the mean is zero, so that the positive and negative values are in balance, thus -->

<!-- $$ \sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)=0, \tag{1.1.11}$$ -->

<!-- and -->

<!-- $$ \sum_{i=1}^{n}\left(\bar x \bar y - y_i \bar x\right)=0. \tag{1.1.12}$$ -->

<!-- This can be used in order to expand the previous term and finally to rewrite $\hat\beta_1$ as the ratio of $Cov(x,y)$ to $Var(x)$: -->

<!-- $$ -->
<!-- \begin{align} -->
<!-- \hat\beta_1&=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+\sum_{i=1}^{n}\left(\bar x\bar y - y_i \bar x\right)}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+\sum_{i=1}^{n}\left( \bar x^2-x_i\bar x\right)}=\frac{\sum_{i=1}^{n}\left( x_iy_i-x_i\bar y\right)+0}{\sum_{i=1}^{n}\left( x_i^2-x_i\bar x\right)+0}\\ -->
<!-- \\ -->
<!-- &=\frac{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right) \left(y_i- \bar y \right)}{\frac{1}{n}\sum_{i=1}^{n}\left( x_i-\bar x\right)^2}\\ -->
<!-- \\ -->
<!-- &=\frac{Cov(x,y)}{Var(x)}. -->
<!-- \tag{1.1.13} -->
<!-- \end{align}$$ -->

<!-- The solution is indeed a minimum as the second partial derivative is positive: -->

<!-- $$\frac{\partial^2Q}{\partial \hat\beta_1^2}= 2 \sum_{i=1}^{n}x_i^2 >0. \tag{1.1.14}$$ -->

<!-- #### Theorem 1b: Generalization of Theorem 1a to $k >=1$ -->

<!-- *[see e.g., @bremer2012, pp. 21-23; @gonzalez2014, pp. 5-15]* -->

<!-- The model of multiple linear regression is given by the following expression: -->

<!-- $$y=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_kx_k+\epsilon \tag{1.2.1}$$ -->
<!-- Suppose we have $n$ observations, then we can write: -->
<!-- $$\begin{align} -->
<!-- y_1&=\beta_0+\beta_{1}x_{11}+\beta_2x_{21}+...+\beta_kx_{k1}+\epsilon_1\\ -->
<!-- y_2&=\beta_0+\beta_{1}x_{12}+\beta_2x_{22}+...+\beta_kx_{k2}+\epsilon_2\\ -->
<!-- ...\\ -->
<!-- y_n&=\beta_0+\beta_{1}x_{1n}+\beta_2x_{2n}+...+\beta_kx_{kn}+\epsilon_n -->
<!-- \tag{1.2.2} -->
<!-- \end{align}$$ -->
<!-- The model of multiple linear regression is often expressed in matrix notation: -->

<!-- $$\begin{bmatrix} y_1\\y_2\\...\\y_n \end{bmatrix}= \begin{bmatrix}1&x_{11}& x_{21}&...&x_{k1}\\1&x_{12}& x_{22}&...&x_{k2}\\...& ...&...&...&...\\1&x_{1n}&x_{2n}&...&x_{kn}\end{bmatrix}\begin{bmatrix}\beta_0\\\beta_1\\...\\\beta_n \end{bmatrix}+\begin{bmatrix}\epsilon_1\\\epsilon_2\\...\\\epsilon_n \end{bmatrix} \tag{1.2.3}$$ -->

<!-- Which can be expressed in a compact form as -->

<!-- $$\mathbf{Y=X\beta+\epsilon} \tag{1.2.4}$$ -->
<!-- where $y$ is a vector $n\times 1$, $X$ is a matrix $n \times k$, $\beta$ is a vector $k \times 1$ and $\epsilon$ is a vector $n \times 1$. -->

<!-- The OLS estimator is obtained (like in the special case) by minimizing the residual sum of squares (RSS).  -->

<!-- $$RSS \rightarrow min.$$  -->

<!-- The RSS for the multiple linear regression model is -->

<!-- $$Q=RSS=\sum_{i=1}^n \hat\epsilon_i^2=\sum_{i=1}^n (y_i - \hat{y}_i)^2=\sum_{i=1}^n \left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]^2 \tag{1.2.5}$$ -->

<!-- to apply the least-squares criterion in the model of multiple linear regression, thus to minimize $RSS$, we calculate the first partial derivative from $Q$ with respect to each $\hat\beta_j$in the expression: -->

<!-- $$\begin{align} -->
<!-- \frac{\partial Q}{\partial\hat\beta_0}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-1]\\ -->
<!-- \\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_1}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{1i}]\\ -->
<!-- \\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_2}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{2i}]\\ -->
<!-- ...\\ -->
<!-- \frac{\partial Q}{\partial\hat\beta_k}&=2\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right][-x_{ki}] -->
<!-- \tag{1.2.6} -->
<!-- \end{align}$$ -->

<!-- Then the derivative of each equation is set to zero: -->

<!-- $$\begin{align} -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]=0\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{1i}=0\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{2i}=0\\ -->
<!-- &...\\ -->
<!-- &\sum_{i=1}^n\left[y_i-\hat\beta_0-\hat\beta_1x_{1i}-\hat\beta_2x_{2i}-...-\hat\beta_kx_{ki}\right]x_{ki}=0 -->
<!-- \tag{1.2.7} -->
<!-- \end{align}$$ -->

<!-- Alternatively, we can use matrix notation and combine the above equations into the following form: -->

<!-- $$\mathbf{X'Y-X'X\hat\beta=0}.\tag{1.2.8}$$ -->

<!-- Whereby the following expression is known as **normal equations**: -->

<!-- $$\mathbf{X'X\hat\beta=X'Y}.\tag{1.2.9}$$ -->

<!-- The system of normal equations in expanded matrix notation is: -->

<!-- $$\begin{bmatrix} n&\sum_{i=1}^nx_{1i}&...&\sum_{i=1}^nx_{ki}\\ -->
<!-- \sum_{i=1}^nx_{1i}&\sum_{i=1}^nx_{1i}^2&...&\sum_{i=1}^nx_{1i}x_{ki}\\...&...&...&...\\ -->
<!-- \sum_{i=1}^nx_{ki}&\sum_{i=1}^nx_{ki}x_{1i}&...&\sum_{i=1}^nx_{ki}^2\end{bmatrix}\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\...\\\hat\beta_k\end{bmatrix}=\begin{bmatrix}\sum_{i=1}^ny_i\\\sum_{i=1}^nx_{1i}y_i\\...\\\sum_{i=1}^nx_{ki}y_i  -->
<!-- \tag{1.2.10} -->
<!-- \end{bmatrix}$$ -->

<!-- In order to obtain the estimator $\hat\beta$, we have to rearrange (1.2.10) and get the solution: -->

<!-- $$\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\...\\\hat\beta_k\end{bmatrix}=\mathbf{\hat\beta}=[\mathbf{X'X}]^{-1}\mathbf{X'Y}\tag{1.2.11}$$ -->

<!-- Where $\hat\beta$ is a global minimizer of the OLS criterion as the second order condition is always a semidefinite positive matrix. -->

<!-- $$\frac{\partial^2 Q}{\partial \mathbf{\hat\beta}^2}=2X'X >0.$$ -->

<!-- --- -->

<!-- ### Theorem 2 -->

<!-- <div class = "blue"> -->
<!-- **Best fit under OLS is equivalent with best fit under MLE** -->

<!-- *The parameters $\beta_0, ..., \beta_k$ minimize the residual sum of squares (RSS) iff they maximize the (log-)likelihood (LH).* -->

<!-- </div> -->

<!-- ##### Proof -->

<!-- *[see e.g., @naveen2019; @croot2010; @eppes2019]* -->

<!-- #### Maximum Likelihood Estimation -->
<!-- We consider again the linear regression model of the population with: -->

<!-- $$Y= \beta_0 + \beta_1X + \epsilon.\tag{2.1}$$ -->

<!-- This simplifies to the following form on the observed data: -->

<!-- $$y= \beta_0 + \beta_1x + \epsilon.\tag{2.2}$$ -->

<!-- Using a sample in order to obtain the **maximum likelihood estimates** the equation simplifies to: -->

<!-- $$y= \hat\beta_0 + \hat\beta_1x. \tag{2.3}$$ -->

<!-- **Assumptions** that we make for the model: -->

<!-- - True underlying distribution of the errors is Gaussian, -->
<!-- - Expected value of the error term is 0, -->
<!-- - Variance of the error term is constant with respect to x, and -->
<!-- - the 'lagged' errors are independent of each other -->
<!-- where the error term is normally distributed. -->

<!-- We can write: -->

<!-- $$ \epsilon \sim N(0,\sigma^2).\tag{2.4}$$ -->

<!-- Since $Y$ is a linear function of $\epsilon$ it will also be normally distributed. -->

<!-- $$f(\epsilon|\beta_0,\beta_1)= \frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon^2}{\sigma^2}\right)}\right]. \tag{2.5}$$  -->

<!-- Given the whole data set with $i=1,...,n$ observations the **likelihood function** ($LH$) is the joint density of all the observations, given a value for the parameters $\beta_0$ and $\beta_1$. Since independence is assumed, this is simply the product of the individual densities from the previous equation.  -->

<!-- $$LH(\epsilon_i|\beta_0,\beta_1) =\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi} \sigma} \exp\left[{-\frac{1}{2}\left(\frac{\epsilon_i^2}{\sigma^2}\right)}\right].\tag{2.6}$$ -->

<!-- In order to find the maximum of (2.6) we have to find the first derivative. -->

<!-- But as the derivation of a product with a lot of factors is inconvenient, we take first the logarithm of the likelihood, called **log-likelihhod**. This is possible as $\log$ is a monotone transformation and the maximum likelihood estimate does not change on log transformation. -->

<!-- The log-likelihood is the sum of the logs of the individual densities: -->

<!-- $$\begin{align} -->
<!-- LL&=\log \left(LH(\epsilon_i|\beta_0,\beta_1)\right)\\ -->
<!-- &=-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2 -->
<!-- \tag{2.7} -->
<!-- \end{align}$$ -->

<!-- The log-likelihood can be used in order to find the *Maximum Likelihood estimates* $\hat\beta_0$ and $\hat\beta_1$ for the parameters $\beta_0$ and $\beta_1$. -->


<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1}LL=\mathrm{argmax}_{\beta_0,\beta_1}\left[-\left( \frac{n}{2}\right) \log(2\pi)-\left( \frac{n}{2}\right) \log(\sigma^2)-\left( \frac{1}{2}\sigma^2\right) \sum_{i=1}^n(\epsilon_i)^2\right].\tag{2.8}$$ -->

<!-- Removing the constant terms results in: -->

<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-\epsilon_i^2.\tag{2.9}$$ -->

<!-- Substituting $\epsilon$, derived from (2.2), gives: -->

<!-- $$\hat\beta_0,\hat\beta_1=\mathrm{argmax}_{\beta_0,\beta_1} \sum_{i=1}^n-(y-\beta_0-\beta_1x)^2.\tag{2.10}$$ -->

<!-- **Conclusion:** -->

<!-- Deriving parameter estimates according to the OLS method: -->

<!-- $$Q_{OLS}=\sum_{i=1}^{n}(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow min.$$ -->

<!-- Deriving parameter estimates according to the ML method: -->

<!-- $$Q_{ML}=\sum_{i=1}^n-(y_i-\hat\beta_0-\hat\beta_1x_i)^2 \rightarrow max.$$ -->

<!-- Maximizing $-z$ is equivalent to minimizing $z$, thus, the best parameter fit under ML is equivalent to the best fit under OLS. -->

<!-- --- -->

<!-- ### Theorem 3 -->

<!-- <div class = "blue"> -->
<!-- **The variance in a regression model can be decompossed by using the notion of sum of squares:** -->

<!-- *The following decomposition holds:* -->

<!-- $$\begin{align} -->
<!-- TSS &= ESS + RSS \textrm{, or}\\ -->
<!-- \sum_{i=1}^n (y_i - \bar{y})^2&=\sum_{i=1}^n (\hat{y}_i - \bar{y})^2+ \sum_{i=1}^n \hat u_i^2  -->
<!-- \end{align}$$ -->

<!-- </div> -->


<!-- with  -->

<!-- $$\hat u_i =y_i-\hat y_i $$ -->

<!-- which can be rewritten as -->

<!-- $$y_i=\hat y_i + \hat u_i \tag{3.1}$$ -->

<!-- #### Proof -->

<!-- [see e.g., @gonzalez2014 pp. 29-31] -->

<!-- $$\begin{align} -->
<!-- TSS=\sum_{i=1}^n (y_i - \bar{y})^2&=\sum_{i=1}^n\left( \hat y_i+\hat u_i - \bar y \right)^2\\ -->
<!-- &= \sum_{i=1}^n \left[(\hat y_i-\bar y)+\hat u_i\right]^2\\ -->
<!-- &= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n \hat u_i^2+ 2\sum_{i=1}^n(\hat y_i-\bar y)\hat u_i\\ -->
<!-- &= \sum_{i=1}^n (\hat y_i-\bar y)^2 + \sum_{i=1}^n \hat u_i^2\\ -->
<!-- \tag{3.2} -->
<!-- \end{align}$$ -->

<!-- Or, using (3.1) we can write: -->

<!-- $$ -->
<!-- TSS=\sum_{i=1}^n (\hat{y}_i - \bar{y})^2+\sum_{i=1}^n (y_i - \hat{y}_i)^2=ESS+RSS. -->
<!-- \tag{3.3} -->
<!-- $$ -->

<!-- --- -->

<!-- ### Theorem 4 -->

<!-- <div class = "blue"> -->
<!-- **Expressing variance explained $R^2$ in terms of sum of squares** -->

<!-- *The following relationship holds:*  -->

<!--   $$R^2 = 1 - \frac{RSS}{TSS} = \frac{ESS}{TSS}$$ -->
<!-- </div> -->

<!-- #### Proof -->

<!-- [see e.g., @urban2018 pp. 56-72, 101-103 ] -->

<!-- We will first derive the variance explained $R^2$ considering a bivariate model (simple linear regression), afterward some note to the generalized case of multiple regression is made. -->

<!-- The variance explained can be derived in the bivariate model from the regression coefficient $\beta$ and the standard deviation of $X$ and $Y$, whereby the following holds: -->

<!-- $$R^2=\left( \beta \cdot \frac{S_x}{S_y}\right)^2=\beta^2 \cdot \frac{Var(X)}{Var(Y)}.\tag{4.1}$$ -->

<!-- We will show that (4.1) holds in the next steps. -->

<!-- Consider on the one hand the calculation of the correlation coefficient: -->

<!-- $$\begin{align} -->
<!-- r_{xy}&=\frac{cov(X,Y)}{S_x S_y}\\ -->
<!-- \\ -->
<!-- &=\frac{\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)(Y_i-\bar Y)}{\sqrt{\frac{\sum_{i=1}^n(X_i-\bar X)^2}{n}} \sqrt{\frac{\sum_{i=1}^n(Y_i-\bar Y)^2}{n}}} -->
<!-- \tag{4.2} -->
<!-- \end{align}$$ -->

<!-- On the other hand, if we assume $Y$ as dependent and $X$ as an independent variable, we can write: -->

<!-- $$\beta_{yx}=\frac{\sum_{i=1}^{n}X_iY_i}{\sum_{i=1}^n X_i^2}. \tag{4.3}$$ -->

<!-- Or when $Y$ is the independent and $X$ the dependent variable, then: -->

<!-- $$\beta_{xy}=\frac{\sum_{i=1}^{n}Y_iX_i}{\sum_{i=1}^n Y_i^2}.\tag{4.4}$$ -->

<!-- Through dividing equations (4.3) and (4.4) by the number of observations, we get the covariances and variances of $X$ and $Y$: -->

<!-- $$\begin{align} -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}X_iY_i=\frac{1}{n}(X_i-\bar X)(Y_i-\bar Y) = cov(X,Y)\tag{4.5}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}Y_iX_i=\frac{1}{n}(Y_i-\bar Y)(X_i-\bar X) = cov(Y,X)\tag{4.6}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}X_i^2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2=S_x^2\tag{4.7}\\ -->
<!-- &\frac{1}{n}\sum_{i=1}^{n}Y_i^2=\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar Y)^2=S_y^2 -->
<!-- \tag{4.8} -->
<!-- \end{align}$$ -->

<!-- Substituting these results in equations (4.3) and (4.4), we get: -->

<!-- $$\beta_{yx}=\frac{cov(X,Y)}{S_x^2} \textrm{ , and } -->
<!-- \beta_{xy}=\frac{cov(Y,X)}{S_y^2}\tag{4.9}$$ -->

<!-- For calculating the *average* between $\beta_{yx}$ and $\beta_{xy}$ we have to use the *geometric mean*: -->

<!-- $$\bar\beta_{geom}=\sqrt{\frac{cov(X,Y)}{S_x^2}\frac{cov(Y,X)}{S_y^2}}=\frac{cov(X,Y)}{S_x S_y}=r_{xy}\tag{4.10}$$ -->

<!-- Thus, the geometric mean of $\beta_{yx}$ and $\beta_{xy}$ is identical to the correlation coefficient.  -->

<!-- Comparing equations (4.9) and (4.10), -->

<!-- $$\begin{align} -->
<!-- \beta_{yx}&=\frac{cov(X,Y)}{S_x^2}\textrm{ , and}\\ -->
<!-- \\ -->
<!-- r_{yx}&=\frac{cov(X,Y)}{S_x S_y}, -->
<!-- \end{align}$$ -->

<!-- shows that, we can convert $r_{yx}$ in $\beta_{yx}$: -->

<!-- $$\frac{cov(X,Y)}{S_x S_y} \cdot \frac{S_y}{S_x}=\frac{cov(X,Y)}{S_x^2}=\beta_{yx}.\tag{4.11}$$ -->

<!-- Therefore, the relationship stated in (4.1) holds: -->

<!-- $$\beta_{yx}=r_{yx}\cdot \frac{S_y}{S_x}.$$ -->

<!-- Using this definition we can now derive the **variance explained** $R^2$. -->

<!-- We have seen that the following holds: -->

<!-- $$\beta_{yx}=r_{yx}\cdot \frac{S_y}{S_x}=r_{yx}\sqrt{\frac{\frac{1}{n}\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar X)^2}}.$$ -->

<!-- Eliminating the number of observations and squaring the equation gives: -->

<!-- $$\beta_{yx}^2=r_{yx}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2}.\tag{4.12}$$ -->

<!-- The numerator $\sum_{i=1}^{n}(Y_i-\bar Y)^2$ is the total sum of squares (TSS), thus, from (0.1) follows: -->

<!-- $$\sum_{i=1}^{n}(Y_i-\bar Y)^2=\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2.\tag{4.13}$$ -->

<!-- The explained sum of squares (ESS) $\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2$ are statistically explained by the determination of the regression line: -->

<!-- $$ESS=\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2=\beta X_i=\beta(X_i-\bar X).\tag{4.14}$$ -->

<!-- Substituting the result of (4.14) in equation (4.13) gives: -->

<!-- $$\sum_{i=1}^{n}(Y_i-\bar Y)^2=\beta^2\sum_{i=1}^{n}(X_i-\bar X)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2.\tag{4.15}$$ -->

<!-- As we know from (4.12): -->

<!-- $$\beta_{yx}^2=r_{yx}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2},$$ -->

<!-- we can substitute $\beta$ in equation (4.15) with the result from (4.12) and get: -->

<!-- $$\begin{align} -->
<!-- \sum_{i=1}^{n}(Y_i-\bar Y)^2&=r_{xy}^2\frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2}{\sum_{i=1}^{n}(X_i-\bar X)^2}\sum_{i=1}^{n}(X_i-\bar X)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2\\ -->
<!-- \\ -->
<!-- &=r_{xy}^2 \sum_{i=1}^{n}(Y_i-\bar Y)^2+\sum_{i=1}^{n}(Y_i-\hat Y_i)^2\tag{4.16}\\ -->
<!-- \\ -->
<!-- \frac{\sum_{i=1}^{n}(Y_i-\bar Y)^2-\sum_{i=1}^{n}(Y_i-\hat Y_i)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}&=r_{xy}^2 \textrm{ ,which is}\\ -->
<!-- \\ -->
<!-- 1-\frac{\sum_{i=1}^{n}(Y_i-\hat Y_i)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}=1-\frac{RSS}{TSS}&=r_{xy}^2. -->
<!-- \tag{4.17} -->
<!-- \end{align}$$ -->

<!-- Similarly, according to (0.2) the explained sum of squares (ESS) are the deviation between total (TSS) and residual sum of squares (RSS): -->

<!-- $$\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2 = \sum_{i=1}^{n}( Y_i-\bar Y)^2-\sum_{i=1}^{n}(Y_i-\hat Y)^2,$$ -->

<!-- therefore, we can write equation (4.17) as well as: -->

<!-- $$\begin{align} -->
<!-- \frac{\sum_{i=1}^{n}(\hat Y_i-\bar Y)^2}{\sum_{i=1}^{n}(Y_i-\bar Y)^2}&=r_{xy}^2\\ -->
<!-- \\ -->
<!-- \frac{ESS}{RSS}&=r_{xy}^2=R^2. -->
<!-- \tag{4.18} -->
<!-- \end{align}$$ -->

<!-- **Final remarks:** -->
<!-- In the bivariate model (simple linear regression) the variance explained is equivalent to the squared bivariate correlation coefficient (Bravais Pearson).  -->
<!-- In multiple regression model the relation: -->

<!-- $$\frac{ESS}{TSS}=R^2$$ -->

<!-- holds as well. Here the variance explained is equivalent to the squared multiple correlation coefficient between the estimated and observed Y-value ($r_{\hat Y Y}$). -->

<!-- In the multiple linear regression model, the variance explained is also dependent on the number of X-variables. Therefore an adjusted $R^2$ measure should be used. -->
