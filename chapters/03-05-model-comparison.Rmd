# Model Comparison {#Chap-03-06-model-comparison}

<hr>

<div style = "float:right; width:40%;">
<img src="visuals/badge-model-comparison.png" alt="badge model comparison">  
</div>  

Parameter estimation asks: given a single model and the data, what are good (e.g., credible) values of the model's parameters? Hypothesis testing applies this logic: given a model and fixing (for the sake of construction) a certain parameter value, is it plausible that the data was generated by the assumed model + parameter setting? Finally, model comparison (the topic of this chapter) asks: based on the data at hand, which of several models is better? Or even: *how much* better is this model compared to another, given the data?

Frequentist and Bayesian approaches agree that the pivotal criterion by which to compare models is how well a model explains the observed data. A good explanation of observed data $D$ is one that makes $D$ unsurprising. Intuitively, we long for an explanation for things that puzzle us. A good explanation is a way of looking at the world in which puzzle disappear, in which all observations make sense, in which what we have seen would have been quite expectable after all. Consequently, the pivotal quantity for comparing models is how likely $D$ is given a model $M_i$: $P(D \mid M_i)$.

But there is more to a good explanation, also intuitively. All else equal, a good explanation is simple. If theories $A$ and $B$ both explain the facts equally well, but $A$ does so with less "mental machinery", most people would choose the more economical explanation $A$. 

In this chapter, we will look at three common methods of comparing models. Two are frequentist (Akaike information criterion & likelihood-ratio test). The third is Bayesian (Bayes factor). There are more approaches and methods (e.g., $K$-fold cross-validation, other varieties of information criteria, ANOVA-based model comparisons). Our goal is not to be exhaustive, but to introduce the main ideas of model comparison and showcase a reasonable selection of alternative approaches.

```{block, type='infobox'}
The learning goals for this chapter are:

- understand the differences between estimation, testing and model comparison
- understand the idea behind and become able to apply the covered methods:
  - Akaike information criterion
  - likelihood-ratio test
  - Bayes factor
- become familiar with pro's and con's of each of these methods
```


## Case study: recall models {#Chap-03-06-model-comparison-case-study}

As a running example for this chapter, we borrow from @Myung2003:Tutorial-on-Max and consider a fictitious data set of recall rates and two models to explain this data. 

As for data, for each time point (in seconds) $t \in \{1, 3, 6, 9, 12, 18\}$, we have 100 (binary) observations of whether a previously memorized item was recalled correctly.


```{r}
# time after memorization (in seconds)
t <- c(1, 3, 6, 9, 12, 18)
# proportion (out of 100) of correct recall
y <- c(.94, .77, .40, .26, .24, .16)
# number of observed correct recalls (out of 100)
obs <- y * 100
```

A visual representation of this data set is here:

```{r echo = F}
tibble(
  t, obs
) %>% 
  ggplot(aes(x = t, y = y)) +
  geom_point() +
  labs(
    x = "time (in seconds)",
    y = "proportion of correct recall",
    title = "(fictitious) recall data"
  ) +
  ylim(c(0,1))
```

We are interested in comparing two theoretically different models for this data. Models differ in their assumption about the functional relationship between recall probability and time. The **exponential model** assumes that the recall probability $\theta_t$ at time $t$ is an exponential decay function with parameters $a$ and $b$:

$$\theta_t(a, b) = a \exp (-bt), \ \ \ \ \text{where } a,b>0 $$

The resulting (frequentist) exponential model, assuming a Binomial likelihood function of the recall data is therefore:

$$
\begin{aligned}
P(D = \langle k, N \rangle \mid \langle a, b\rangle) & = \text{Binom}(k,N, a \exp (-bt)), \ \ \ \ \text{where } a,b>0 
\end{aligned}
$$

In contrast, the **power model** assumes that the relationship is that of a power function:

$$\theta_t(c, d) = ct^{-d}, \ \ \ \ \text{where } c,d>0 $$

The resulting (frequentist) power model, assuming a Binomial likelihood function of the recall data is therefore:

$$
\begin{aligned}
P(D = \langle k, N \rangle \mid \langle c, d\rangle) & = \text{Binom}(k,N, c\ t^{-d}), \ \ \ \ \text{where } c,d>0 
\end{aligned}
$$

These models therefore make different (parameterized) predictions about the time course of forgetting/recall. Figure \@ref(fig:Chap-03-06-model-comparison-model-predictions) shows predictions of each model for $\theta_t$ for different parameter values:

```{r Chap-03-06-model-comparison-model-predictions, echo = F, fig.height=12, fig.cap = "Examples of predictions of the exponential and the power model of forgetting for different values of each model's paramters."}
forgetData = data.frame(t = t, obs = obs, y = y)
expo = function(x, c, d) return( c* exp(-x*d) )
power = function(x, a, b) return( a*x^(-b) )
myCols = project_colors[1:3]

forgetPlotExpo = ggplot(data.frame(x = c(1,20)), aes(x)) +
         stat_function(fun = function(x) expo(x, 1,1), aes(color = "a,b=1")) +
         stat_function(fun = function(x) expo(x, 2,2), aes(color = "a,b=2")) +
         stat_function(fun = function(x) expo(x, 1,0.2), aes(color = "a=1,b=0.1")) +
         scale_colour_manual("Function", breaks = c("a,b=1", "a,b=2", "a=1,b=0.1"), values = myCols) +
          ggtitle("exponential model") + geom_point(data = forgetData, aes(x = t, y = y)) + ylab("recall prob.") + xlab("time t")
forgetPlotPower = ggplot(data.frame(x = c(1,20)), aes(x)) +
         stat_function(fun = function(x) power(x, 1,1), aes(color = "c,d=1")) +
         stat_function(fun = function(x) power(x, 2,2), aes(color = "c,d=2")) +
         stat_function(fun = function(x) power(x, 2,1), aes(color = "c=2, d=1")) +
         scale_colour_manual("Function", breaks = c("c,d=1", "c,d=2", "c=2, d=1"), values = myCols) +
          ggtitle("power model") + geom_point(data = forgetData, aes(x = t, y = y)) + ylab("recall prob.") + xlab("time t")
cowplot::plot_grid(forgetPlotExpo, forgetPlotPower, nrow = 2)
```

The research question of relevance is: which of these two models is a better model for the observed data?

## Akaike Information Criterion {#Chap-03-06-model-comparison-AIC}

A wide-spread approach to model comparison used in the frequentist paradigm is to use the **Akaike information criterion (AIC)**. The AIC is the most common instance of a class of measures for model comparison known as *information criteria*, which all draw on information-theoretic notions to compare how good each model is.

If $M_i$ is a frequentist model, specified by likelihood function $P(D \mid \theta_i, M_i)$, with $k$ model parameters in parameter vector $\theta_i$, and if $D_\text{obs}$ is the observed data, then the AIC score of model $M_i$ given $D_\text{obs}$ is defined as:

$$
\begin{aligned}
\text{AIC}(M_i, D_\text{obs}) & = 2k - 2\log P(D_\text{obs} \mid \hat{\theta_i}, M_i)
\end{aligned}
$$
Here, $\hat{\theta}_i = \arg \max_{\theta_i} P(D_\text{obs} \mid \theta_i, M_i)$ is the best-fitting parameter vector, i.e., the maximum likelihood estimate (MLE), and $k$ is the number of free parameters in model $M_i$.

The lower an AIC score the better the model (in comparison to other models for the same data $D_\text{obs}$). All else equal, the higher the number of free parameters $k$ the worse the model's AIC score. The first summand in the definition above can, therefore, be conveived of as a measure of **model complexity**. As for the second summand, think of $- \log P(D_\text{obs} \mid \hat{\theta}_i)$ as a measure of (information-theoretic) surprisal: how surprising is the observed data $D_\text{obs}$ from the point of view of model $M$ under the most favorable circumstances (that is, the MLE of $\theta_i$). The higher the probability $P(D_\text{obs} \mid \hat{\theta}_i)$, the better the model's AIC score, all else equal.

To apply AIC-based model comparison to the recall models, we first need to compute the MLE of each model (see Chapter \@ref(ch-03-03-estimation-frequentist)). Here are functions that return the negative log-likelihood of each model, for any (suitable) pair of parameter values:

```{r}
# generic neg-log-LH function (covers both models)
nLL_generic <- function(par, model_name) {
  w1 <- par[1]
  w2 <- par[2]
  # make sure paramters are in acceptable range
  if (w1 < 0 | w2 < 0 | w1 > 20 | w2 > 20) {
    return(NA)
  }
  # calculate predicted recall rates for given parameters
  if (model_name == "exponential") {
    theta <- w1*exp(-w2*t)  # exponential model
  } else {
    theta <- w1*t^(-w2)     # power model
  }
  # avoid edge cases of infinite log-likelihood
  theta[theta <= 0.0] <- 1.0e-4
  theta[theta >= 1.0] <- 1-1.0e-4
  # return negative log-likelihood of data
  - sum(dbinom(x = obs, prob = theta, size = 100, log = T))
}
# negative log likelihood of exponential model
nLL_exp <- function(par) {nLL_generic(par, "exponential")}
# negative log likelihood of power model
nLL_pow <- function(par) {nLL_generic(par, "power")}
```

These functions are then optimized with built-in function `optim`. The results are shown in the table below.

```{r}
# getting the best fitting values
bestExpo <- optim(nLL_exp, par = c(1,0.5))
bestPow  <- optim(nLL_pow, par = c(0.5,0.2))
MLEstimates = data.frame(model = rep(c("exponential", "power"), each = 2),
                         parameter = c("a", "b", "c", "d"),
                         value = c(bestExpo$par, bestPow$par))
knitr::kable(MLEstimates)
```

The MLE-predictions of each model are shown in Figure \@ref(fig:Chap-03-06-model-comparison-MLE-fits) below, alongside the observed data.

```{r Chap-03-06-model-comparison-MLE-fits, echo = F, fig.cap = "Predictions of the exponential and the power model under best-fitting parameter values."}
a <- bestExpo$par[1]
b <- bestExpo$par[2]
c <- bestPow$par[1]
d <- bestPow$par[2]
forgetPlotBest <- ggplot(data.frame(x = c(1,20)), aes(x)) +
  stat_function(fun = function(x) expo(x, a, b), aes(color = "exponential")) +
  stat_function(fun = function(x) power(x, c, d), aes(color = "power")) +
  scale_colour_manual(
    "Function", 
    breaks = c("exponential", "power"), 
    values = project_colors[1:2]
  ) +
  ggtitle("MLE fits") + geom_point(data = forgetData, aes(x = t, y = y)) +
  labs(
    x = "time",
    y = "recall rate"
  )
forgetPlotBest
```


It is impossible to say by visual inspection of Figure \@ref(fig:Chap-03-06-model-comparison-MLE-fits) which model should be preferred. Numbers might help see more fine-grained differences:

```{r}
predExp <- expo(t,a,b)
predPow <- power(t,c,d)
modelStats <- tibble(
  model = c("expo", "power"),
  `log likelihood` = round(c(-bestExpo$value, -bestPow$value),3),
  probability = signif(exp(c(-bestExpo$value, -bestPow$value)),3),
  # sum of squared errors
  SS = round(c( sum((predExp-y)^2), sum((predPow-y)^2)),3)
)
modelStats
```

The exponential model has a higher log-likelihood, a higher probability, and a lower sum of squares. This suggests that the exponential model is better. 

The AIC-score of these models is a direct function of the negative log-likelihood. Since both models have the same number of parameters, we also arrive at the same verdict: based on comparison of AIC-scores, the exponential model is the better model.

```{r}
get_AIC <- function(optim_fit) {
  2 * length(optim_fit$par) + 2 * optim_fit$value
}
AIC_scores <- tibble(
  AIC_exponential = get_AIC(bestExpo),
  AIC_power = get_AIC(bestPow)
)
AIC_scores
```

How should we interpret the difference in AIC-scores? Some suggest that differences in AIC-scores larger than 10 should be treated as implying that the weaker model has practically no empirical support [@BurnhamAnderson2002:Model-Selection]. Adopting such a criterion, we would therefore favor the exponential model based on the data observed. 

But we could also try to walk a more nuanced, a more quantitative road. Indeed, we can look at relative AIC-scores in terms of so-called **Akaike weights** [@WagenmakersFarrell2004:AIC-model-selec;@BurnhamAnderson2002:Model-Selection]. If models $M_1, \dots, M_n$ are on the table and $\text{AIC}(M_i, D)$ is the AIC-score of model $M_i$ for observed data $D$, then the Akaike weight of model $M_i$ is defined as:



$$
\begin{aligned}
w_{\text{AIC}}(M_i, D) & = \frac{\exp (- 0.5 * \Delta_{\text{AIC}}(M_i,D) )} {\sum_{j=1}^k\exp (- 0.5 * \Delta_{\text{AIC}}(M_j,D) )}\, \ \ \ \ \text{where} \\
\Delta_{\text{AIC}}(M_i,D) & = \text{AIC}(M_i, D) - \min_j \text{AIC}(M_j, D)
\end{aligned}
$$

Akaike weights are relative and normalized measures, and may serve as an approximate measure of a model's posterior probability given the data:

$$ P(M_i \mid D) \approx w_{\text{AIC}}(M_i, D) $$ 

For the running example at hand, this would mean that we could conclude that the posterior probability of the exponential model is approximately:

```{r}
delta_AIC_power <- AIC_scores$AIC_power - AIC_scores$AIC_exponential
delta_AIC_exponential <- 0
Akaike_weight_exponential <- exp(-0.5 * delta_AIC_exponential) /
  (exp(-0.5 * delta_AIC_exponential) + exp(-0.5 * delta_AIC_power))
Akaike_weight_exponential
```

We would conclude from this approximate quantitative assessment that the evidence in favor of the exponential model is rather strong.

Our approximation is better, the more data we have. We will see a method below, the Bayesian method, which computes $P(M_i \mid D)$ in a non-approximate way.

## Likelihood-Ratio Test {#Chap-03-06-model-comparison-LR-test}

The likelihood-ratio (LR) test is a very popular frequentist method of model comparison. The LR-test assimilates model comparison to frequentist hypothesis testing. It defines a suitable test statistic and supplies an approximation of the sampling distribution. The LR-test first and foremost applies to the comparison of **nested models**, but there are results about how approximate results can be obtained when comparing non-nested models with an LR-test [@Vuong1989:Likelihood-Rati]. 

A frequentist model $M_i$ is **nested** inside another frequentist model $M_j$ iff $M_i$ can be obtained from $M_j$ by fixing at least one of $M_j$'s free parameters to a specific value. If $M_i$ is nested under $M_j$, $M_i$ is called the nested model, and $M_j$ is called the **nesting model** or the **encompassing model**. Obviously, the nested model is simpler (of lower complexity) than the nesting model.

For example, we had the two-parameter exponential model previously:

$$
\begin{aligned}
P(D = \langle k, N \rangle \mid \langle a, b\rangle) & = \text{Binom}(k,N, a \exp (-bt)), \ \ \ \ \text{where } a,b>0 
\end{aligned}
$$

An example of a model that is nested under this two-parameter model is the following one-parameter model, which fixes $a = 1.1$.

$$
\begin{aligned}
P(D = \langle k, N \rangle \mid b) & = \text{Binom}(k,N, 1.1 \ \exp (-bt)), \ \ \ \ \text{where } b>0 
\end{aligned}
$$

Here's an ML-estimation for the nested model:

```{r}
nLL_expo_nested <- function(b) {
  # calculate predicted recall rates for given parameters
  theta <- 1.1*exp(-b*t)  # one-param exponential model 
  # avoid edge cases of infinite log-likelihood
  theta[theta <= 0.0] <- 1.0e-4
  theta[theta >= 1.0] <- 1-1.0e-4
  # return negative log-likelihood of data
  - sum(dbinom(x = obs, prob = theta, size = 100, log = T))
}

bestExpo_nested <- optim(
  nLL_expo_nested, 
  par = 0.5, 
  method = "Brent", 
  lower = 0, 
  upper = 20
)
bestExpo_nested
```

The LR-test looks at the likelihood ratio of the nested model $M_0$ over the encompassing model $M_1$ using the following test statistic:

$$\text{LR}(M_1, M_0) = -2\log \left(\frac{P_{M_0}(D_\text{obs} \mid \hat{\theta}_0)}{P_{M_1}(D_\text{obs} \mid \hat{\theta}_1)}\right)$$

We can calculate the value of this test statistic for the current example as follows:

```{r}
LR_observed <- 2 * bestExpo_nested$value - 2 * bestExpo$value
LR_observed
```

If the simpler (nested) model is true, the sampling distribution of this test statistic approximates a $\chi^2$-distribution with $d$ if we have more and more data. The degrees of freedom $d$ is given by the difference in free parameters, i.e., the number of parameters the nested model fixes to specific values, but which are free in the nesting model. 

We can therefore calculate the $p$-value for the LR-test for our current example like so:

```{r}
p_value_LR_test <- 1 - pchisq(LR_observed, 1)
p_value_LR_test
```

The $p$-value of this test quantifies the evidence against the assumption that the data was generated by the simpler model. A significant test result would therefore indicate that it would be surprising if the data was generated by the simpler model. This is standardly taken as evidence in favor of the more complex, nesting model. For the current $p$-value  $p \approx `r p_value_LR_test %>% round(4)`$, there is no strong evidence against the simpler model and we would therefore rather favor the nested model, due to its simplicity; the data at hand does not seem to warrant the added complexity of the nesting model; the nested model seems to suffice.



## Bayes factors {#Chap-03-06-model-comparison-BF}

Bayes factors are the flagship Bayesian measures of comparing models. Since Bayesians do not hesitate to assign probabilities to models, parameters and hypotheses, we would ideally want to know the *absolute probability* of $M_i$ given the data: $P(M_i \mid D)$. Unfortunately, to calculate this (by Bayes rule) we would need to normalize by quantifying over *all* models. Alternatively, we look at the relative probability of a small selection of models, or, even more economic, compare only two models, preferably in terms of their odds, as follows.

Take two Bayesian models:

- $M_1$ has prior $P(\theta_1 \mid M_1)$ and likelihood $P(D \mid \theta_1, M_1)$
- $M_2$ has prior $P(\theta_2 \mid M_2)$ and likelihood $P(D \mid \theta_2, M_2)$
    
Using Bayes rule, we compute the posterior odds of models (given the data) as the product of the likelihood ratio and the prior odds.

$$\underbrace{\frac{P(M_1 \mid D)}{P(M_2 \mid D)}}_{\text{posterior odds}} = \underbrace{\frac{P(D \mid M_1)}{P(D \mid M_2)}}_{\text{Bayes factor}} \ \underbrace{\frac{P(M_1)}{P(M_2)}}_{\text{prior odds}}$$

The likelihood ratio is also called the **Bayes factor**. Formally, the Bayes factor is the factor by which a rational agent changes her prior odds in the light of observed data to arrive at updated, posterior odds. More intuitively, the Bayes factor quantifies the strength of evidence given by the data about the models of interest. It expresses this evidence in terms of the models' relative prior predictive accuracy. To see the latter, let's expand the Bayes factor as what it actually is: the ratio of marginal likelihoods.

$$
\frac{P(D \mid M_1)}{P(D \mid M_2)} = \frac{\int P(\theta_1 \mid M_1) \ P(D \mid \theta_1, M_1) \text{ d}\theta_1}{\int P(\theta_2 \mid M_2) \ P(D \mid \theta_2, M_2) \text{ d}\theta_2}
$$

Three insights are to be gained from this expansion. Firstly, the Bayes factor is a measure of how well each model would have predicted the data *ex ante*, i.e., before having seen any data. In this way, it is diametrically opposed to a concept like AIC or the LR-test, which both relied on models' maximum liklihood fits (therefore *using the data*, so being *ex post*). 

Secondly, the marginal likelihood of a model is exactly the quantity which we identified (in the context of parameter estimation) as being very hard to compute, especially for complex models. The fact that marginal likelihoods are hard to compute was the reason that we looked briefly at MCMC sampling, for example. It follows that Bayes factors are very tough nuts to crack. But, as we will see, there are very clever approaches to computing Bayes factors after all.

Thirdly, Bayes factor model comparison implicitly (and quite vigorously) punishes model complexity, but in a more sophisticated manner than just counting free parameters. To appreciate this intuitively, imagine a model with a large parameter set a very diffuse prior that spreads its probability over a wide range of parameter values. Since Bayes factors are computed based on *ex ante* predictions, a diffuse model is punished for its imprecision of prior predictions, because we integrate over all parameters (weighted by priors) and their associated likelihood. 

As for notation, we write:

$$\text{BF}_{12} = \frac{P(D \mid M_1)}{P(D \mid M_2)}$$
for the Bayes factor in favor of model $M_1$ over model $M_2$. This quantity can take on positive values, which are often translated into natural language as follows:

$BF_{12}$ | interpretation
:---:|:---:|
1 | irrelevant data
1 - 3 | hardly worth ink or breath
3 - 6 | anecdotal
6 - 10 | now we're talking: substantial
10 - 30 | strong
30 - 100 | very strong
100 + | decisive (bye, bye $M_2$!)

As $\text{BF}_{12} = \text{BF}_{21}^{-1}$ it suffices to give this translation into natural language only for values $\ge 1$.

There are at least two general approaches to calculating or approximating Bayes factors, paired here with a (non-exhaustive) list of instantiations, only some of which will be dealt with here

1. get each model's marginal likelihood 
    - grid approximation        
    - by Monte Carlo sampling   
    - brute force clever math   
    - bridge sampling 
2. get Bayes factor directly
    - Savage-Dickey method  
    - using encompassing priors
    - transdimensional MCMC 
    - supermodels

In the following, we will concern ourselves with grid approximation and MC sampling, based on the running example of memory recall as a function of time.

### Grid approximation

Grid approximation for a model's marginal likelihood works for relatively small models with, say, no more than 4-5 free parameters. Grid approximation considers discrete values for each parameter evenly spaced over the whole range of plausible parameter values, thereby approximating the integral in the definition of marginal likelihoods. 

To begin with, we need to define a prior over parameters to obtain Bayesian versions of the exponential and power model of forgetting. Here, we assume flat priors over a reasonable range of parameter values for simplicity. For the exponential model, we choose:

$$
\begin{aligned}
P(D = \langle k_i, N \rangle \mid \langle a, b\rangle, M_{\text{exp}}) & = \text{Binom}(k,N, a \exp (-bt_i)) \\
P(a \mid M_{\text{exp}}) & = \text{Uniform}(a, 0, 1.5) \\
P(b \mid M_{\text{exp}}) & = \text{Uniform}(b, 0, 1.5) 
\end{aligned}
$$

The (Bayesian) power model is then:


$$
\begin{aligned}
P(D = \langle k_i, N \rangle \mid \langle c, d\rangle, M_{\text{pow}}) & = \text{Binom}(k,N, c\ t_i^{-d}) \\
P(c \mid M_{\text{pow}}) & = \text{Uniform}(c, 0, 1.5) \\
P(d \mid M_{\text{pow}}) & = \text{Uniform}(d, 0, 1.5) 
\end{aligned}
$$

We can also express these models in code, like so:

```{r}
# prior exponential model
priorExp <- function(a, b){
  dunif(a, 0, 1.5) * dunif(b, 0, 1.5)
}
# likelihood function exponential model
lhExp <- function(a, b){
  theta <- a*exp(-b*t)
  theta[theta <= 0.0] <- 1.0e-5
  theta[theta >= 1.0] <- 1-1.0e-5
  prod(dbinom(x = obs, prob = theta, size = 100))
}

# prior power model
priorPow <- function(c, d){
  dunif(c, 0, 1.5) * dunif(d, 0, 1.5)
}
# likelihood function power model
lhPow <- function(c, d){
  theta <- c*t^(-d)
  theta[theta <= 0.0] <- 1.0e-5
  theta[theta >= 1.0] <- 1-1.0e-5
  prod(dbinom(x = obs, prob = theta, size = 100))
}
```

To approximate each model's marginal likelihood via grid approximation, we consider equally spaced values for both parameters (a tighly knit grid), asses the prior and likelihood for each parameter pair and finally take the sum over all of the visited values:

```{r}
# make sure the functions accept vector input
lhExp <- Vectorize(lhExp)
lhPow <- Vectorize(lhPow)

# define the step size of the grid
stepsize <- 0.01
# calculate the "evidence" aka marginal likelihood
evidence <- expand.grid(
  x = seq(0.005, 1.495, by = stepsize),
  y = seq(0.005, 1.495, by = stepsize)
) %>% 
  mutate(
    lhExp = lhExp(x,y), priExp = 1 / length(x),  # uniform priors!
    lhPow = lhPow(x,y), priPow = 1 / length(x)
  )
# output result
message(
  "BF in favor of exponential model: ", 
  with(evidence, sum(priExp*lhExp)/ sum(priPow*lhPow)) %>% round(2)
)
```

Based on this computation, we would be entitled to conclude that the data provides overwhelming evidence in favor of the exponential model. No matter what we believed at the outset about which model is more likely, we should adjust these beliefs by a factor of more than 1000 in favor of the exponential model. 

### Naive Monte Carlo

For simple models (with maybe 4-5 free parameters), we can also use naive Monte Carlo sampling to approximate Bayes factors. In particular, we can approximate the marginal likelihood by taking samples from the prior, calculating the likelihood of the data for each sampled parameter tuple, and then averaging over all calculated likelihoods:

$$P(D, M_i) = \int P(D \mid \theta, M_i) \ P(\theta \mid M_i) \ \text{d}\theta \approx \frac{1}{n} \sum^{n}_{\theta_j \sim P(\theta \mid M_i)} P(D \mid \theta_j, M_i)$$

Here is a calculation using one million samples from the prior of each model:

```{r}
nSamples <- 1000000
a <- runif(nSamples, 0, 1.5)
b <- runif(nSamples, 0, 1.5)
lhExpVec <- lhExp(a,b)
lhPowVec <- lhPow(a,b)
message(
 "BF in favor of exponential model: ", 
 signif(sum(lhExpVec) / sum(lhPowVec)),6 
)
```

We can also check the time course of our MC-estimate by a plot like that in Figure \@ref(fig:Chap-03-06-model-comparison-MC-estimate-time).

```{r Chap-03-06-model-comparison-MC-estimate-time, fig.cap = "Temporal development (as more samples come in) of the Monte Carlo esimtate of the Bayes factor in favor of the exponential model over the power model of forgetting."}
BFVec <- map_dbl(
  # start at 10.000 and then inspect every 500 samples
  seq(10000, nSamples, by = 500), 
  function(i){
    # what's the BF-estimate at that point in time?
    sum(lhExpVec[1:i]) / sum(lhPowVec[1:i])
  } 
)

tibble(
  i = seq(10000, nSamples, by = 500), 
  BF = BFVec
) %>% 
ggplot(aes(x = i, y = BF)) +
  geom_line() + 
  geom_hline(
    yintercept = 1221, 
    color = "firebrick"
  ) + 
  xlab("number of samples")
```

## Outlook

For more complex models (e.g., high-dimensional / hierarchical parameter spaces) naive Monte Carlo methods can be highly inefficient. If random sampling of parameter values from the priors is unlikely to deliver values for which the likelihood of the data is reasonably high, most naive MC samples will contribute very little information to the overall estimate of the marginal likelihood. For this reason, there are better sampling-based procedures which preferentially sample *a posteriori* credible parameter values (given the data) and use clever math to compensate for using the wrong distribution to sample from. This is the main idea behind approaches like [importance sampling](https://en.wikipedia.org/wiki/Importance_sampling). A very promising approach is in particular **bridge sampling**, which also has its own R package (but we will not be able to deal with this in this course) [@GronauSarafoglou2017:A-tutorial-on-b].

For many prominent models (e.g., Bayesian $t$-tests, ANOVA, linear regression), it is possible to calculate Bayes factors analytically if the right kind of priors are specified [@RouderSpeckman2009:Bayesian-t-test;RouderMorey2012:Default-Bayes-F;@GronauLy2019:Informed-Bayesi].

