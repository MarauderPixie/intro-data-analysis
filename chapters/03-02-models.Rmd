# Models {#Chap-03-03-models}

<hr>

<div style = "float:right; width:45%;">
<img src="visuals/badge-models.png" alt="badge models">
</div>

Uninterpreted data is uninformative. We cannot generalize, draw inferences or attempt to make predictions unless we make (however minimal) assumptions about the data at hand: what it represents, how it came into existence, which parts relate to which other parts, etc. One way of explicitly acknowledging these assumptions is to engage in model-based data analysis. **A statistical model is a conventionally condensed formal representation of the assumptions we make about what the data is and how it might have been generated.** In this way, model-based data analysis is more explicit about the analyst's assumptions than other approaches, such as test-based approaches, which we will encounter in Chapter \@ref(ch-03-05-hypothesis-testing). 

There is room for divergence in how to think about a statistical model, the assumptions it encodes and the truth. Some will want to reason with models using language like "if we assume that model $M$ is true, then ..." or "this shows convincingly that $M$ is likely to be the true model". Others feel very uncomfortable with such language. In times of heavy discomfort they might repeat their soothing mantra:

> All models are wrong, but some are useful.
> --- @Box1979:Robustness-in-t

To become familiar with model-based data analysis, Section \@ref(Chap-03-03-models-general) introduces the concept of a **probabilistic statistical model**. 
Section \@ref(Chap-03-03-models-parameters-priors) enlarges on the crucial aspects of parameters and priors. 
Section \@ref(Chap-03-03-models-three-pillars) discusses what goals models can help achieve. 
Section \@ref(Chap-03-03-models-representation) expands on the notation, both formulaic and graphical, which we will use in this book to communicate about models. 
Finally, Section \@ref(Chap-03-03-models-examples) discusses examples of interesting models, some of which we will frequently use in the following chapters.

```{block, type='infobox'}
The learning goals for this chapter are:

- become familiar with the notion of a (Bayesian) statistical model
- understand the key ingredients of a model:
  - likelihood function, parameters, prior, prior distribution
- understand notation to communicate models
  - formulas & graphs
```

## Statistical models {#Chap-03-03-models-general}

In its most common natural sense, a "model" is a model *of* something. It intends to represent something else in a condensed, abstract, and more practical form; where what is practical is conditioned by a given purpose. 
For any given purpose, a good model will try to represent some aspects of reality and abstract away from irrelevant features that might otherwise blur our vision.
The most common purpose of a statistical model is to either learn something about reality by drawing inferences from data - possibly with the ulterior goal of making an informed practical decision or to make predictions about unknown events (future, present or past unknowns).

A statistical model $M$ is a model of a random process $R$ that could have generated some kind of observable data that we are interested in.^[In most common parlance, however, we often speak of "a model of the data" or of "modeling the data", but this is only sloppy slang for "a model of (what we assume is) a random process that could generate data of the relevant kind".]
The model $M$ is then a formally precise formulation of our assumptions about this random process $R$. 

Often, we want to explain some part of our data observations, the dependent variable(s) $D_{\text{DV}}$, in terms of some other observations, the independent variables $D_{\text{IV}}$ (see Chapter \@ref(Chap-02-01-data-variables) for more on the notion of (in-)dependent variables).
But it is also possible that there are no independent variables in terms of which we would like to model the dependent variable $D_{\text{DV}}$.

A model $M$ for data $D$ fixes a **likelihood function** for $D_\text{DV}$. The likelihood function determines how likely any potential data observation $D_\text{DV}$ is, given the corresponding observations in $D_\text{IV}$. Most often, the likelihood function also has **free parameters**, represented by a parameter vector $\theta$. The basic (and yet rather uninformative) notation for a likelihood function of model $M$ for data $D$ with parameter vector $\theta$ is therefore:

$$ P_M(D_\text{DV} \mid D_\text{IV}, \theta) $$ 

*Bayesian models* have an additional component, namely a **prior distribution** over parameter values, commonly written as:

$$ P_M(\theta) $$.

The Bayesian prior over parameter values can be used to regularize inference and/or to represent any motivated and justifiable *a priori* assumptions about parameter values that are plausible given our knowledge so far. Section \@ref(Chap-03-03-models-parameters-priors) elaborates on parameters and priors.
But first, we should take a look at an example, which we will use in the remainder of this chapter for further illustration.

<div class="infobox">

**Example: Binomial Model.** 
The data we are interested in comes from a sequence of flips of a coin with bias $\theta \in [0;1]$. 
We have observed that $k$ of the $N$ flips turned out to be heads. 
We know $N$ and $k$, but we do not know $\theta$.
We will use the Binomial Model in later sections to infer the latent (= not directly observable) coin bias $\theta$.

The coin's bias $\theta$ is the only parameter of this model.
The dependent variable is $k$.
$N$ is another data observation (treated here as an independent variable^[It is fair to treat $N$ as an independent variables if it was determined at the beginning of the experiment (=sequence of flips), so that the only dependent measure is the number $k$ of head outcomes for fixed $N$.]).

The likelihood function for this model is the [Binomial distribution](#app-91-distributions-binomial):

$$ P_M(k \mid \theta, N) = \text{Binomial}(k, N, \theta) = \binom{N}{k}\theta^k(1-\theta)^{N-k} $$

For reasons that will become clear later, we use a [Beta distribution](#app-91-distributions-beta) for the prior of $\theta$.
For example, we can use parameters so that the ensuing distribution is flat (a so-called "uninformative prior"; more on this below):

$$ P_M(\theta) = \text{Beta}(\theta, 1, 1) $$ 

</div>

There are three main uses for models in statistical data analysis:

1. **Prediction**: Models can be used to make predictions about future or hypothetical data observations. We will see an example of this is Section \@ref(Chap-03-03-models-parameters-priors) in this chapter. 
2. **Parameter estimation**: Based on model $M$ and data $D$, we try to infer which value of the parameter vector $\theta$ we should believe in or work with (e.g., base our decision on). Parameter estimation can also serve knowledge gain, especially if (some component of) $\theta$ is theoretically interesting. We will deal with parameter estimation in Chapter \@ref(ch-03-04-parameter-estimation).
3. **Model comparison**: If we formulate at least two alternative models, we can ask which model better explains or better predicts some data. In some of its guises, model comparison helps with the question of whether a given data set provides evidence in favor of one model and against another other, and if so, how much. Model comparison is the topic of Chapter \@ref(Chap-03-06-model-comparison).


## Notation & graphical representation {#Chap-03-03-models-representation}

If it is important to communicate the assumptions underlying a statistical argument, and if models are means of making formally explicit these assumptions, then it follows that efficient communication of models is important too. We here follow the common practice of representing models using a special purpose formulaic notation and, where useful, a graph-based visual display in which probabilistic dependencies are lucidly represented.

Recall that the *Binomial Model* has a binomial likelihood function:

$$ P_M(k \mid \theta, N) = \text{Binomial}(k, N, \theta) = \binom{N}{k}\theta^k(1-\theta)^{N-k} $$

And a Beta distribution as a prior, e.g., with shape parameters set so that all values of $\theta$ are equally likely. 

$$ P_M(\theta) = \text{Beta}(\theta, 1, 1) $$ 

### Formula notation

To concisely represent models, we use a special notation, which is very intuitive when we think about sampling. Instead of the above notation for the prior we write:

$$ \theta \sim \text{Beta}(1,1) $$ 

The symbol "$\sim$" is often read as "is distributed as". You can also think of it as meaning that $\theta$ is sampled from a $\text{Beta}(1,1)$ distribution distribution.

Similarly, for the likelihood function, we just write:

$$k \sim \text{Binomial}(\theta, N).$$

### Graphical notation

When models get very complex and incorporate many parameters it can be difficult to tease out the relations between all of the model's components. 
In such a situation a graphical notation of a model is helpful. 
We here adopt the conventions described by @leeWagen2014.
We represent every relevant variable as a node in a directed acyclic graph structure (a probabilistic network).
The graph structure is used to indicate dependencies between the variables, with children depending on their parents.
In visualizing this, we use the following general conventions:

- known or unknown (= latent) variable
  - *shaded nodes*: observed variables
  - *unshaded nodes*: unobserved / latent variables
  
- kind of variable: 
  - *circular nodes*: continuous variables
  - *square nodes*: discrete variables
  
- kind of dependency:
  - *single line*: stochastic dependency
  - *double line*: deterministic dependency

For the Binomial Model this results in the relevant variables:

- number of trials ($N$)
- number of success ($k$)
- probability for success ($\theta$)

Of these $N$ and $k$ are observed and discrete variables, and $\theta$ is a latent continuous variable. Clearly, the number of heads $k$ depends on the coin bias $\theta$ as well as on the number of trials $N$. This yields a graphical and formulaic notation as in Figure \@ref(fig:ch-03-02-Binomial-Model).

```{r ch-03-02-Binomial-Model, echo = F, fig.cap="The Binomial Model. Notice that any specific Beta prior shape would yield what we here call a Binomial Model, which is why there are no concrete shape parameters given in this graph.", out.width = '40%'}
knitr::include_graphics("visuals/binomial-model.png")
```


## Parameters, priors, and prior predictions {#Chap-03-03-models-parameters-priors}

We defined a Bayesian model as a pair consisting of a parameterized likelihood function and a prior distribution over parameter values:

$$ 
\begin{aligned}
  & \text{Likelihood: } & P_M(D \mid \theta) \\ 
  & \text{Prior: } & P_M(\theta) 
\end{aligned}
$$ 

In this section we dive deeper into what a parameter is, what a prior distribution $P_M(\theta)$ is, and how we can use a model to make predictions about data.

The running example for this section is the *Binomial Model* as introduced above. 
As a concrete example of data, we consider a case with $N=24$ flips and $k=7$ head outcomes.

### What's a model parameter?

A model parameter is a value that the likelihood depends on. 
In the graphical notation we introduced in Section \@ref(Chap-03-03-models-representation), parameters usually (but not necessarily) show up as white nodes, because they are unknowns.

For example, the single parameter $\theta$ in the Binomial Model shapes or fine-tunes the likelihood function.
Remember that the likelihood function for the Binomial Model is:

$$ P_M(k \mid \theta, N) = \text{Binomial}(k, N, \theta) = \binom{N}{k}\theta^k(1-\theta)^{N-k} $$

To understand the role of the parameter $\theta$, we can plot the likelihood of the observed data (here: $k=7$ and $N=24$) as a function of $\theta$. 
This is what is shown in Figure \@ref(fig:ch-03-02-LH-Binomial-Model).
For each logically possible value of $\theta \in [0;1]$ on the horizontal axis, Figure \@ref(fig:ch-03-02-LH-Binomial-Model) plots the resulting likelihood of the observed data on the vertical axis.
What this plot shows is how the likelihood function depends on its parameter $\theta$.
Different values of $\theta$ make the data we observed more or less likely.


```{r ch-03-02-LH-Binomial-Model, fig.cap= "Likelihood function for the Binomial Model, for $k=7$ and $N=24$.", echo = F}
# data
k <- 7
N <- 24

# plot LH-function
tibble(
  theta = seq(0,1, length.out = 401),
  LH = dbinom(x = k, size = N, prob = theta)
) %>% 
  ggplot(aes(x = theta, y = LH)) +
  geom_line(size = 3) +
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("likelihood $Binomial(k=7, N=24, \\theta)$")
  )
```


<div class = "exercises">
**Exercise 8.3** 

a. Use R to calculate how likely is it to get $k=22$ heads when tossing a coin with bias $\theta = 0.5$ a total of $N=100$ times.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

```{r}
dbinom(22, size = 100, prob = 0.5, log = FALSE)
```

</div>
</div>

b. Which parameter value, $\theta = 0.4$ or $\theta = 0.6$, makes the data from the previous part of this exercise ($N=100$ and $k=22$) more likely? - Give a reason for your intuitive guess and use R to check your intuition. 

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

The number of heads $k=22$ is (far) less than half of the total number of coin flips $N=100$. This should be more likely for a bias towards tails than for a bias towards heads. So, we might assume that $\theta=0.4$ makes the data more likely than $\theta = 0.6$.

```{r}
dbinom(22, size = 100, prob = 0.4, log = FALSE)
dbinom(22, size = 100, prob = 0.6, log = FALSE)
```

</div>
</div>

</div>

### Priors over parameters {#Chap-03-02-models-priors}

The prior distribution over parameter values $P_M(\theta)$ is an integral part of a model when we adopt a Bayesian approach to data analysis. This entails that two (Bayesian) models can share the same likelihood function, and yet ought to be considered as different models. (This also means that, when we say "Binomial Model" we really mean a whole class of models all varying in the prior on $\theta$.) 

In Bayesian data analysis priors $P_M(\theta)$ are most saliently interpreted as encoding the modeler's prior beliefs about the parameters in question. 
Ideally, the beliefs that support the specification of a prior should be supported by an argument, results of previous research, or other justifiable motivations. 
However, informed subjective priors are just one of the ways to justify priors over parameters.

There are three main types of motivations for priors $P_M(\theta)$; though the choice of a particular prior for a particular application might have mixed motives. 

1. **Subjective priors** capture the modeler's genuine subjective beliefs in the sense described above.
2. **Practical priors** are priors that are used pragmatically because of their specific usefulness, e.g., because they simplify a mathematical calculation or a computer simulation, or because they help in statistical reasoning, such as when *skeptical priors* are formulated that work against a particular conclusion.
3. **Objective priors** are priors that, as some argue, *should* be adopted for a given likelihood function to avoid conceptually paradoxical consequences. **We will not deal with objective priors in this introductory course beyond mentioning them here for completeness.**

Orthogonal to the kind of motivation given for a prior, we can distinguish different priors based on how strongly they commit the modeler to a particular range of parameter values. The most extreme case of ignorance are **uninformative priors** which assign the same level of credence to all parameter values. Uninformative priors are also called *flat priors* because they express themselves as flat lines for discrete probability distributions and continuous distributions defined over an interval with finite lower and upper bounds.^[It is possible to use uninformative priors also for continuous distributions defined over an unbounded interval, in which case we speak of *improper priors* (to remind ourselves that, mathematically, we are doing something tricky).] Informative priors, on the other hand, can be *weakly informative* or *strongly informative*, depending on how much commitment they express. The most extreme case of commitment would be expressed in a **point-valued prior**, which puts all probability (mass or density) on a single value of a parameter. Since this is no longer a respectable probability distribution, although it satisfies the definition, we speak of a *degenerate prior* here. 

Figure \@ref(fig:ch-03-02-models-types-of-priors) shows examples of uninformative, weakly or strongly informative priors, as well as point-valued priors for the Binomial Model. The priors shown here (resulting in four different Bayesian models all falling inside the family of Binomial Models) are as follows:

- *uninformative* : $\theta \sim \text{Beta}(1,1)$
- *weakly informative* : $\theta \sim \text{Beta}(5,2)$
- *strongly informative* : $\theta \sim \text{Beta}(50,20)$
- *point-valued* : $\theta \sim \text{Beta}(\alpha, \beta$ with $\alpha, \beta \rightarrow \infty$ and $\frac{\alpha}{\beta} = \frac{5}{2}$ 


```{r ch-03-02-models-types-of-priors, echo = F, fig.cap = "Examples of different kinds of Bayesian priors for coin bias $\\theta$ in the Binomial Model."}

tibble(
  theta = seq(0, 1, length.out = 401),
  uninformative = dbeta(theta, 1, 1),
  `weakly informative` = dbeta(theta, 5, 2),
  `strongly informative` = dbeta(theta, 50, 20),
  `point-valued` = dbeta(theta, 50000, 20000)
) %>% 
  pivot_longer(
    cols = -1,
    names_to = "prior_type",
    values_to = "prior"
  ) %>%
  mutate(
    prior_type = factor(prior_type, levels = c('uninformative', 'weakly informative', 'strongly informative', 'point-valued'))
  ) %>%
  ggplot(aes(x = theta, y = prior)) +
  geom_line(size = 2) +
  facet_wrap(~ prior_type, ncol = 2, scales = "free") +
  labs(
    x = latex2exp::TeX("Coin bias $\\theta$"),
    y = latex2exp::TeX("Prior probability $P_{M_i}(\\theta)$"),
    title = latex2exp::TeX("Different kinds of priors over bias $\\theta$ (Binomial Model family).")
  ) 

```

### Prior predictions

How should priors be specified for a Bayesian model?
Several aspects might inform this decision.
Practical considerations may matter (maybe the model can only be implemented and run with common software for certain priors).
If subjective beliefs play a role, it may be hard to specify an exact shape of the prior distribution over some or all parameters, especially when these parameters are not easily interpretable in an intuitive way.
Therefore, two principles for the specification of priors are important:

1. **Sensitity analysis:** Researchers should always check dilligently whether or how much their results depend on the specific choices of priors, e.g., by running the same analysis with a wide range of different priors.
2. **Inspecting the prior predicitve distribution:** It is one thing to ask whether a particular value for some parameter makes intuitive or conceptual sense. It is another at least as important question whether the predictions that the model makes about the data are intuitively or conceptually reasonable from an *a priori* perspective.^[Obviously, priors should not be chosen _after_ having seen the data in such a way that they engineer in the conclusions that a researcher wants to reach because of prior conviction, pride or ambition.]

Indeed, by specifying priors over parameter values, Bayesian models make predictions about how likely a particular data outcome is, even before having seen any data at all. 
The (Bayesian) **prior predictive distribution** of model $M$ is a probability distribution over future or hypothetical data observations, written here as $D_{\text{pred}}$ for "predicted data":

$$ 
\begin{aligned}
P_M(D_{\text{pred}})  & = \sum_{\theta} P_M(D_{\text{pred}} \mid \theta) \ P_M(\theta)  && \text{[discrete parameter space]} \\
P_M(D_{\text{pred}})  & = \int P_M(D_{\text{pred}} \mid \theta) \ P_M(\theta) \ \text{d}\theta  && \text{[continuous parameter space]}
\end{aligned}
$$

The formula above is obtained by marginalization over parameter values (represented here as an integral for the continuous case). 
We can think of the prior predictive distribution also in terms of samples.
We want to know how likely a given logically possible data observation $D_{\text{pred}}$ is, according to the model with its _a priori_ distribution over parameters.
So we sample, repeatedly, parameter vectors $\theta$ from the prior distribution.
For each sampled $\theta$, we then sample a potential data observation $D_{\text{pred}}$.
The prior predictive distribution captures how likely it is under this sampling process to see each logically possible data observation $D_{\text{pred}}$. -
Notice that this sampling process corresponds exactly to the way in which we write down models using the conventions layed out in Section \@ref(Chap-03-03-models-representation), underlining once more how a model is really a representation of a random process that could have generated the data.

In the case of the Binomial Model when we use a Beta prior over $\theta$, the prior predictive distribution is so prominent that it has its own name and fame. It's called the [Beta-binomial distribution](#app-91-distributions-beta-binomial). Figure \@ref(fig:ch-03-02-models-prior-predictives-binomial) shows the prior predictions for the four kinds of priors from Figure \@ref(fig:ch-03-02-models-types-of-priors) when $N = 24$.

```{r ch-03-02-models-prior-predictives-binomial, echo = F, fig.cap = "Prior predictive distributions for Binomial Models with the Beta-priors from the previous figure."}
trials <- 24
tibble(
  x = seq(from = 0, to = 24),
  `point-valued` = dbbinom(x, size = trials, alpha = 500000, beta = 200000),
  uninformative = dbbinom(x, size = trials, alpha = 1, beta = 1),
  `weakly informative` = dbbinom(x, size = trials, alpha = 5, beta = 2),
  `strongly informative` = dbbinom(x, size = trials, alpha = 50, beta = 20)
) %>% 
  pivot_longer(
    cols = -1,
    names_to  = "prior_type",
    values_to = "prior"
  ) %>% 
    mutate(
    prior_type = factor(prior_type, levels = c('uninformative', 'weakly informative', 'strongly informative', 'point-valued'))
  ) %>%
  ggplot(aes(x = x, y = prior)) +
  geom_col() +
  facet_wrap(~ prior_type, ncol = 2, scales = "free") +
  labs(
    x = latex2exp::TeX("Number of heads $k$"),
    y = latex2exp::TeX("Prior predictive probability $P_{M_i}(k)$"),
    title = latex2exp::TeX("Prior predictives for different Binomial Models.")
  ) 

```


