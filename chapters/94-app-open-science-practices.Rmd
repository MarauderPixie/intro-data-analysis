
# Open science practices {#app-94-open-science}

This chapter is dedicated to reflecting on open science practices regarding data collection, analysis and sharing the results with the scientific community. To motivate why this is necessary, we start off in chapter \@ref(app-94-replication-crisis) by looking at how publication bias, questionable research practices, inflated error rates, and the lack of transparency threaten the credibility of psychological science. This chapter will be somewhat pessimistic. However, there is no need to despair! In chapter \@ref(app-94-remedies), we talk about measures to solve (or at least reduce) the problems from the previous chapter. Chapter \@ref(app-94-recap) provides an overview of what everybody can do to embrace transparent, open, and replicable research.

## Psychology's replication crisis {#app-94-replication-crisis}

What happens with a scientific discipline if it predominantly fails to replicate^[*Direct replication* is the repetition of an experiment as close as possible to the original methodology. *Replicability* is the ability to obtain the same results as the original experiment on new data. This term is often interchangeably used with *reproducibility*, which is the ability to obtain the same results as the original experiment on the same data.] previous discoveries? This question frequently arose after a groundbreaking project revealed that psychology is facing a replication crisis. In 2011, the @OpenScienceCollab2015 launched a large-scale project – the so-called “Reproducibility Project” – in which they attempted 100 direct replications of experimental and correlational studies in psychology. The results are worrisome: 97% of the original studies reported statistically significant results, whereas the initiative could merely replicate 36% of the results.^[There are several methods to assess the replicability of a study result. The Reproducibility Project separately evaluated the replication success – among others – based on a) statistical significance ($p < 0.05$) b) inclusion of original effect sizes within the 95% CI of replication effect sizes c) magnitude between effect sizes d) subjective assessment (“Did it replicate?”). Here, replication success is only evaluated based on statistical significance.] This low replicability rate, however, does not imply that about two-thirds of the discoveries are wrong. It emphasizes that research outcomes should not be taken at face value but scrutinized by the scientific community. Most of all, the results show that scientists should take action to increase the replicability of their studies. This urgent need is further fueled by the discovery that the prevalence of low replicability rates diminishes the public’s trust [e.g., @WingenBerkessel2019] and, in the long run, might undermine the credibility of psychology as a science. 

In order to know how to increase a study’s replicability, it is crucial to investigate what causes replications to fail. Essentially, failing to replicate the significant results of the original study has three roots: The original study yielded a false-positive, the replication study yielded a false-negative, or too divergent methodologies led to two different outcomes [@OpenScienceCollab2015]. We focus here on false-positives and diverging methodologies. We only briefly touch on false-negatives in the replication study when we talk about low statistical power.

### Publication bias, QRP's, and false-positives

Weighing evidence in favor of verifying preconceptions and beliefs rather than falsifying them is a cognitive bias (confirmation bias). This natural form of reasoning can be a considerable challenge in doing proper research, as the full amount of information should be taken into account and not just those consistent with prior beliefs. Confirmation bias also manifests itself in a tendency to see patterns in the data and perceive meaning, when there is only noise (apophenia) and overestimating the prediction of an event after it occurred, typically expressed as “I knew it all along!” (hindsight bias). 

These biases further pave the way for a skewed incentive structure that prefers confirmation over inconclusiveness or contradiction. In psychological science, there is a vast prevalence of publications that report significant ($p < 0.05$) and novel findings in contrast to null-results [e.g., @Sterling1959] or replication studies [e.g., @MakelPlucker2012]. This substantial publication bias towards positive and novel results may initially seem entirely plausible. Journals might want to publish flashy headlines that catch the reader’s attention rather than “wasting” resources for studies that remain inconclusive. Furthermore, scientific articles that report significant outcomes are more likely to be cited [@DuyxUrlings2017] and thus may increase the journal’s impact factor (JIF). Replication studies might not be incentivized because they are considered tedious and redundant. Why publish results that don't make new contributions to science?^[This way of thinking might be the reason why direct replications were virtually replaced by *conceptual replications*, which is the replication of a basic idea from previous research, albeit with different experimental methods to preserve novelty.]

Publication bias operates at the expense of replicability and thus the reliability of science. The pressure of generating significant results can further fuel the researcher’s bias [@Fanelli2010]. Increasing cognitive biases towards the desired positive result could therefore lead researchers to draw false conclusions. To cope with this “Publish or Perish” mindset, researchers may increasingly engage in **questionable research practices** (QRP’s) as a way of somehow achieving a $p$-value that is under the significance level $\alpha$. QRP’s fall into the grey area of research and might be the norm in psychological science. Commonly researchers “$p$-hack” their way to a significant $p$-value by analyzing the data multiple ways through exploiting the flexibility in data collection and data analysis (researcher degrees of freedom). This exploratory behavior is frequently followed by selective reporting of what “worked”, so-called cherry-picking. $p$-hacking also takes on the form of unreported omission of statistical outliers and conditions, post hoc decisions to analyze a subgroup, or to change statistical analyses. Furthermore, researchers make rounding errors by reporting their results to cross the significance threshold (.049 becomes .04), they randomly stop collecting data when the desired $p$-value of under .05 pops up, or they present exploratory hypotheses^[An exploratory hypothesis is one that was generated after the data was inspected, that is, *explored* for unanticipated patterns. In contrast, a confirmatory hypothesis is one that is defined *before* data collection. Both types are important: generating theories by exploring the data and testing them as confirmatory hypotheses in a new experiment. However, it gets fraudulent when passing serendipitous findings off as being predicted in advance.] as being confirmatory (HARKing, Hypothesizing After the Results are Known).

Diederik Stapel, a former professor of social psychology at Tilburg University, writes in his book *Faking Science: A True Story of Academic Fraud* about his scientific misconduct. He shows how easy it is to not just fool the scientific community (in a discipline where transparency is not common practice) but also oneself:

> I did a lot of experiments, but not all of them worked. […] But when I really believed in something […] I found it hard to give up, and tried one more time. If it seemed logical, it must be true. […] You can always make another couple of little adjustments to improve the results. […] I ran some extra statistical analyses looking for a pattern that would tell me what had gone wrong. When I found something strange, I changed the experiment and ran it again, until it worked [@Stapel2014, pp. 100-101].

If the publication of a long-standing study is deciding whether researchers get funding or a job – and that means food – then it is somehow understandable why they consciously or subconsciously engage in such practices. However, exploiting researcher degrees of freedom by engaging in QRP’s poses a significant threat to the validity of the scientific discovery by blatantly inflating the probability of false-positives. By not correcting the significance threshold accordingly, many analyses are likely to be statistically significant just by chance, and reporting solely those that “worked” additionally paints a distorted picture on the confidence of the finding. 

```{block, type='infobox'}

<div style = "float:right; width:15%;">
  <img src="visuals/green-jelly-bean.png">
</div>

**Example.** Let's illustrate $p$-hacking based on a [popular comic by xkcd](https://www.explainxkcd.com/wiki/index.php/882:_Significant). In the comic, two researchers investigate whether jelly beans cause acne - a weird research question to begin with. A $p$-value of above 0.05 doesn't allow the researchers to reject the null hypothesis of no relationship. Instead of concluding that they did not find a link between jelly beans and acne, they decide to distinguish between the 20 different colors of jelly beans. For 19 colors, they still don't observe statistically significant results - except for green jelly beans. In a research paper, they report that green jelly beans are linked to acne at a 95% confidence level. 

However, it is very likely that this finding was pure coincidence! Let's check what happened:

In the first experiment (without color distinctions), there was an upper bound of 5% chance to erroneously reject $H_0$. In the case of color distinctions, the confidence level dropped to $0.95^{20} \approx 35.85\%$, leaving room for a 64.15% chance that the significant $p$-value for green jelly beans fell prey to a false-positive. The probability of encountering at least one false-positive due to conducting multiple hypothesis tests on the same data set is called the *family-wise error rate* (FWER). Formally, it can be calculated like so:

$$\alpha_{FWER} = 1 - (1 - \alpha)^n,$$

where $\alpha$ denotes the "per-contrast" error rate, which is conventionally set to $\alpha = 0.05$, and $n$ the total number of comparisons, i.e., statistical analyses on the same data set.

Conducting multiple tests on the same data set and not correcting the family-wise error rate accordingly, therefore makes it more likely that a study finds a statistically significant result by chance.
```

To investigate how prevalent QRP’s are in psychological science, Leslie John et al. [@JohnLoewenstein2012] surveyed over 2000 psychologists regarding their engagement in QRP’s. They found that 66.5% of the respondents admitted that they failed to report all dependent measures, 58% collected more data after seeing whether the results were significant, 50% selectively reported studies that “worked”, and 43.4% excluded data after looking at the impact of doing so. Based on the self-admission estimate, they derived a prevalence estimate of 100% for each mentioned QRP. These numbers once more reinforce the suspicion that QRP’s are the norm in psychology. Together with the fact that these practices can blatantly inflate the false-positive rates, one might conclude that much of the psychological literature cannot be successfully replicated and thus *might* be wrong.

### Low statistical power

Another factor that can account for unreliable discoveries in the scientific literature is the persistence of highly underpowered studies in psychology [e.g., @Cohen1962; @SedlmeierGigerenzer1989; @BakkerVanDijk2012; @SzucsIoannidis2017]. A study’s statistical power is the probability of correctly rejecting a false null-hypothesis, i.e., the ideal in NHST. Defined as $1 − \beta$, power is directly related to the probability of encountering a false-negative, meaning that low powered studies are less likely to find a true effect.^[*True effect* means that the detected difference is really present in the population. The *effect size* is a quantification of the difference between the two groups investigated.] Figure \@ref(fig:ch-94-error-dists) shows the relationship between $\alpha$-errors and $\beta$-errors (repeated from Chapter \@ref(ch-03-05-hypothesis-testing-3-approaches)), as well as the power to correctly rejecting $H_0$.

```{r ch-94-error-dists, echo = F, fig.cap="Illustration of power, $\\alpha$-and $\\beta$-errors."}
plot_data %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
  # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.02,
      yend = dnorm(0, sd = 2)
    ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 0,
      y = -0.025,
      label = "H_0"
    ),
    color = "darkgray",
    label.size = 0,
    size = 6
  ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x = 4,
      xend = 4,
      y = -0.02,
      yend = dnorm(0, sd = 2)
    ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 4,
      y = -0.025,
      label = "H_a"
    ),
    color = "darkgray",
    label.size = 0,
    size = 6
  ) +
  # power area
  geom_area(
    data = filter(
      plot_data, name == "alternative", 
      x >= qnorm(0.95, sd = 2)
    ), 
    fill = "firebrick",
    color = "firebrick",
    alpha = 0.3
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data, name == "null", 
      x >= qnorm(0.95, sd = 2)
    ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data, name == "alternative", 
      x <= qnorm(0.95, sd = 2)
    ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # Power explanation
  annotate(
    geom ='text', 
    x = 10, 
    y = 0.2, 
    label = ("Power"), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
    geom_segment(
    aes(
      x = 4.3,
      xend = 10,
      y = 0.105,
      yend = 0.19
    ),
    color = "black"
  ) +
  # alpha error explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.1, 
    label = TeX("$\\alpha$-error", output='character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 4.3,
      xend = 10,
      y = 0.005,
      yend = 0.09
    ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom='text', 
    x = -6, 
    y = 0.1, 
    label = TeX("$\\beta$-error", output='character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = -0.3,
      xend = -6,
      y = 0.005,
      yend = 0.09
    ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  theme(
      axis.text = element_blank(),
      axis.ticks = element_blank()
  )
```

It may be tempting to conclude that a statistically significant result of an underpowered study is "more convincing". However, low statistical power also decreases the probability that a significant result is a true effect, referred to as the *Positive Predictive Value* (PPV). The PPV is defined as $$PPV = \frac{(1 - \beta) \cdot R}{(1 - \beta) \cdot R + \alpha},$$ where $1 − \beta$ is the statistical power, $R$ is the pre-study odds (the prior probability of a true effect), and $\alpha$ is the type-I-error. Assuming $R$ to be 25%, the PPV for a statistically significant result of a study with 80% power – which is deemed acceptable – is 0.8. If the power is reduced to 35%, the PPV is 0.64. A 64% chance of a discovered effect to be true implies that there is a 36% chance of making a false discovery. Therefore, low powered studies are more likely to obtain flawed and unreliable outcomes, which contribute to the poor replicability of discoveries in the scientific record.

Another consequence of underpowered studies is the overestimation of effect sizes and a higher probability of an effect size in the wrong direction. Those errors are referred to as **Type M (Magnitude)** and **Type S (sign) errors**, respectively [@GelmanCarlin2014]. If, for example, the true effect size (which is unknown in reality) between group $A$ and $B$ is 20 ms, finding a significant effect size of 50 ms would overestimate the true effect size by a factor of 2.5. Observing an effect size of -50 ms would even let us falsely assume that group $B$ performed faster than group $A$.

These error rates can be estimated through simulation. Suppose that we want to conduct a two-tailed, one-sample $t$-test to investigate whether the measurements in a group differ from a known mean. We set $H_0: \delta = 0$ and $H_a: \delta \neq 0$. We plan on recruiting 25 participants. 

Before we start with the real experiment, we check its power, Type S, and Type M error rates by hypothetically running the same experiment 20000 times in the WebPPL code box below. From the previous literature, we estimate the true effect size to be 1 and the standard deviation to be 15. Since we want to know how many times we correctly reject the null hypothesis of equal means, we set the true effect size as ground truth (`delta` variable). Variable `t_crit` stores the demarcation point for statistical significance in a $t$-distribution with `n` - 1  degrees of freedom.

We address the following questions:

* If the true effect size is 1, what is the probability of correctly rejecting the null hypothesis of equal means (i.e., an effect size of 0)?
* If the true effect size is 1, what is the probability that a significant result will reflect a negative effect size?
* If the true effect size is 1 and we obtain a statistically significant result, what is the ratio of the estimated effect size to the true effect size (exaggeration ratio)?

Play around with the parameter values to get a feeling of how power can be increased. Remember to change the `t_crit` variable when choosing a different sample size. The critical $t$-value can be easily looked up in a $t$-table or computed with the respective quantile function in R (e.g, `qt(c(0.025,0.975), 13)` for a two-sided test with $\alpha = 0.05$ and $n = 14$). For $n \geq 30$, the $t$-distribution approximates the standard normal distribution.

<pre class="webppl">
var delta = 1;         // true effect size between the population and H_0
var sigma = 15;        // standard deviation
var n = 25;            // sample size per experiment
var t_crit = 2.063899; // +- critical t-value for n-1 degrees of freedom
var n_sim = 20000;     // number of simulations (1 simulation = 1 experiment)
///fold:

var se = sigma/Math.sqrt(n); // standard error

// Effect size estimates:

/* drep(n_sim) takes an integer representing the number of simulations. For each 
simulation, it takes n samples from a normal distribution centered around the true 
effect size and returns a vector of the sample means */
var drep = function(n_sim) {
  if(n_sim == 1) {
    var sample = repeat(n, function(){gaussian({mu: delta, sigma: sigma})});
    var mean_drep = [_.mean(sample)];
    return mean_drep;
  } else {
    var sample = repeat(n, function(){gaussian({mu: delta, sigma: sigma})});
    var mean_drep = [_.mean(sample)];
    return mean_drep.concat(drep(n_sim-1));
  }
}

// vector of all effect sizes
var ES = drep(n_sim);

// Power:

/* check_signif(n_sim) takes an integer representing the number of simulations and 
returns a vector of only significant effect sizes. It calculates the observed 
t-value for each effect size estimate and compares it with the critical t-value. 
If the observed absolute t-value is larger than the critical t-value, the difference 
in means is statistically significant.

Note that we take the absolute t-value since we're conducting a two-sided t-test and
therefore have to consider values that are also in the lower tail of the sampling 
distribution. */
var check_signif = function(n_sim) {
  if(n_sim == 1) {
   var t_obs = Math.abs(ES[0]/se);
    if(t_obs > t_crit) {
      return [ES[0]];
    } else {
      return [];
    }
  } else {
    var t_obs = Math.abs(ES[n_sim-1]/se);
     if(t_obs > t_crit) {
       return [ES[n_sim-1]].concat(check_signif(n_sim-1));
    } else {
      return [].concat(check_signif(n_sim-1));
    }
  }
}

// vector of only significant effect size estimates
var signif_ES = check_signif(n_sim);

// proportion of times where the null hypothesis would have been correctly rejected
var power = signif_ES.length/n_sim;

// Type S error:

/* type_s(n_sim) takes an integer representing the number of simulations and returns
a vector of significant effect sizes that are negative. */
var type_s = function(n_sim){
  if(n_sim == 1){
    if(signif_ES[n_sim-1] < 0){
      return [signif_ES[n_sim-1]];
    } else {
      return [];
    }
  } else {
    if(signif_ES[n_sim-1] < 0){
      return [signif_ES[n_sim-1]].concat(type_s(n_sim-1));
    } else {
      return [].concat(type_s(n_sim-1));
    }
  }
}

// vector of only significant effect size estimates that are negative
var neg_ES = type_s(n_sim);

/* If at least one simulation yielded statistical significance, calculate the
proportion of significant+negative effect sizes to all significant effect sizes. */
var mean_type_s = function(){
 if(signif_ES.length == 0) {
      return "No significant effect size found";
    } else {
      return neg_ES.length/signif_ES.length;
    } 
}

// proportion of significant results having a negative effect size
var type_s = mean_type_s();

// Type M error:

// take the absolute value of all significant effect sizes
var absolute_ES = _.map(signif_ES, Math.abs);

/* If at least one simulation yielded statistical significance, calculate 
the ratio of the estimated effect size to the true effect size. */
var mean_type_m = function(){
 if(signif_ES.length == 0) {
      return "No significant effect size found";
    } else {
      return _.mean(absolute_ES)/delta; 
    } 
}

// exaggeration ratio
var type_m = mean_type_m();

// Results:

// print results
console.log("Power: " + power + 
            "\nType II error: " + (1-power) + 
            "\nType S error: " + type_s +
            "\nType M error: " + type_m
           );

// print interpretation depending on results
if(power != 0) {
  if(_.round(type_m,1) == 1) {
    console.log(
    "Interpretation:\n" +
     // Power
     "If the true effect size is " + delta + ", there is a " + 
     _.round((power*100),2) + "% chance of detecting a significant difference. " +
     // Type S error
     "If a significant difference is detected, there is a " + 
     _.round((type_s*100),2) +
     "% chance \nthat the effect size estimate has the wrong sign. " + 
     // Type M error
     "Further, the absolute estimated effect size is expected to be about the " +
     "same as the true effect size of " + delta + "."
  );
  } else {
    console.log(
    "Interpretation:\n" +
     // Power
     "If the true effect size is " + delta + ", there is a " + 
     _.round((power*100),2) + "% chance of detecting a significant difference. " +
     // Type S error
     "If a significant difference is detected, there is a " + 
     _.round((type_s*100),2) +
     "% chance \nthat the effect size estimate has the wrong sign. " + 
     // Typ M error
     "Further, the absolute estimated effect size is expected to be on average " +
     _.round(type_m,1) + " times too high."
  );
  }
} else {
  console.log(
    "Interpretation:\n" +
    // Power
    "If the true effect size is " + delta + ", there is no " +
    "chance of detecting a significant \ndifference at all. " + 
    "Since Type S and Type M errors are conditioned on a \nsignificant " + 
    "result, there is no chance of encountering them in this case."
  );
}
///
</pre>

<pre class=" CodeMirror-line " role="presentation">
</pre>

<script>
// find all <pre> elements and set up the editor on them
var preEls = Array.prototype.slice.call(document.getElementsByClassName("webppl"));
preEls.map(function(el) { console.log(el); editor.setup(el, {language: 'webppl'}); });
</script>

As the power of replication studies is typically based on the reported effect size of the original study, an inflated effect size also renders the power of the replication study to be much lower than anticipated. Hence, an underpowered study may additionally increase the replications’ probability of encountering a type II error, which may lead replicators to misinterpret the statistical significance of the original study as being a false-positive. Besides being self-defeating for authors of the original study, this may compromise the veracity of the cumulative knowledge base that direct replications aim to build.

### Lack of transparency

When it comes to the reporting of methodologies, there seem to be disagreements within the scientific community. In his *new Etiquette for Replication*, Daniel Kahneman [-@Kahneman2014] called for new standards for conducting direct replication studies. Concretely, replicators should be obliged to consult the authors of the original study – otherwise, the replication should not be valid. According to him, the described methodologies in psychology papers are too vague to permit direct replications. He argues that “[…] behavior is easily affected by seemingly irrelevant factors” and that paraphrasing experimental instructions discards crucial information, as “[…] their wording and even the font in which they are printed are known to be significant”. Kahneman’s proposed rules for the interaction between authors and replicators led to heated discussions within the discipline. Chris Chambers [-@Chambers2017, pp. 52-55] refers to several responses to Kahneman, among others, from psychologist Andrew Wilson. In his blog post, titled *Psychology's real replication problem: our Methods sections*, he takes an unequivocal stand on rejecting rigid standards for replication studies:

> If you can't stand the replication heat, get out of the empirical kitchen because publishing your work means you think it's ready for prime time, and if other people can't make it work based on your published methods then that's your problem and not theirs [@Wilson2014].

Of course there are also voices between those extremes that, even if they strongly disapprove of Kahneman’s proposal, agree that there are shortcomings in reporting methodologies. So why are method sections not as informative as they should be? A reason might be that the trend toward disregarding direct replications – due to lacking incentives – decreases the importance of detailed descriptions about the experimental design or data analyses. Furthermore, editors may favor brief method descriptions due to a lack of space in the paper. To minimize a variation in methodologies that might account for different outcomes, it is essential that journal policies change accordingly.

In addition to detailed reporting of methodologies, further materials such as scripts and raw data are known to facilitate replication efforts. In an attempt to retrieve data from previous studies, @HardwickeIoannidis2018 encountered that almost 40% of the authors did not respond to their request in any form, followed by almost 30% not willing to share their data. The reluctance to share data for reanalysis can be related to weaker evidence and more errors in reporting statistical results [@WichertsBakker2011]. This finding further intensifies the need for assessing the veracity of the reported results by reanalyzing the raw data, i.e., checking its computational reproducibility. However, computational replication attempts can hardly be conducted without transparency of the original study. To end this vicious circle and make sharing common practice, journals could establish mandatory sharing policies or provide incentives for open practices.

## Possible remedies {#app-94-remedies}

The intertwined connections between contributing factors show how quickly the reproducibility and replicability of a study can be compromised. In order to combat the replication crisis, it is therefore necessary that many factors are addressed. Over the past years, the severity of the consequences of irreplicable research spawned many solution attempts. An overview of the most effective measures so far is provided in the following.

### Improve scientific rigor

#### Preregistration

A preregistration is a protocolled research plan *prior* to data collection – including hypotheses, methodology, research design, and statistical analyses. With this commitment, researcher degrees of freedom are drastically constrained, so that (sub)conscious engagement in QRP’s like $p$-hacking or HARKing is limited. Such practices are limited because preregistered studies draw a clear line between exploratory and confirmatory analyses. The confirmatory analyses regarding the prespecified hypothesis (i.e., the prediction) are registered in advance so that exploring the data and then selectively reporting is not possible without being detectable. 

Some may perceive preregistrations as a limitation on "scientific creativity". However, preregistrations do not keep researchers from exploring their data. Examining the data is the core of generating new hypotheses for further *confirmatory* analyses, which is ultimately vital for scientific progress. Nevertheless, due to an inflation of false-positive rates, any $p$-values calculated after data inspection are virtually invalid (think confirmation bias) and thus need to be clearly marked as post hoc.

You do not need to write a preregistration from scratch. In fact, websites such as the [Open Science Framework (OSF)](https://osf.io/) or [AsPredicted](https://www.aspredicted.org/) offer useful templates, only needed to be filled out as detailed as possible. Below is a list of what your preregistration should ideally include. For more information on each point listed, please take a look at the [preregistration template](https://docs.google.com/document/d/1DaNmJEtBy04bq1l5OxS4JAscdZEkUGATURWwnBKLYxk/edit?pli=1) provided by the OSF.

* **Study information:** title, authors, description (optional), hypotheses

* **Design plan:** study type, blinding, study design, randomization (optional)

* **Sampling plan:** existing data, explanation of existing data (optional), data collection procedures, sample size, sample size rationale (optional), stopping rule (optional)

* **Analysis plan:** statistical models, transformations (optional), inference criteria (optional), data exclusion (optional), missing data (optional), exploratory analyses (optional)

#### Know your stats

**1. Learn more about statistical frameworks**

A good departure point for increasing research quality is to learn more about statistical frameworks, such as Null Hypothesis Significance Testing. A common misinterpreted and misused construct in NHST is the $p$-value itself. Therefore, when adopting a frequentist approach to hypothesis testing, it is important to intensively engage with the purpose and interpretation of the $p$-value. In the exercise box below (repeated from before), you can test your knowledge about it. If you feel like you need a refresher on the topic, please re-visit chapter \@ref(ch-03-05-hypothesis-p-values). 

<div class = "exercises">
**Self-test on $p$-values**

Which statement(s) about $p$-values is/are true? The $p$-value is...

a. ...the probability that the null hypothesis $H_0$ is true.
b. ...the probability that the alternative hypothesis $H_a$ is true.
c. ...the probability, derived from the assumption that $H_0$ is true, of obtaining an outcome for the chosen test statistic that is the exact same as the observed outcome.
d. ...a measure of evidence in favor of $H_0$.
e. ...the probability, derived from the assumption that $H_0$ is true, of obtaining an outcome for the chosen test statistic that is the same as the observed outcome or more extreme towards $H_a$.
f. ...the probability of a Type-I error.
g. ...a measure of evidence against $H_0$.

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

Statements e. and g. are correct. 
   
</div>
</div>
</div>

Another statistical framework worth thinking about adopting is a [Bayesian approach to hypothesis testing](#ch-03-07-hypothesis-testing-Bayes). In Bayesian statistics, there is no binary decision as to reject or not to reject a given null hypothesis. Instead, evidence is *quantified*. Assigning less importance to a single value that decides whether a study is published or not might reduce the pressure to pass the threshold at all costs. Furthermore, Bayesian statistics provide a broader spectrum for drawing conclusions from the results. The posterior tells us how probable a hypothesis is given the data, i.e., $P(H|D)$. In frequentists statistics, it is the other way around. We check how probable it is to see the data if the null hypothesis were true, i.e., $P(D|H_0)$. This lacks not only information on the alternative hypothesis being true (which is often the research question) but also the null hypothesis itself being true. While a non-significant $p$-value is no go-ahead for accepting $H_0$, Bayes factors can in fact give evidence *in favor* of $H_0$ by comparing both hypotheses in light of the data.

**2. Control for the error rate**

The most obvious way to reduce the probability of encountering a false-positive is to lower the significance threshold. Indeed, many researchers argue that a 1 in 20 chance of obtaining a false-positive is too high and call for lowering the conventional $\alpha$ threshold from 0.05 to 0.005 [@BenjaminBerger2017]. This call sparked a hot debate in the scientific community. @LakensAdolfi2018 argue that switching to a lower threshold might even have negative consequences on the feasibility of replication studies. Researchers should instead justify the choice of the threshold individually. Resources that provide arguments against "Team Redefine" and "Team Justify" are linked in chapter \@ref(app-94-resources).

Regardless of whether you agree with "Team Redefine" or not, it is crucial to further correct the threshold if at least two analyses were conducted on the same data set. Probably the simplest and most conservative method to adjust the family-wise error rate to 0.05 is to use a *Bonferroni* correction, where the per-contrast $\alpha$-level is divided by the number $n$ of tests on the same data set. In the fishy jelly bean study from the previous chapter, the researchers could have corrected the error rate as follows:

$$
\alpha_{corrected} = \frac{\alpha}{n} = \frac{0.05}{20} = 0.0025
$$

This new cut-off point at 0.0025 ensures an upper bound of 5% for a false-positive result despite multiple testing:

$$
\alpha_{FWER} = 1-(1-0.0025)^{20} \approx 0.049
$$

So in order to claim a statistically significant link between acne and the jelly bean flavor at hand, the $p$-value in this particular test has to be smaller than 0.0025. 

**3. Increase statistical power**

Power is mainly improved by increasing the underlying effect size, decreasing the standard deviation, increasing the $\alpha$-level, and increasing the sample size (see figure \@ref(fig:ch-94-improve-power)). However, not all factors are feasible, practical, or common. The true effect size is unknown in reality and needs to be estimated when calculating the power. Since it is not influenceable, the true effect size is as big as it gets. The standard deviation can theoretically be reduced (e.g., by controlling for measurement error), but only up to a point. A higher demarcation point for statistical significance simultaneously entails a higher risk of making a false discovery and is thus not desirable at all. 

This leaves us with the sample size. Intuitively put, increasing the number of observations in the sample will provide more information about the population we want to generalize to (think Law of Large Numbers). Sampling 100 avocados per type (conventional, organic) will bring us closer to the true mean price per type than just sampling 20 avocados. Concretely, a larger sample size will decrease the standard error of the mean (SEM), which is the standard deviation of the sample means. With a lower standard error, smaller effect sizes are detectable.

The minimum required sample size to detect a given effect size is thereby often calculated with a so-called *power analysis*. If the power analysis yields a sample size estimate that is difficult to realize, it may be an indicator that the effect size is so minor that the study could even not be worth the effort. If authors still want to conduct their studies, inviting many more participants and thereby prolonging the experiment can conflict with time and financial limitations. According to the motto “where there is a will, there is a way”, authors can team up with other labs to achieve the calculated sample size.

Useful resources and software for power analyses are linked in Chapter \@ref(app-94-resources).

```{r ch-94-improve-power, echo=FALSE, fig.height=10, fig.cap='Factors that affect the power of a study. **A|** An example of low statistical power. **B|** A larger effect size makes differences easier to detect. **C|** Less variability in the groups makes smaller differences detectable. **D|** A higher $\\alpha$-level increases the probability of rejecting $H_0$.'}
plot_data1 <- tibble(
  x = seq(-8, 12, length.out = 1000),
  null = dnorm(x, sd = 2),
  alternative = dnorm(x, mean = 1, sd = 2)
  ) %>% 
  pivot_longer(cols = 2:3)

a <- plot_data1 %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
  # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = -0.2,
      y = -0.025,
      label = "H_0"
    ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x = 1,
      xend = 1,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 1.2,
      y = -0.025,
      label = "H_a"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # power area
  geom_area(
    data = filter(
      plot_data1, name == "alternative", 
      x >= qnorm(0.95, sd = 2)
      ), 
    fill = "firebrick",
    color = "firebrick",
    alpha = 0.3
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data1, name == "null", 
      x >= qnorm(0.95, sd = 2)
      ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data1, name == "alternative", 
      x <= qnorm(0.95, sd = 2)
      ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # Power explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.1595, 
    label = ("Power"), 
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 3.4,
      xend = 10,
      y = 0.075,
      yend = 0.1475
      ),
    color = "black"
  ) +
  # alpha error explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.09, 
    label = TeX("$\\alpha$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 3.4,
      xend = 10,
      y = 0.0075,
      yend = 0.08
      ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom = 'text', 
    x = -6, 
    y = 0.1, 
    label = TeX("$\\beta$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = -0.5,
      xend = -6,
      y = 0.05,
      yend = 0.09
      ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  annotate(geom = "text", label = "A", x = -8, y = 0.2, fontface = "bold", size = 5) + 
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )


plot_data2 <- tibble(
  x = seq(-8, 12, length.out = 1000),
  null = dnorm(x, sd = 2),
  alternative = dnorm(x, mean = 4, sd = 2)
  ) %>% 
  pivot_longer(cols = 2:3)

b <- plot_data2 %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
  # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
    ) + 
  geom_label(
    aes(
      x = 0,
      y = -0.025,
      label = "H_0"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
    ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x = 4,
      xend = 4,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 4,
      y = -0.025,
      label = "H_a"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # power area
  geom_area(
    data = filter(
      plot_data2, name == "alternative", 
      x >= qnorm(0.95, sd = 2)
      ), 
    fill = "firebrick",
    color = "firebrick",
    alpha = 0.3
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data2, name == "null", 
      x >= qnorm(0.95, sd = 2)
      ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data2, name == "alternative", 
      x <= qnorm(0.95, sd = 2)
      ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # Power explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.203, 
    label = ("Power"), 
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 4.3,
      xend = 10,
      y = 0.105,
      yend = 0.19
      ),
    color = "black"
  ) +
  # alpha error explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.1, 
    label = TeX("$\\alpha$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 4.3,
      xend = 10,
      y = 0.005,
      yend = 0.09
      ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom='text', 
    x = -6, 
    y = 0.1, 
    label = TeX("$\\beta$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 2,
      xend = -6,
      y = 0.05,
      yend = 0.09
      ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  annotate(geom = "text", label = "B", x = -8, y = 0.2, fontface = "bold", size = 5) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

plot_data3 <- tibble(
  x = seq(-8, 12, length.out = 1000),
  null = dnorm(x, sd = 0.5),
  alternative = dnorm(x, mean = 1, sd = 0.5)
  ) %>% 
  pivot_longer(cols = 2:3)

c <- plot_data3 %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
  # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.08,
      yend = dnorm(0, sd = 0.5)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = -0.2,
      y = -0.1,
      label = "H_0"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x = 1,
      xend = 1,
      y = -0.08,
      yend = dnorm(0, sd = 0.5)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 1.2,
      y = -0.1,
      label = "H_a"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # power area
  geom_area(
    data = filter(
      plot_data3, name == "alternative", 
      x >= qnorm(0.95, sd = 0.5)
      ), 
    fill = "firebrick",
    color = "firebrick",
    alpha = 0.3
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data3, name == "null", 
      x >= qnorm(0.95, sd = 0.5)
      ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data3, name == "alternative", 
      x <= qnorm(0.95, sd = 0.5)
      ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # Power explanation
  annotate(
    geom = 'text', 
    x = 5, 
    y = 0.810, 
    label = ("Power"), 
    size = 5,
    color = "black",
    label.size = 0
  ) +
    geom_segment(
    aes(
      x = 1.2,
      xend = 5,
      y = 0.46,
      yend = 0.76
      ),
    color = "black"
  ) +
  # alpha error explanation
  annotate(
    geom = 'text', 
    x = 5, 
    y = 0.4, 
    label = TeX("$\\alpha$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 1,
      xend = 5,
      y = 0.06,
      yend = 0.36
      ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom = 'text', 
    x = -5, 
    y = 0.4, 
    label = TeX("$\\beta$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 0.5,
      xend = -5,
      y = 0.1,
      yend = 0.36
      ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  annotate(geom = "text", label = "C", x = -8, y = 0.8, fontface = "bold", size = 5) + 
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

d <- plot_data1 %>% 
  ggplot(aes(x = x, y = value, color = name)) + 
    # null hypothesis segment & label
  geom_segment(
    aes(
      x = 0,
      xend = 0,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = -0.2,
      y = -0.025,
      label = "H_0"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # alternative hypothesis segment & label
  geom_segment(
    aes(
      x = 1,
      xend = 1,
      y = -0.02,
      yend = dnorm(0, sd = 2)
      ),
    color = "darkgray"
  ) + 
  geom_label(
    aes(
      x = 1.2,
      y = -0.025,
      label = "H_a"
      ),
    color = "darkgray",
    label.size = 0,
    size = 4
  ) +
  # power area
  geom_area(
    data = filter(
      plot_data1, name == "alternative", 
      x >= qnorm(0.90, sd = 2)
      ), 
    fill = "firebrick",
    color = "firebrick",
    alpha = 0.3
  ) +
  # alpha error area
  geom_area(
    data = filter(
      plot_data1, name == "null", 
      x >= qnorm(0.90, sd = 2)
      ), 
    fill = project_colors[2],
    color = project_colors[2],
    alpha = 0.3
  ) +
  # beta error area
  geom_area(
    data = filter(
      plot_data1, name == "alternative", 
      x <= qnorm(0.90, sd = 2)
      ), 
    fill = project_colors[1],
    color = project_colors[1],
    alpha = 0.3
  ) +
  # main density lines
  geom_line(size = 2) +
  # Power explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.203, 
    label = ("Power"), 
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 2.8,
      xend = 10,
      y = 0.11,
      yend = 0.19
      ),
    color = "black"
  ) +
  # alpha error explanation
  annotate(
    geom = 'text', 
    x = 10, 
    y = 0.1, 
    label = TeX("$\\alpha$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = 2.8,
      xend = 10,
      y = 0.025,
      yend = 0.09
      ),
    color = "black"
  ) +
  # beta error explanation
  annotate(
    geom = 'text', 
    x = -6, 
    y = 0.1, 
    label = TeX("$\\beta$-error", output = 'character'), 
    parse = TRUE,
    size = 5,
    color = "black",
    label.size = 0
  ) +
  geom_segment(
    aes(
      x = -0.5,
      xend = -6,
      y = 0.05,
      yend = 0.09
      ),
    color = "black"
  ) +
  # general cosmetics
  guides(color = F) +
  labs(x = "", y = "") +
  annotate(geom = "text", label = "D", x = -8, y = 0.2, fontface = "bold", size = 5) + 
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

cowplot::plot_grid(a,b,c,d, ncol = 1)
```

**4. Correlation $\neq$ Causation**

As already outlined in Chapter \@ref(Chap-02-03-summary-statistics-2D), a positive or negative correlation must not be interpreted as a causal relationship between two measurements $A$ and $B$. In fact, there can also be a third variable $C$ that caused both $A$ and $B$. Let's illustrate erroneous conflation of correlation and causation with an example. It is known that a rapid drop in barometric pressure ($A$) is followed by stormy weather ($B$). This co-occurrence, however, cannot be interpreted as $A$ causing $B$, i.e., the barometric pressure causing a storm. Rather, there is a third factor $C$, that causes both the drop in barometric pressure and stormy weather, namely the falling air pressure that precedes both events.

An observed correlation between two measurements, attributed to either a third (confounding) variable or to random chance, is called a *spurious relationship* or *spurious correlation*. Note that spurious correlations are often not readily identifiable, which is why we never can surely speak of causation. This realization is important to keep in mind when interpreting the results of a study.

### Realigning incentive structures

#### Registered Reports

A Registered Report (RR) is a preregistration that is integrated into the publication process. Here, peer review splits into two stages: one before data collection and one after conducting the experiment. Initially, the manuscript is triaged by an editorial team for rigor, feasibility, and suitability of the research proposal. If approved, the manuscript passes to the first stage of peer review, where the study design, methods, and proposed analyses are assessed in more depth. In addition to other methodological aspects, reviewers assess if proposed studies are adequately powered, requiring the *a priori* statistical power to be at least 90%. In the first stage, peer reviewers preliminary accept the submitted manuscript, ask the researchers to revise or reject it right away. If the manuscript passes the first stage, it is “in principle accepted” (IPA) and guaranteed to be published – irrespective of the results. After data collection and analyses, the manuscript is completed with the results section and discussion and then submitted to stage two peer review. To be admissible, authors are often required to make their anonymized raw data and study materials publicly available and provide a link within the stage two manuscript. In this last stage, the adherence to the manuscript approved in stage one is assessed. If the results are reported thoroughly, and potential deviations from the IPA report are justified and transparent, the paper is published.

Registered Reports eliminate publication bias. By accepting the manuscript in stage one of peer review, the respective journal guarantees the publication of the study, irrespective of whether the outcomes are null results or significant. With results-blind peer review, fewer papers with non-significant results should land in the file drawers^[This is a reference to the "file drawer problem" coined in the late seventies. The term aptly describes publication bias, in that non-significant studies are locked away in the file drawer of the researcher, with no prospect of being seen by the scientific community.] and instead be added to the scientific literature. By eliminating publication bias, Registered Reports aim to take off the pressure from researchers to chase significant $p$-values and shift the focus on valuing good quality research that complies with the [hypothetico-deductive model of the scientific method](https://en.wikipedia.org/wiki/Hypothetico-deductive_model).

Two recent studies suggest that Registered Reports are indeed effective in realigning incentive structures. @ScheelSchijen2020 found that 56% of the Registered Reports did not yield statistically significant results. In a study by @AllenMehler2019, the percentage was even higher. The results vary tremendously from the authors’ estimation of 5%-20% of null results in traditional literature. If many journals and authors promote this publication format, it is conceivable that the skewed incentive structure can be normalized entirely. Indeed, the number of journals offering Registered Reports is increasing exponentially. What started with three journals in 2013 – with *Cortex* as the first journal to implement them – has been steadily growing ever since, with currently 272 participating journals at the time of writing.

With realigning incentive structures, it may be reasonable that also a journal’s prestige is evaluated differently. Instead of depending on citations, a journal’s impact could be assessed on how much it promotes open science practices (see [TOP Factor](https://www.cos.io/top)) and replicable research (see [Replicability-Index](https://replicationindex.com/about/)). 

#### Replication initiatives

Direct replications are the cornerstone of science. They assure the validity and credibility of scientific discoveries on which further research can build upon. However, a neophiliac incentive system makes this scrutiny unlikely to be published and may lead researchers to see no sense in attempting direct replications of previous findings. Here are two solution attempts journals may adopt to stop neophilia in favor of preserving psychology's "self-correcting system":

**1. Pottery Barn Rule**

The "Pottery Barn Rule" is a solution attempt proposed by psychologist Sanjay Srivastava [-@Srivastava2012]. According to the motto “you break it, you buy it”, he proposes that once a journal has published a study, it should also be accountable to publish direct replications of the findings. Importantly, this commitment also includes failed replications and replication attempts despite a flawed methodology of the original study. An example of the implementation of such a concept is the Psychology and Neuroscience Section of *Royal Society Open Science*. 

**2. Registered Replication Reports**

A Registered Replication Report (RRRs) is a publication format that is entirely dedicated to direct replications. It consists of  multi-lab direct replication studies that aim to precisely estimate effect sizes in studies whose results are highly influential in their field or attracted media attention. The *Association for Psychological Science* (APS) is the first organization to implement and encourage this extension of classic Registered Reports.

### Promote transparency

#### Open Science Badges

To provide an incentive for open science practices, more and more journals are adopting [badges](https://www.cos.io/initiatives/badges). By sharing data, providing materials, or preregistering, the authors “earn” the corresponding badge, which will be depicted in the respective article. The three types of badges are shown in figure \@ref(fig:os-badges). 

```{r os-badges, echo = F, out.width = '40%', fig.cap="Badges to incentivize preregistration, as well as data and material sharing."}
knitr::include_graphics("visuals/open-science-badges.png")
```

This simple solution already shows initial indications of a positive impact. @KidwellLazarević2016 found that implementing badges increases data and material sharing. Although the authors did not include the impact of the preregistration badge in their assessment, it is conceivable that the possibility of earning the badge will result in more preregistrations – or at least awareness of their existence.

#### TOP Guidelines

The **T**ransparency and **O**penness **P**romotion Guidelines comprise open science standards for eight categories. Journals that promote open science practices can implement one or more guidelines into their policy, thereby choosing the level of stringency. For example, if a journal adopts level III (the most stringent level) of the "Data" standard, it requires authors to make their data retrievable for readers. Furthermore, the journal commits to reproduce the results independently before publishing the paper. 

A full list of the TOP Guidelines can be found on the [website of the Center for Open Science](https://www.cos.io/initiatives/top-guidelines).

#### Disclosure statements

Sometimes it doesn't need much to cause a great effect. In fact, it can only take 21 words to enhance integrity in research papers:

> We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study.

This 21-word solution by @SimmonsNelson2012 requests from researchers to honestly report everything in order to render this statement true. For example, data exclusion in a reaction time study may be disclosed like this: "We excluded every individual trial faster than 100 ms and slower than 1500 ms." 

If such disclosure statements were the norm or mandatory, authors who want to conceal details of data collection and analysis would have to actually lie about their disclosure. And deliberate lying probably exceeds the "grey area" in research. 

```{block, type='infobox'}
**Excursion: How to share** 

Nothing will facilitate replication efforts more than providing a high degree of methodological detail, raw data, study materials, and analysis scripts. However, there are also a few things to consider with regard to *how* to share. Here are three best practices to further increase transparency and ease the navigation in your project repository (e.g., on [OSF](https://osf.io/) or [GitHub](https://github.com/)).

**Add a README file.** A README file is usually the first place to go when exploring a project. Ideally, it should give methodological information, a walkthrough on the files and the folder structure of the project, and, importantly, contact information (especially the full name and a permanent email address). It is also useful to document the software (preferably open source), versioning, and packages needed for analysis.

**Provide metadata.** Metadata, that is, "data about data", refers to structural, descriptive, or administrative properties of the gathered data. It gives, to name a few, information about who collected the data, the time period, and the location of data collection. Furthermore, it relates to the actual content of the data, e.g., what the column names mean and what type of observations are stored. A detailed description of all measured variables is also called a "Codebook". Providing rich information about your data will certainly facilitate the correct interpretation and reusage of it.

**Comment your code.** Reading into someone else's code is hard enough. It is more time and nerve consuming if the code is also not accompanied with proper comments. Let the reader know what your code does by providing useful comments and descriptive variable names. This also has the benefit that *you* have a better overview of the analysis script when you come back to it at a later point in time (e.g., reuse of code).

```

## Chapter summary {#app-94-recap}

In this chapter, we peeked into the abyss of psychology's replication crisis and learned how biases towards significant outcomes fuel the engagement in questionable research practices, ultimately compromising the validity of the finding. We also learned that underpowered studies will not only hardly find true-positives, but also render detected effects useless. Lastly, the lack of transparency, be it in reported methods or keeping data and analysis code private, may account for moderators between the original and replication study. Later, we looked at several promising solution attempts and initial prospects of improvement. It is important to note though that there is no single panacea in combating the replication crisis. It is rather a *combination* of several measures, such as preregistration/Registered Reports, badges, TOP guidelines, and replication initiatives. The existence and increased implementation of such measures manifest that standards in psychological research are about to change. The replication crisis is followed by a credibility revolution, prompting psychology to reward what should be rewarded - scientific rigor and integrity. 

```{block, type='recap'}

Our little journey through the replication crisis ends here. Here are the most important points to keep in mind:

**Limit exploitation of researcher degrees of freedom**

* Be aware of cognitive biases, such as confirmation bias, hindsight bias, and apophenia 
* Raise awareness and be aware of QRP's and their implications for the validity of research outcomes
* Preregister your study

**Limit statistical fallacies**

* Learn more about Null Hypothesis Significance Testing 
* Consider adopting a Bayesian approach to hypothesis testing
* Control for error rates
* Make sure that your study has sufficient power (conduct a power analysis)
* Keep in mind that correlation does not imply causation

**Eliminate Publication bias**

* Consider submitting your study as a Registered Report
* Replicate!

**Embrace transparency**

* Make your raw data, materials, and analysis scripts retrievable
* You did not engage in selective reporting? Awesome! Let the reader know by explicitly writing a disclosure statement (e.g., 21-word solution)
* Structure your project in a way that others can easily navigate through your project, thereby providing
  - README files
  - Metadata/Codebooks
  - Descriptive comments and variable names in analysis scripts
```

## Further resources {#app-94-resources}

**Incentives**

More information on Registered Reports: 

* https://www.cos.io/initiatives/registered-reports

Assessing the effectiveness of Registered Reports:

* Chambers, C. D., Tzavella, L. (2020). Registered Reports: Past, Present and Future. https://doi.org/10.31222/osf.io/43298

* Hardwicke, T. E., Ioannidis, J. P. A. (2018). Mapping the universe of registered reports. *Nature Human Behaviour*, 2, 793–796. https://doi.org/10.1038/s41562-018-0444-y

Workflow of Registered Replication Reports (example AMPPS): 

* https://www.psychologicalscience.org/publications/ampps/rrr-guidelines)

Brian Nosek on the importance of replication:

* https://www.youtube.com/watch?v=wsRmyW8GmJs

**Statistics**

<u>Researcher degrees of freedom and QRP's</u>

Two papers on the reasons behind false discoveries:

* Simmons, J. P., Nelson, L. D., Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. *Psychological Science*, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632

* Ioannidis, J. P. A. (2005) Why Most Published Research Findings Are False. *PLoS Med* 2(8): e124. https://doi.org/10.1371/journal.pmed.0020124

More on the $\alpha$-debate: 

* Morey, R. 2017. “Redefining statistical significance: the statistical arguments.” 26, 2017. https://medium.com/@richarddmorey/when-the-statistical-tail-wags-the-scientific-dog-d09a9f1a7c63

* de Ruiter, J. 2019. “Redefine or Justify? Comments on the Alpha Debate.” *Psychon Bull Rev* 26: 430–33. https://doi.org/10.3758/s13423-018-1523-9

Overview of methods to adjust the familywise error rate: 

* Chen, S. Y., Feng, Z., Yi, X. (2017). A general introduction to adjustment for multiple comparisons. *Journal of thoracic disease*, 9(6), 1725–1729. https://doi.org/10.21037/jtd.2017.05.34

<u>Statistical power</u>

An introductory video on statistical power by the OSF:

* https://www.youtube.com/watch?v=-ZU7fbvSJ60

A video on consequences of low statistical power by the OSF: 

* https://www.youtube.com/watch?v=7daQRvRO-NE&t=20s

An R package to assess Type S and Type M error rates: 

* https://cran.r-project.org/web/packages/retrodesign/index.html

Correcting for inflated effect sizes fueled by publication bias:

* Simonsohn, U., Nelson, L. D., Simmons, J. P. (2014). p-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results. *Perspectives on Psychological Science*, 9(6), 666–681. https://doi.org/10.1177/1745691614553988

Frequentist power analysis in R: 

* https://www.statmethods.net/stats/power.html

G*Power - A Software for frequentist power analysis: 

* https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html

A blogpost on Bayesian power analysis: 

* https://solomonkurz.netlify.app/post/bayesian-power-analysis-part-i/

**Transparency**

A survey that aimed to gain insight into why authors keep their data private:

* Houtkoop, B. L., Chambers, C., Macleod, M., Bishop, D. V. M., Nichols, T. E., Wagenmakers, E.-J. (2018). Data Sharing in Psychology: A Survey on Barriers and Preconditions. *Advances in Methods and Practices in Psychological Science*, 1(1), 70–85. https://doi.org/10.1177/2515245917751886

More information on open science badges: 

* https://www.cos.io/initiatives/badges

More information on TOP Guidelines: 

* https://www.cos.io/initiatives/top-guidelines

A platform for disclosure statements: 

* https://psychdisclosure.org/

Guidelines to write README files and Metadata: 

* https://data.research.cornell.edu/content/readme

**Miscellaneous**

A comic about the replication crisis: 

* https://thenib.com/repeat-after-me/?t=defau

A youtube-playlist of 5-10 minute videos on open science:

* https://www.youtube.com/watch?v=1rFWeTryiW4&list=PLtAL5tCifMi5zG70dslERYcGApAQcvj1s

A video on norms in science:

* https://www.youtube.com/watch?v=00btFojQPiU&t=36s

Highly recommended resources that provide a broad overview of the replication crisis, contributing factors, and solution attempts:

* Chambers, C. (2017). *The Seven Deadly Sins of Psychology: A Manifesto for Reforming the Culture of Scientific Practice*. Princeton University Press.

* Munafò, M., Nosek, B., Bishop, D. et al. A manifesto for reproducible science. *Nat Hum Behav* 1, 0021 (2017). https://doi.org/10.1038/s41562-016-0021
