# Categorical predictors {#Chap-04-03-predictors}

<hr>

The previous chapters applied linear regression models to cases where we wanted to predict a metric variable $y$ based on the values of associated metric measurements $x_i$ ($1 \le i \le n$).
In this chapter we are generalizing this approach to also deal with the case where a predictor $x_i$ is a categorical variable, such as an indicator value showing which group or experimental condition a measurement of $y$ belongs to.
In this way, at the end of the chapter we will be able to apply linear regression modeling to the analysis of (metric) measurements, for instance, from a factorial design - a common design type of psychological experiments (see Chapter \@ref(Chap-02-01-data)).

As we will see in this chapter, the only "trick" to generalizing linear regression modeling to also cover **categorical predictors**, as we will call them, is to map levels of a categorical variable onto numbers.
For example, if we have two groups in a predictor variable $x$, say group $A$ and $B$, we could just encode group $A$ as a value of $x = 0$ and group $B$ as a value of $x = 1$.
But there are many sensible mappings of this kind, and many helplessly ridiculous ones.
The relevant technical term here is **contract coding**, i.e., a scheme of mapping categorical distinctions onto numeric representations in such a way that the theoretically interesting *contrasts* (i.e., the distinctions between groups that we want to test) can be easily tested with the resulting regression model. 

The chapter is structured as follows.
Section \@ref(Chap-04-03-predictors-two-levels) uses the [Simon task data](app-93-data-sets-simon-task) to cover the case of a single categorical predictor with just two category levels (e.g., just groups $A$ and $B$).
Section \@ref(Chap-04-03-predictors-multi-levels) then uses data from the [Mental Chronometry experiment](app-93-data-sets-mental-chronometry) to look at the more general case of a single categorical predictor with more than two category levels (e.g., just groups $A$, $B$ and $C$). 
Section \@ref(Chap-04-03-predictors-multiple-predictors) considers cases with several categorical predictors, including their **interaction**, based on the [politeness data set]().

The video below provides a dense coverage of single-predictor contrast coding, i.e., the contents of Sections \@ref(Chap-04-03-predictors-two-levels) and \@ref(Chap-04-03-predictors-multi-levels).

<iframe src="https://player.vimeo.com/video/422832571" width="640" height="360" frameborder="0" allow="fullscreen" allowfullscreen></iframe>

```{block, type='infobox'}
The learning goals for this chapter are:

- understand the basic idea behind *contrast coding*
- be able to interpret the results of a regression analysis which uses *treatment coding*
- be able to interpret inferences for models including an *interaction coefficient* for two categorical predictors

```

## Single two-level predictor {#Chap-04-03-predictors-two-levels}

Let's revisit the data from the [Simon task](app-93-data-sets-simon-task).
Just like in chapter \@ref(), we will be looking at the hypothesis that, among all correct responses, the mean reaction times for the congruent condition are lower than those of the incongruent condition.

```{r}
# extract just the currently relevant columns 
#  from the data set
data_ST_excerpt <- aida::data_ST %>% 
  filter(correctness == "correct") %>% 
  select(RT, condition)

# show the first couple of lines
head(data_ST_excerpt, 5)
```

Notice that this tibble contains the data in a tidy format, i.e., each row contains a pair of associated measurements. 
We want to explain or predict the variable `RT` in terms of the variable `condition`.
The variable `RT` is a metric measurement.
But the variable `condition` is categorical variable with two category levels.

Before we head on, let's look at the data (again).
Here's a visualization of the distribution of RTs in each condition:

```{r}
data_ST_excerpt %>% 
  ggplot(aes(x = condition, y = RT, color = condition, fill = condition)) +
  geom_violin() +
  theme(legend.position = "none")
```

The means for both conditions are:

```{r}
data_ST_excerpt %>% 
  group_by(condition) %>% 
  summarize(mean_RT = mean(RT))
```

The difference between the means of conditions is:

```{r}
data_ST_excerpt %>% filter(condition == "incongruent") %>% pull(RT) %>% mean() -
  data_ST_excerpt %>% filter(condition == "congruent") %>% pull(RT) %>% mean()
```

While numerically this difference seems high, the question remains whether this difference is, say, big enough to earn our trust.
We address this question here using posterior inference based on a regression model. 
Notice that we simply use the same formula syntax as before: we want a model that explains `RT` in terms of `condition`.

```{r}
fit_brms_ST <- brm(
  formula = RT ~ condition,
  data = data_ST_excerpt
)
```

Let's inspect the summary information for the posterior samples, which we do here using the `summary` function for the `brms_fit` object from which we extract information only about the `fixed` effects, showing the mean (here called "Estimate") and indicators of the lower and upper 95% inner quantile.

```{r}
summary(fit_brms_ST)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```

We see that the model inferred a value for an "Intercept" variable and for another variable called "conditionincongruent".
What could these be?
If you look back at the empirically inferred means, you will see that the mean estimate for "Intercept" corresponds to the mean of RTs in the "congruent" condition and that the mean estimate for the variable "conditionincongruent" closely matches the computed difference between the means of the two conditions.
This is great!
Using a uniform formula syntax `brms` has set up a regression model in which a predictor, given as a character (string) column, was internally coerced somehow into a format that produced an estimate the for the mean of one condition and an estimate for the difference between conditions.

UP TO HERE










Compare this with the summary of the posterior estimates we obtained from the Bayesian $t$-test model for this data which we implemented in `Stan` and ran in Chapter \@ref(ch-03-07-hypothesis-testing-Bayes).

```{r echo = F, eval = F}
draws_t_test_2 <- readRDS('models_greta/ttest_2_draws.rds')
```

```{r}
# cast results (type 'mcmc.list') into tidy tibble
tidy_draws_tt2 = ggmcmc::ggs(stan_fit_ttest)

# get means and 95% HDI
Bayes_estimates_eco <- tidy_draws_tt2 %>% 
  group_by(Parameter) %>%
  summarise(
    '|95%' = HDInterval::hdi(value)[1],
    mean = mean(value),
    '95|%' = HDInterval::hdi(value)[2]
  )

Bayes_estimates_eco
```

```{r echo = F}
delta <- Bayes_estimates_eco$mean[1] %>% signif(digits = 3)
mu <- Bayes_estimates_eco$mean[2] %>% signif(digits = 3)
sigma <- Bayes_estimates_eco$mean[3] %>% signif(digits = 3)

groupA <- mu+sigma*delta/2
groupB <- mu-sigma*delta/2 
```

At first glance, both outputs do not seem to be related in any way, but appearances are deceptive! Recall that the mean of group $A$ in the $t$-test model from Chapter \@ref(ch-03-07-hypothesis-testing-Bayes) is the sum of the grand mean $\mu$ and $\frac{\sigma \cdot \delta}{2}$. If we plug in the mean values from the summary of the posterior samples above, we get $\mu_A = `r groupA`$. Similarly, the mean of group $B$ can be calculated like so:

$$\mu_B = \mu - \frac{\sigma \cdot \delta}{2} = `r mu` - \frac{`r sigma*delta`}{2} = `r groupB`$$

We get an equivalent result for $\mu_B$ if we subtract $\sigma \cdot \delta$ not from the grand mean, but from the mean of group $A$ instead:

$$\mu_B = \mu_A - \sigma \cdot \delta = `r groupA` - `r sigma*delta` = `r groupA - sigma*delta`$$

Well, that looks familiar. The result for $\mu_A$ looks similar to the mean estimate for the `Intercept` parameter from the `brms` model fit. Multiplying the mean of the $\sigma$ parameter with the mean of the $\delta$ parameter yields a suspiciously similar result as the ominous `groupB` parameter shown in the output of the `brms` model fit. The 95% HDI of the estimated posterior for $\sigma \cdot \delta$ also looks suspiciously like the values of the 95% inter-quantile range for the `groupB` parameter. This is no coincidence! In fact, the regression model that `brms` calculates here is essentially the same as the $t$-test model we implemented in `Stan` by hand except for slight differences in the choice of the priors and inessential differences in the mathematical formulation of the group mean comparison. 

The call to `brm` above implicitly computed a linear regression model of the following form:

$$
\begin{aligned}
\hat{y}_i & = \beta_0 + \beta_1 x_i & y_i & \sim \text{Normal}(\mu = \hat{y}_i, \sigma)
\end{aligned}
$$

The only important point is that the vector $x$, which corresponds to the group information in the column `group` (which contains entries of strings `"A"` and `"B"`), is implicitly treated as a vector of zeros and ones. Implicitly, `brm` has chosen the string `"A"` as the **reference category** which is encoded as zero. The string `"B"` is the other category, encoded as number 1. As a consequence, the linear model's intercept parameter $\beta_0$ can be interpreted as the predicted mean of the reference category: if for some $i$ we have $x_i = 0$, then the predictor $\hat{y}_i$ will just be $\hat{y}_i = \beta_0$; whence that the intercept $\beta_0$ will be fitted to the mean of the reference category. If for some $i$ we have $x_i = 1$ instead, the predicted value will be computed as $\hat{y}_i = \beta_0 + \beta_1$, so that the slope term $\beta_1$ will effectively play the role of the difference $\delta$ between the mean of the groups. Modulo choice of priors and variable naming, the `brm` model encodes a Bayesian $t$-test model similarly like we previously did using `Stan`. The upshot is that we can conceive of a **$t$-test as a special case of a linear regression model!**

Of course, nothing in this correspondence depends on a Bayesian analysis in particular. Let's look at the frequentist version:

```{r}
fit_glm_eco <- glm(
  # specify what to explain in terms of what
  #  using the formula syntax
  formula = measurement ~ group,
  # which data to use
  data = eco_sensitivity_data
)
summary(fit_glm_eco)
```

The $p$-value associated with the test of whether the slope coefficient (the difference between means) is plausibly zero corresponds to the result we get from a $t$-test (when assuming equal variance in groups; an assumption that the linear modeling approach makes per default).

```{r}
t.test(x_A, x_B, paired = F, var.equal = T)
```

<div class = "exercises">
**Exercise 14.1**
For the given data below, compute the coefficients of linear regression by hand. Choose the appropriate encoding of group information.

groupA: (1,0,2) and groupB: (10,13,7)

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
For $\hat{y}_i  = \beta_0 + \beta_1 x_i$, let $x_i =0$ if the data point if from groupA and $x_i=1$ if it's from groupB. Then the mean of groupA is computed by the intercept $\mu_A  = \beta_0$ and the mean of groupB is computed as the sum of the intercept and the slope $\mu_B  = \beta_0 + \beta_1$. Since $\mu_A = 1$ and $\mu_B = 10$, we can guess that $\beta_0 = 1$ and $\beta_1 = 10 - 1 = 9$.
</div>
</div>
</div>

## Single multi-level predictors {#Chap-04-03-predictors-multi-levels}

<div style = "float:right; width:15%;">
<img src="visuals/badge-mental-chronometry.png" alt="badge-mental-chronometry">  
</div>  

A $t$-test is only applicable to at most two groups. The classical approach to generalizing frequentist testing to the comparison of more than two groups is to use ANOVA, as briefly discussed in Chapter \@ref(ch-03-05-hypothesis-testing-ANOVA). But we can also use a linear regression approach for this, as demonstrated here based on the [mental chronometry data](#app-93-data-sets-mental-chronometry). 

We load the data as usual, but also immediately mutate the column `block` (which captures the experimental manipulation we want to use to explain the dependent variable `RT` (= reaction times)) so that the "goNoGo" condition will come first in alphabetic order. This has the effect that, later in regression modeling, the "goNoGo" condition will be treated as the reference level to compare other groups against. This makes sense because our main question of interest is whether these inequalities are supported by the data:

$$
\text{RT in 'reaction'} < 
\text{RT in 'goNoGo'} <
\text{RT in 'discrimination'}
$$

So we are interested in the $\delta$s, so to speak, between 'reaction' and 'goNoGo' and between 'discrimination' and 'goNoGo'.

```{r}
mc_data_cleaned <- aida::data_MC_cleaned %>% 
  # renaming to make 'goNoGo' the reference level 
  # (dirty hack, but simpler than messing with contrast coding)
  mutate(
    block = case_when(
      block == "reaction"         ~ "B_reaction",
      block == "goNoGo"           ~ "A_goNoGo",
      block == "discrimination"   ~ "C_discrimination"
    )
  )
```

To fit this model with `brm` we then just need a simple function call with the formula `RT ~ block` that precisely describes what we are interested in: to explain reaction times as a function of the experimental condition:

```{r eval = F}
fit_brms_mc <- brm(
  # model 'RT' as a function of 'block'
  formula = RT ~ block,
  data = mc_data_cleaned
)
```


```{r echo = F}
fit_brms_mc <- readRDS('models_brms/mc_data_fit.rds')
```

To inspect the posterior fits of this model, we can extract the relevant summary statistics as before:

```{r}
summary(fit_brms_mc)$fixed[,c("Estimate","l-95% CI", "u-95% CI")]
```

Notice that there is an intercept term, as before. This corresponds to the mean reaction time of the reference level (here: 'goNoGo'). There are two slope coefficients, one for the difference between the 'goNoGo' and the 'reaction' condition ('blockB_reaction') and another for the difference between the 'goNoGo' and the 'discrimination' condition ('blockC_discrimination'). In formula, $\hat{y}_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$, whereas $\beta_0$: Intercept, $\beta_1$: blockB_reaction and $\beta_2$: blockC_discrimination. $x_{i1} = 1$ iff the data point is from the 'reaction' condition and $x_{i2} = 1$ iff it's from the 'discrimination' condition.

As we may have expected, the 95% inter-quantile range for both slope coefficients (which, given the amount of data we have, is almost surely almost identical to the 95% HDI) does not include 0 by a very wide margin. We could, therefore, conclude, based on a Bayesian approach to hypothesis testing in terms of posterior estimation, that the reaction times of conditions are credibly different.

<div class = "exercises">
**Exercise 14.2**

Suppose that $\beta_2$ encodes the difference in reaction time between group B and group C. What should be the value of $(x_{i1}, x_{i2})$ to get the mean reaction time of group A, group B and group C?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
As group A is a reference category, $\beta_0$ expresses the mean reaction time of group A. The mean reaction time of group B is $\beta_0 + \beta_1$, so we need $(x_{i1} =1 , x_{i2} = 0)$. In the text above, the mean reaction time of group C is given by $\beta_0 + \beta_2$. However, now the value is given by $\beta_0 + \beta_1 + \beta_2$, so $(x_{i1} =1 , x_{i2} = 1)$ will give us the value we need.
</div>
</div>
</div>

The function call for a frequentist analysis with `glm` is almost identical:

```{r}
fit_glm_mc <- glm(
  # model 'RT' as a function of 'block'
  formula = RT ~ block,
  data = mc_data_cleaned
)
```

The summary of the model fit reveals $p$-values for both slope coefficients, which indicate (unsurprisingly) that there is very strong evidence against the null hypothesis of no difference between the means of the two pairs of conditions we compare with this model:

```{r}
summary(fit_glm_mc)
```

## Multiple predictors {#Chap-04-03-predictors-multiple-predictors}

<div style = "float:right; width:15%;">
<img src="visuals/badge-politeness.png" alt="badge-politeness">
</div>

The following content is a distilled version of a short tutorial on Bayesian regression modeling for factorial designs [@FrankeRoettger2019:Bayesian-regres], which can be downloaded [here](https://psyarxiv.com/cdxv3). We consider data on voice pitch in a $2 \times 2$ factorial design, with factors `gender` and `context`. This is laboratory data measuring the voice pitch of male and female speakers (factor `gender`) in two different kinds of linguistic contexts, namely a polite and an informal situation (factor `context`).

We load the data, inspect and plot it.

```{r}
politeness_data <- aida::data_polite
politeness_data
```

```{r, echo = F}
# this code is copy pasted from tutorial paper
politedata <- politeness_data 
politedata.agg <- 
  politedata %>% 
    group_by(gender, context, sentence) %>% 
    summarize(mean_frequency = mean(pitch))

politedata.agg2 <- 
  politedata %>%
  group_by(gender, context) %>% 
  summarize(mean_frequency = round(mean(pitch), 0))

ggplot(data = politedata.agg, 
       aes(x = gender, 
           y = mean_frequency, 
           colour = context)) + 
  geom_point(position = position_dodge(0.5), 
             alpha = 0.3, 
             size = 3) +
  geom_point(data = politedata.agg2, 
             aes(x = gender, 
                 y = mean_frequency, 
                 #colour = context,
                 fill = context),
             position = position_dodge(0.5), 
             pch = 21, 
             colour = "black",
             size = 5) +
  scale_x_discrete(breaks = c("F", "M"),
                  labels = c("female", "male")) +
  scale_y_continuous(expand = c(0, 0), breaks = (c(50,100,150,200,250,300)), limits = c(50,300)) +
  scale_colour_manual(breaks = c("inf", "pol"),
                      labels = c("informal", "polite"),
                      values = c("#f1a340", "#998ec3")) +
  scale_fill_manual(breaks = c("inf", "pol"),
                      labels = c("informal", "polite"),
                      values = c("#f1a340", "#998ec3")) +
  ylab("pitch in Hz\n") +
  xlab("\ngender")
```

In a $2 \times 2$ factorial design like this, there are essentially four pairs of factor levels (so-called **design cells**): female speakers in informal contexts, female speakers in polite contexts, male speakers in informal contexts and male speakers in polite contexts. Different schemes exist by means of which different comparisons of means of design cells (or single factors) can be probed. A simple coding scheme for differences in our $2 \times 2$ design is shown in Figure \@ref(fig:Chap-04-02-beyond-simple-regression-factorial-coefficients). We consider the cell "female+informal" as the reference level and therefore model its mean as intercept $\beta_0$. We then have a slope term $\beta_{\text{pol}}$ which encodes the difference between female pitch in informal and female pitch in polite contexts. Analogous reasoning holds for $\beta_{\text{male}}$. Finally, we also include a so-called **interaction term**, denoted as $\beta_{\text{pol\&male}}$ in Figure \@ref(fig:Chap-04-02-beyond-simple-regression-factorial-coefficients). The interaction term quantifies how much a change away from the reference level in both variables differs from the sum of unilateral changes.

```{r Chap-04-02-beyond-simple-regression-factorial-coefficients, echo = F, fig.cap="Regression coefficients for a factorial design (using so-called 'treatment coding').", fig.width=4}
knitr::include_graphics("visuals/coefficients_factorial_design.png")
```

We can fit a regression model with this coding scheme using the formula `pitch ~ gender * context`. Importantly the star `*` between explanatory variables `gender` and `context` indicates that we also want to include the interaction term.

```{r eval = F}
fit_brms_politeness <- brm(
  # model 'pitch' as a function of 'gender' and 'context',
  #  also including the interaction between `gender` and `context`
  formula = pitch ~ gender * context,
  data = politeness_data
)
```

```{r echo = F}
fit_brms_politeness <- readRDS('models_brms/politeness_fit.rds')
```

The summary statistics below lists the model parameters indicated in Figure \@ref(fig:Chap-04-02-beyond-simple-regression-factorial-coefficients).

```{r}
summary(fit_brms_politeness)$fixed[,c("Estimate","l-95% CI", "u-95% CI")]
```

We could conclude from this that, given model and data, it is plausible to think that male speakers had lower voices than female speakers in informal contexts: this shows in the exclusion of 0 in the 95% inter-quantile range for parameter `genderM`. We may also conclude that given model and data, it is plausible to think that female speakers used lower voices in polite contexts than in formal ones  (parameter `contextpol`).
The posterior of the interaction term `genderM:contextpol` does not give any indication to think that 0, or any value near it, is not plausible. This can be interpreted as saying that there is no indication, given model and data, to believe that male speakers' voice pitch changes differently from informal to polite contexts than female speakers' voice pitch does.

<div class = "exercises">
**Exercise 14.3**

Based on the estimate given above, what is the mean estimate for male speakers speaking in informal contexts?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
The mean estimate for male speakers speaking in informal contexts is given by $\beta_0 +\beta_{\text{male}} = 261.02993 -116.53009 \approx 144$.
</div>
</div>
</div>
