# Categorical predictors {#Chap-04-03-predictors}

<hr>

The previous chapters applied linear regression models to cases where we wanted to predict a metric variable $y$ based on the values of associated metric measurements $x_i$ ($1 \le i \le n$).
In this chapter we are generalizing this approach to also deal with the case where a predictor $x_i$ is a categorical variable, such as an indicator value showing which group or experimental condition a measurement of $y$ belongs to.
In this way, at the end of the chapter we will be able to apply linear regression modeling to the analysis of (metric) measurements, for instance, from a factorial design - a common design type of psychological experiments (see Chapter \@ref(Chap-02-01-data)).

As we will see in this chapter, the only "trick" to generalizing linear regression modeling to also cover **categorical predictors**, as we will call them, is to map levels of a categorical variable onto numbers.
For example, if we have two groups in a predictor variable $x$, say group $A$ and $B$, we could just encode group $A$ as a value of $x = 0$ and group $B$ as a value of $x = 1$.
But there are many sensible mappings of this kind, and many helplessly ridiculous ones.
The relevant technical term here is **contract coding**, i.e., a scheme of mapping categorical distinctions onto numeric representations in such a way that the theoretically interesting *contrasts* (i.e., the distinctions between groups that we want to test) can be easily tested with the resulting regression model. 

The chapter is structured as follows.
Section \@ref(Chap-04-03-predictors-two-levels) uses the [Simon task data](app-93-data-sets-simon-task) to cover the case of a single categorical predictor with just two category levels (e.g., just groups $A$ and $B$).
Section \@ref(Chap-04-03-predictors-multi-levels) then uses data from the [Mental Chronometry experiment](app-93-data-sets-mental-chronometry) to look at the more general case of a single categorical predictor with more than two category levels (e.g., just groups $A$, $B$ and $C$). 
Section \@ref(Chap-04-03-predictors-multiple-predictors) considers cases with several categorical predictors, including their **interaction**, based on the [politeness data set]().

The video below provides a dense coverage of single-predictor contrast coding, i.e., the contents of Sections \@ref(Chap-04-03-predictors-two-levels) and \@ref(Chap-04-03-predictors-multi-levels).

<iframe src="https://player.vimeo.com/video/422832571" width="640" height="360" frameborder="0" allow="fullscreen" allowfullscreen></iframe>

```{block, type='infobox'}
The learning goals for this chapter are:

- understand the basic idea behind *contrast coding*
- be able to interpret the results of a regression analysis which uses *treatment coding*
- be able to interpret inferences for models including an *interaction coefficient* for two categorical predictors

```

## Single two-level predictor {#Chap-04-03-predictors-two-levels}

Let's revisit the data from the [Simon task](app-93-data-sets-simon-task).
Just like in chapter \@ref(), we will be looking at the hypothesis that, among all correct responses, the mean reaction times for the congruent condition are lower than those of the incongruent condition.

```{r}
# extract just the currently relevant columns 
#  from the data set
data_ST_excerpt <- aida::data_ST %>% 
  filter(correctness == "correct") %>% 
  select(RT, condition)

# show the first couple of lines
head(data_ST_excerpt, 5)
```

Notice that this tibble contains the data in a tidy format, i.e., each row contains a pair of associated measurements. 
We want to explain or predict the variable `RT` in terms of the variable `condition`.
The variable `RT` is a metric measurement.
But the variable `condition` is categorical variable with two category levels.

Before we head on, let's look at the data (again).
Here's a visualization of the distribution of RTs in each condition:

```{r}
data_ST_excerpt %>% 
  ggplot(aes(x = condition, y = RT, color = condition, fill = condition)) +
  geom_violin() +
  theme(legend.position = "none")
```

The means for both conditions are:

```{r}
data_ST_excerpt %>% 
  group_by(condition) %>% 
  summarize(mean_RT = mean(RT))
```

The difference between the means of conditions is:

```{r}
data_ST_excerpt %>% filter(condition == "incongruent") %>% pull(RT) %>% mean() -
  data_ST_excerpt %>% filter(condition == "congruent") %>% pull(RT) %>% mean()
```

While numerically this difference seems high, the question remains whether this difference is, say, big enough to earn our trust.
We address this question here using posterior inference based on a regression model. 
Notice that we simply use the same formula syntax as before: we want a model that explains `RT` in terms of `condition`.

```{r}
fit_brms_ST <- brm(
  formula = RT ~ condition,
  data = data_ST_excerpt
)
```

Let's inspect the summary information for the posterior samples, which we do here using the `summary` function for the `brms_fit` object from which we extract information only about the `fixed` effects, showing the mean (here called "Estimate") and indicators of the lower and upper 95% inner quantile.

```{r}
summary(fit_brms_ST)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```

We see that the model inferred a value for an "Intercept" variable and for another variable called "conditionincongruent".
What are these?
If you look back at the empirically inferred means, you will see that the mean estimate for "Intercept" corresponds to the mean of RTs in the "congruent" condition and that the mean estimate for the variable "conditionincongruent" closely matches the computed difference between the means of the two conditions.
And, indeed, that is what this regression model is doing for us.
Using a uniform formula syntax `brms` has set up a regression model in which a predictor, given as a character (string) column, was internally coerced somehow into a format that produced an estimate the for the mean of one condition and an estimate for the difference between conditions.

How does this results come about?
And why are the variables returned by `brms` called "Intercept" and "conditionincongruent"?
In order to use the simply linear regression model, the categorical predictor $x$ has been coded as either $0$ or $1$.
Concretely, `brms` has introduced a new predictor variable, call it `new_predictor`, which has value $0$ for the "congruent" condition and $1$ for the "incongruent" condition.
Per default `brms` chooses the level that is alphanumerically first as the so-called **reference level**, assigning to it the value $0$.
Here, that's "congruent".

The result would look like this:

```{r}
data_ST_excerpt %>% 
  mutate(new_predictor = ifelse(condition == "congruent", 0, 1)) %>% 
  head(5)
```
Now, with this new numeric coding of the predictor, we can calculate the linear regression model as ususal:

$$
\begin{aligned}
\xi_i & = \beta_0 + \beta_1 x_i & y_i & \sim \text{Normal}(\mu = \xi_i, \sigma)
\end{aligned}
$$

As a consequence, the linear model's intercept parameter $\beta_0$ can be interpreted as the predicted mean of the reference level: if for some $i$ we have $x_i = 0$, then the predictor $\xi_i$ will just be $\xi_i = \beta_0$; whence that the intercept $\beta_0$ will be fitted to the mean of the reference level If for some $i$ we have $x_i = 1$ instead, the predicted value will be computed as $\xi_i = \beta_0 + \beta_1$, so that the slope term $\beta_1$ will effectively play the role of the difference $\delta$ between the mean of the groups.
The upshot is that we can conceive of a **$t$-test as a special case of a linear regression model!**

Schematically, we can represent this coding scheme for coefficients like so:
```{r, echo = F}
tribble(
  ~"condition", ~"beta_0", ~"betat_1",
  "congruent", 1, 0,
  "incongruent", 1, 1
)
```


<div class = "exercises">
**Exercise 14.1**
For the given data below, compute (or guess) the MLEs of the regression coefficients. Choose the appropriate 0/1 encoding of group information.
We have two groups, and three measurements of $y$ for each:

groupA: (1,0,2) and groupB: (10,13,7)

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

For $\xi_i  = \beta_0 + \beta_1 x_i$, let $x_i =0$ if the data point if from groupA and $x_i=1$ if it's from groupB. Then the mean of groupA is computed by the intercept $\mu_A  = \beta_0$ and the mean of groupB is computed as the sum of the intercept and the slope $\mu_B  = \beta_0 + \beta_1$. Since $\mu_A = 1$ and $\mu_B = 10$, we can guess that $\beta_0 = 1$ and $\beta_1 = 10 - 1 = 9$.

</div>
</div>
</div>

## Single multi-level predictors {#Chap-04-03-predictors-multi-levels}

<div style = "float:right; width:15%;">
<img src="visuals/badge-mental-chronometry.png" alt="badge-mental-chronometry">  
</div>  

The 0/1 coding scheme above works fine for a single categorical predictor value with two levels.
It is possible to use linear regression also for categorical predictors with more that two levels.
Only, in that case, there are quite a few more reasonable **contrast coding** schemes, i.e., ways to choose numbers to encode the levels of the predictor.

The [mental chronometry data](#app-93-data-sets-mental-chronometry) has a single categorical predictor, called `block`, with three levels, called "reaction", "goNoGo" and "discrimination".
We are interested in regression reactions time, stored in variable `RT`, against `block`.
Our main question of interest is whether these inequalities are supported by the data:

$$
\text{RT in 'reaction'} < 
\text{RT in 'goNoGo'} <
\text{RT in 'discrimination'}
$$
So we are interested in the $\delta$s, so to speak, between 'reaction' and 'goNoGo' and between 'discrimination' and 'goNoGo'.


Let's consider only the data relevant for our current purposes:

```{r}
# select the relevant columns
data_MC_excerpt <- aida::data_MC_cleaned %>% 
  select(RT, block)

# show the first couple of lines
data_MC_excerpt %>% 
  head(5)
```

Here are the means of the reaction times for different `block levels:

```{r}
data_MC_excerpt %>% 
  group_by(block) %>% 
  summarize(mean_RT = mean(RT))
```

To fit this model with `brms` we need a simple function call with the formula `RT ~ block` that precisely describes what we are interested in: to explain reaction times as a function of the experimental condition:

```{r eval = T}
fit_brms_mc <- brm(
  formula = RT ~ block,
  data = data_MC_excerpt
)
```


To inspect the posterior fits of this model, we can extract the relevant summary statistics as before:

```{r}
summary(fit_brms_mc)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```

Notice that there is an intercept term, as before. 
This corresponds to the mean reaction time of the reference level, which is again set based on alphanumeric ordering, so corresponding to "discrimination".
There are two slope coefficients, one for the difference between the reference level and "goNoGo" and another for the difference between the reference level and the "reaction" condition.

These intercepts are estimated to be credibly negative, suggesting that the "discrimination" condition indeed had the highest mean reaction times.
This answer one half of the comparisons we are interested in:

$$
\text{RT in 'reaction'} < 
\text{RT in 'goNoGo'} <
\text{RT in 'discrimination'}
$$
Unfortunately, it is not directly possible to read-off information about the second comparison we care about, namely the comparison between "reaction" and "goNoGO".
And here is where we see the point of **contrast coding** pup up for the first time.
We would like to encode predictor levels ideally in such a way that we can read off (test) the hypotheses we care about directly.
In other words, if possible we would like to have parameters in our model, in the form of slope coeffiencts, which directly encode the $\delta$s, so to speak, that we want to test.^[To be precise, it is possible to also test derived random variables from the posterior samples. So, it is not *necessary* to encode the contrasts of interests directly. But, most often, in Bayesian analyses it will make sense to put priors on exactly these $\delta$s (e.g., skeptical priors biased against a hypothesis to be tested) and for *that* purpose it is (almost) practically necessary to have the relevant contrasts expressed as slope coeffients in the model.]

In the case at hand, all we need to do is change the reference level.
If the reference level is the "middle category" (as per our ordered hypothesis), the two slopes will express the contrasts we care about.
To change the reference level, we only need to make `block` a factor and order its levels manually, like so:

```{r}
data_MC_excerpt <- data_MC_excerpt %>% 
  mutate(block_reordered = factor(block, levels = c("goNoGo", "reaction", "discrimination")))
```

We then run another Bayesian regression model, regressing `RT` agains `block_reordered`.

```{r}
fit_brms_mc_reordered <- brm(
  formula = RT ~ block_reordered,
  data = data_MC_excerpt
)
```

And inspect the summary of the posterior samples for the relevant coefficients:

```{r}
summary(fit_brms_mc_reordered)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```
Now the "Intercept" corresponds to the new reference level "goNoGo".
And the two slope coefficients give the differences to the other two levels.

Which numeric encoding leads to this result?
In formulaic terms, we have three coefficients $\beta_0, \dots, \beta_2$.
The predicted mean value for observation $i$ is $\xi_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$. 
We assign numeric value $1$ for predictor $x_1$ when the observation is from the "reaction" block.
We assign numeric value $1$ for predictor $x_2$ when the observation is from the "discrimination" block.
Schematically, what we now have is:
```{r, echo = F}
tribble(
  ~"block", ~"beta_0", ~"betat_1", ~"beta_2",
  "goNoGo", 1, 0, 0,
  "reaction", 1, 1, 0,
  "discrimination", 1, 0, 1
)
```




As we may have expected, the 95% inter-quantile range for both slope coefficients (which, given the amount of data we have, is almost surely almost identical to the 95% HDI) does not include 0 by a very wide margin.
We could, therefore, conclude, based on a Bayesian approach to hypothesis testing in terms of posterior estimation, that the reaction times of conditions are credibly different.

The coding of levels in terms of a reference level is called *treatment coding*, or also *dummy coding*.
The video included at the beginning of this chapter discusses further contrast coding schemes, and also shows in more detail how a coding scheme translates into "directly testable" hypotheses.

<div class = "exercises">
**Exercise 14.2**

Suppose that there are three groups, A, B, and C as levels of your predictor. You want the regression intercept to be the mean of group A. You want the first slope to be the difference between the means of group B and group A. And, you want the second slope to the difference between the mean of C and B. How do you numerically encode these contrasts in terms of numeric predictor values?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">

Schematically, like this:

```{r, echo = F}
tribble(
  ~"group", ~"beta_0", ~"betat_1", ~"beta_2",
  "A", 1, 0, 0,
  "B", 1, 1, 0,
  "C", 1, 1, 1
)
```


As group A is a reference category, $\beta_0$ expresses the mean reaction time of group A. The mean reaction time of group B is $\beta_0 + \beta_1$, so we need $(x_{i1} =1 , x_{i2} = 0)$ for any $i$ which is of group B. In the text above, the mean reaction time of group C is given by $\beta_0 + \beta_2$. However, now the value is given by $\beta_0 + \beta_1 + \beta_2$, so $(x_{i1} =1 , x_{i2} = 1)$ will give us the value we need.

</div>
</div>
</div>

## Multiple predictors {#Chap-04-03-predictors-multiple-predictors}

Factorial designs, which have more than one categorical predictor variable, are common in experimental psychology.
Any contrast coding scheme usable for encoding a single categorical predictor can, in principle, also be used when there are multiple categorical predictors.
But multiple categorical predictors also require some additional considerations relating to how (the model assumes that) different predictors might or might not interact with another.

Here is an informal example.
Suppose that we have metric measurements of how tasty a snack is perceived to be.
There are two categorical factors which we want to use to predict the average tastiness of a snack.
The first predictor is `mayo` and we encode it numerically as: 0 if the dish does not contain mayonnaise and 1 if it does.
The second predictor is `chocolate` and we encode it similarly as: 0 if the dish does not contain chocolate and 1 if it does.
Suppose we estimate the two (!) slope coefficients for `mayo` and `chocolate` for our imaginary data set and find that both are credibly positive.
That means that there is reason to believe that, all else equal, when we find `mayo` in a snack we may expect it to be rated as more tasty, and, all else equal, when we find `chocolate` in a snack we may also expect it to be rated as more tasty.^[Notice the deliberate avoidance of causal terminology. We should not say that there is reason to believe that *adding* `mayo` to any dish will make it more tasty, just that, epistemically put, observing it in a dish should alter our expectations.] 
But what about a dish with *both* `mayo` *and* `chocolate`?
Maybe we can agree to assume for the sake of argument that, on average, snacks containing both `mayo` and `chocolate` are *not* rated as tasty at all.
Or at least, we might want to include in our model the possibility that the combination of `mayo` and `chocolate` has a different effect than the sum of the contributions of `mayo` on its own and that of `chocolate` of its own.
That is why, when we have multiple categorical predictors, we also often want to include yet another type of slope coefficients, so-called **interaction terms**, that capture how the combination of different factor levels from different categorical predictors, well, interact.
If you like a more precise characterization at this moment already (although an example below will make things hopefully much clearer), we could say that, in the context of a linear regression model, an interaction between levels of several predictors is a (potential) deviation from the sum of all of the additive effects of the individual predictor levels in isolation.

<div style = "float:right; width:15%;">
<img src="visuals/badge-politeness.png" alt="badge-politeness">
</div>

To make this more precise, let us consider the example of the [politeness data].^[The following content is a distilled version of a short tutorial on Bayesian regression modeling for factorial designs [@FrankeRoettger2019:Bayesian-regres], which can be downloaded [here](https://psyarxiv.com/cdxv3)]
The to-be-predicted data are measurements of voice pitch in a $2 \times 2$ factorial design, with factors `gender` and `context`.
The factor `gender` has (sadly only) two levels: "male" and "female".
The factor `context` has two levels, namely "informal" for informal speech situations and "polite" for polite speech situations.

We load the data, inspect and plot it.

```{r}
politeness_data <- aida::data_polite
politeness_data
```

```{r, echo = F}
# this code is copy pasted from tutorial paper
politedata <- politeness_data 
politedata.agg <- 
  politedata %>% 
    group_by(gender, context, sentence) %>% 
    summarize(mean_frequency = mean(pitch))

politedata.agg2 <- 
  politedata %>%
  group_by(gender, context) %>% 
  summarize(mean_frequency = round(mean(pitch), 0))

ggplot(data = politedata.agg, 
       aes(x = gender, 
           y = mean_frequency, 
           colour = context)) + 
  geom_point(position = position_dodge(0.5), 
             alpha = 0.3, 
             size = 3) +
  geom_point(data = politedata.agg2, 
             aes(x = gender, 
                 y = mean_frequency, 
                 #colour = context,
                 fill = context),
             position = position_dodge(0.5), 
             pch = 21, 
             colour = "black",
             size = 5) +
  scale_x_discrete(breaks = c("F", "M"),
                  labels = c("female", "male")) +
  scale_y_continuous(expand = c(0, 0), breaks = (c(50,100,150,200,250,300)), limits = c(50,300)) +
  scale_colour_manual(breaks = c("inf", "pol"),
                      labels = c("informal", "polite"),
                      values = c("#f1a340", "#998ec3")) +
  scale_fill_manual(breaks = c("inf", "pol"),
                      labels = c("informal", "polite"),
                      values = c("#f1a340", "#998ec3")) +
  ylab("pitch in Hz\n") +
  xlab("\ngender")
```



In a $2 \times 2$ factorial design like this, there are essentially four pairs of factor levels (so-called **design cells**): female speakers in informal contexts, female speakers in polite contexts, male speakers in informal contexts and male speakers in polite contexts. 
Different schemes exist by means of which different comparisons of means of design cells (or single factors) can be probed. 
A simple coding scheme for differences in our $2 \times 2$ design is shown in Figure \@ref(fig:Chap-04-02-beyond-simple-regression-factorial-coefficients).
This is a straightforward extension of *treatment coding* for the single predictors, while also catering to their potential interaction.
Concretely, we consider the cell "female+informal" as the reference level and therefore model its mean as intercept $\beta_0$.
We then have a slope term $\beta_{\text{pol}}$ which encodes the difference between female pitch in informal and female pitch in polite contexts. 
Analogous reasoning holds for $\beta_{\text{male}}$. Finally, we also include a so-called **interaction term**, denoted as $\beta_{\text{pol+male}}$ in Figure \@ref(fig:Chap-04-02-beyond-simple-regression-factorial-coefficients). 
The interaction term quantifies how much a change away from the reference level in both variables differs from the sum of unilateral changes.

```{r Chap-04-02-beyond-simple-regression-factorial-coefficients, echo = F, fig.cap="Regression coefficients for a factorial design (using so-called 'treatment coding').", fig.width=4}
knitr::include_graphics("visuals/coefficients_factorial_design.png")
```

We can fit a regression model with this coding scheme using the formula `pitch ~ gender * context`. Importantly the star `*` between explanatory variables `gender` and `context` indicates that we also want to include the interaction term.^[If the interaction term should be excluded, the formula `pitch ~ gender + context` can be used, so with `+` instead of `*`.]

```{r eval = F}
fit_brms_politeness <- brm(
  # model 'pitch' as a function of 'gender' and 'context',
  #  also including the interaction between `gender` and `context`
  formula = pitch ~ gender * context,
  data = politeness_data
)
```

```{r echo = F}
fit_brms_politeness <- readRDS('models_brms/politeness_fit.rds')
```

The summary statistics below lists the model parameters indicated in Figure \@ref(fig:Chap-04-02-beyond-simple-regression-factorial-coefficients).

```{r}
summary(fit_brms_politeness)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]
```

We can interpret this as saying that, given model and data, it is plausible to think that male speakers had lower voices than female speakers in informal contexts: this shows in the exclusion of 0 in the 95% inter-quantile range for parameter `genderM`. We may also conclude that given model and data, it is plausible to think that female speakers used lower voices in polite contexts than in formal ones  (parameter `contextpol`).
The posterior of the interaction term `genderM:contextpol` does not give any indication to think that 0, or any value near it, is not plausible. This can be interpreted as saying that there is no indication, given model and data, to believe that male speakers' voice pitch changes differently from informal to polite contexts than female speakers' voice pitch does.

<div class = "exercises">
**Exercise 14.3**

Based on the estimate given above, what is the mean estimate for male speakers speaking in informal contexts?

<div class="collapsibleSolution">
<button class="trigger">Solution</button>
<div class="content">
The mean estimate for male speakers speaking in informal contexts is given by $\beta_0 +\beta_{\text{male}} = 261.02993 -116.53009 \approx 144$.
</div>
</div>
</div>
